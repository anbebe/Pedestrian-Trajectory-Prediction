{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 08:50:57.766308: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-04 08:50:58.132847: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-04 08:50:58.135307: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-04 08:50:59.193368: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np \n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>voxelgrids</th>\n",
       "      <th>positions</th>\n",
       "      <th>poses</th>\n",
       "      <th>odoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>[[[25, 53], [32, 2], [17, 54], [26, 50], [30, ...</td>\n",
       "      <td>[[0.7827229554855963, -0.6603078958156333, 4.5...</td>\n",
       "      <td>[[[31.0, 57.0, 0.1766081601381302], [27.0, 52....</td>\n",
       "      <td>[[0.00022100093561436428, -0.00164606522993054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>[[[17, 38], [36, 71], [18, 39], [29, 53], [24,...</td>\n",
       "      <td>[[1.8763295554092785, -0.7392371475816817, 8.6...</td>\n",
       "      <td>[[[17.0, 1.0, 0.22867129743099213], [17.0, 1.0...</td>\n",
       "      <td>[[-0.0010584275078791316, 0.005180639174413457...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>[[[13, 56], [8, 46], [20, 66], [44, 3], [43, 6...</td>\n",
       "      <td>[[1.2298735958886313, -0.6599135136166747, 8.2...</td>\n",
       "      <td>[[[19.0, 13.0, 0.1444173902273178], [17.0, 11....</td>\n",
       "      <td>[[-0.017197470816407222, -0.008501126993886861...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>[[[29, 68], [66, 59], [11, 93], [73, 58], [24,...</td>\n",
       "      <td>[[0.49709940835786537, -0.62631209064576, 7.09...</td>\n",
       "      <td>[[[18.0, 5.0, 0.17500989139080048], [21.0, 2.0...</td>\n",
       "      <td>[[0.00810160200217194, -0.006334385323650088, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>[[[22, 6], [23, 14], [23, 15], [26, 21], [23, ...</td>\n",
       "      <td>[[0.9673029278573837, -0.3436083278432631, 2.9...</td>\n",
       "      <td>[[[62.0, 62.0, 0.14962413907051086], [73.0, 45...</td>\n",
       "      <td>[[-0.027349297901359395, -0.011966854241542754...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                         voxelgrids  \\\n",
       "0  -1  [[[25, 53], [32, 2], [17, 54], [26, 50], [30, ...   \n",
       "1  -1  [[[17, 38], [36, 71], [18, 39], [29, 53], [24,...   \n",
       "2  -1  [[[13, 56], [8, 46], [20, 66], [44, 3], [43, 6...   \n",
       "3  -1  [[[29, 68], [66, 59], [11, 93], [73, 58], [24,...   \n",
       "4  -1  [[[22, 6], [23, 14], [23, 15], [26, 21], [23, ...   \n",
       "\n",
       "                                           positions  \\\n",
       "0  [[0.7827229554855963, -0.6603078958156333, 4.5...   \n",
       "1  [[1.8763295554092785, -0.7392371475816817, 8.6...   \n",
       "2  [[1.2298735958886313, -0.6599135136166747, 8.2...   \n",
       "3  [[0.49709940835786537, -0.62631209064576, 7.09...   \n",
       "4  [[0.9673029278573837, -0.3436083278432631, 2.9...   \n",
       "\n",
       "                                               poses  \\\n",
       "0  [[[31.0, 57.0, 0.1766081601381302], [27.0, 52....   \n",
       "1  [[[17.0, 1.0, 0.22867129743099213], [17.0, 1.0...   \n",
       "2  [[[19.0, 13.0, 0.1444173902273178], [17.0, 11....   \n",
       "3  [[[18.0, 5.0, 0.17500989139080048], [21.0, 2.0...   \n",
       "4  [[[62.0, 62.0, 0.14962413907051086], [73.0, 45...   \n",
       "\n",
       "                                               odoms  \n",
       "0  [[0.00022100093561436428, -0.00164606522993054...  \n",
       "1  [[-0.0010584275078791316, 0.005180639174413457...  \n",
       "2  [[-0.017197470816407222, -0.008501126993886861...  \n",
       "3  [[0.00810160200217194, -0.006334385323650088, ...  \n",
       "4  [[-0.027349297901359395, -0.011966854241542754...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"../final_dataset.pkl\") \n",
    "df.reset_index(inplace=True, drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-04 08:51:45.366624: W tensorflow/core/framework/dataset.cc:956] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2024-07-04 08:52:15.369800: W tensorflow/core/framework/dataset.cc:956] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "position_list = []\n",
    "pose_list = []\n",
    "for index, row in df.iterrows():\n",
    "    if row['positions'].shape[0] >= num_steps:\n",
    "        pos_data = row['positions']\n",
    "        poses_data = row['poses'].reshape(-1,17*3)\n",
    "        vox_data = row['voxelgrids'].reshape(-1,1000*2)\n",
    "        #data_df = tf.data.Dataset.from_tensor_slices({\"input_pos\": pos_data, \"input_poses\":poses_data,\"input_grid\": vox_data})\n",
    "        #print(data_df)\n",
    "        pos_df = tf.data.Dataset.from_tensor_slices(pos_data)\n",
    "        pose_df = tf.data.Dataset.from_tensor_slices(poses_data)\n",
    "        pos_df = pos_df.window(num_steps, shift=3, drop_remainder=True)\n",
    "        pose_df = pose_df.window(num_steps, shift=3, drop_remainder=True)\n",
    "        for windows1, windows2 in zip(pos_df, pose_df):\n",
    "            np_arr1 = np.asarray([item.numpy() for item in windows1])\n",
    "            #print(np_arr1.shape)\n",
    "            position_list.append(np_arr1)\n",
    "\n",
    "            np_arr2 = np.asarray([item.numpy() for item in windows2])\n",
    "            #print(np_arr2.shape)\n",
    "            pose_list.append(np_arr2)\n",
    "\n",
    "            #dataset_list.append(tf.data.Dataset.from_tensors(windows))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_dataset = tf.data.Dataset.from_tensor_slices(np.asarray(position_list))\n",
    "pose_dataset = tf.data.Dataset.from_tensor_slices(np.asarray(pose_list))\n",
    "zip_ds = tf.data.Dataset.zip((pos_dataset, pose_dataset))\n",
    "zip_ds = zip_ds.shuffle(buffer_size=100, reshuffle_each_iteration=True)\n",
    "zip_ds = zip_ds.batch(32).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 15, 3) (32, 15, 51)\n"
     ]
    }
   ],
   "source": [
    "for data1, data2 in zip_ds:\n",
    "    print(data1.shape, data2.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zip_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle, batch and split dataset\n",
    "DATASET_SIZE = len(zip_ds)\n",
    "\n",
    "train_size = int(0.8 * DATASET_SIZE)\n",
    "test_size = int(0.2 * DATASET_SIZE)\n",
    "\n",
    "train_dataset = zip_ds.take(train_size)\n",
    "test_dataset = zip_ds.skip(train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)\n",
    "# 26 batches for len 20 and shift 5\n",
    "# 41 batches for len 20 and shift 3\n",
    "# 60 batches for len 15 and shift 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[2.60000000e+01 3.10000000e+01 2.90879875e-01 ... 3.80000000e+01\n",
      "   1.60000000e+02 4.20909047e-01]\n",
      "  [2.50000000e+01 4.20000000e+01 8.85042250e-02 ... 3.90000000e+01\n",
      "   1.64000000e+02 3.86540353e-01]\n",
      "  [2.90000000e+01 1.80000000e+01 2.38930687e-01 ... 3.40000000e+01\n",
      "   1.15000000e+02 6.04797006e-01]\n",
      "  ...\n",
      "  [2.00000000e+01 1.20000000e+01 3.18613201e-01 ... 2.90000000e+01\n",
      "   1.29000000e+02 6.68862522e-01]\n",
      "  [2.00000000e+01 9.00000000e+00 2.71391094e-01 ... 2.80000000e+01\n",
      "   1.24000000e+02 4.84991878e-01]\n",
      "  [2.10000000e+01 1.30000000e+01 2.11803406e-01 ... 2.20000000e+01\n",
      "   1.19000000e+02 4.10803765e-01]]\n",
      "\n",
      " [[2.90000000e+01 2.20000000e+01 2.21498668e-01 ... 3.50000000e+01\n",
      "   1.83000000e+02 5.08365989e-01]\n",
      "  [2.80000000e+01 2.30000000e+01 2.95080930e-01 ... 3.30000000e+01\n",
      "   1.82000000e+02 4.67873484e-01]\n",
      "  [3.60000000e+01 2.00000000e+01 2.60397196e-01 ... 3.40000000e+01\n",
      "   1.89000000e+02 4.16139275e-01]\n",
      "  ...\n",
      "  [5.00000000e+01 1.70000000e+01 2.74160713e-01 ... 4.70000000e+01\n",
      "   2.11000000e+02 4.75996703e-01]\n",
      "  [5.00000000e+01 6.00000000e+00 3.23044211e-01 ... 4.40000000e+01\n",
      "   2.09000000e+02 4.74330187e-01]\n",
      "  [4.20000000e+01 3.00000000e+00 1.89782634e-01 ... 3.50000000e+01\n",
      "   1.86000000e+02 2.36550316e-01]]\n",
      "\n",
      " [[1.00000000e+01 2.60000000e+01 1.76147193e-01 ... 1.30000000e+01\n",
      "   1.09000000e+02 3.50980252e-01]\n",
      "  [1.50000000e+01 1.00000000e+00 2.55739659e-01 ... 8.00000000e+00\n",
      "   9.70000000e+01 1.38464317e-01]\n",
      "  [2.00000000e+01 2.00000000e+01 1.25135720e-01 ... 1.40000000e+01\n",
      "   1.00000000e+02 1.56098276e-01]\n",
      "  ...\n",
      "  [1.70000000e+01 2.00000000e+01 2.89115101e-01 ... 9.00000000e+00\n",
      "   1.27000000e+02 5.66860884e-02]\n",
      "  [1.30000000e+01 1.80000000e+01 1.66916549e-01 ... 9.00000000e+00\n",
      "   1.40000000e+02 3.30296576e-01]\n",
      "  [2.20000000e+01 1.50000000e+01 2.11173609e-01 ... 1.90000000e+01\n",
      "   1.53000000e+02 4.13409978e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.00000000e+01 6.00000000e+00 1.46352395e-01 ... 1.90000000e+01\n",
      "   8.60000000e+01 5.76897189e-02]\n",
      "  [2.00000000e+01 5.00000000e+00 2.64732689e-01 ... 2.70000000e+01\n",
      "   9.00000000e+01 1.89043105e-01]\n",
      "  [2.10000000e+01 1.50000000e+01 2.06595466e-01 ... 1.60000000e+01\n",
      "   9.40000000e+01 1.40256673e-01]\n",
      "  ...\n",
      "  [8.00000000e+00 9.00000000e+00 1.36701688e-01 ... 1.00000000e+01\n",
      "   1.43000000e+02 2.86799043e-01]\n",
      "  [1.80000000e+01 2.70000000e+01 2.10952505e-01 ... 1.70000000e+01\n",
      "   1.43000000e+02 5.98853469e-01]\n",
      "  [2.50000000e+01 2.10000000e+01 1.71436504e-01 ... 2.90000000e+01\n",
      "   1.56000000e+02 7.48571038e-01]]\n",
      "\n",
      " [[5.10000000e+01 3.30000000e+01 2.97065675e-01 ... 3.00000000e+01\n",
      "   1.97000000e+02 3.04967463e-01]\n",
      "  [4.80000000e+01 2.10000000e+01 2.51336604e-01 ... 5.60000000e+01\n",
      "   2.08000000e+02 5.09380877e-01]\n",
      "  [4.90000000e+01 2.20000000e+01 2.80350983e-01 ... 3.70000000e+01\n",
      "   2.10000000e+02 3.80974710e-01]\n",
      "  ...\n",
      "  [7.30000000e+01 4.50000000e+01 2.67568260e-01 ... 6.90000000e+01\n",
      "   2.55000000e+02 2.97379404e-01]\n",
      "  [5.20000000e+01 1.80000000e+01 1.55937627e-01 ... 6.90000000e+01\n",
      "   2.54000000e+02 3.03607553e-01]\n",
      "  [5.10000000e+01 2.00000000e+01 3.46981317e-01 ... 7.70000000e+01\n",
      "   2.42000000e+02 1.75420433e-01]]\n",
      "\n",
      " [[4.50000000e+01 3.30000000e+01 1.58128500e-01 ... 2.40000000e+01\n",
      "   3.02000000e+02 2.24071696e-01]\n",
      "  [5.40000000e+01 3.00000000e+00 2.46280670e-01 ... 1.20000000e+01\n",
      "   1.66000000e+02 2.33177334e-01]\n",
      "  [6.90000000e+01 6.70000000e+01 8.71194676e-02 ... 4.90000000e+01\n",
      "   3.33000000e+02 3.00322562e-01]\n",
      "  ...\n",
      "  [8.50000000e+01 2.40000000e+01 5.16638570e-02 ... 1.51000000e+02\n",
      "   3.81000000e+02 3.91387850e-01]\n",
      "  [9.30000000e+01 2.00000000e+00 5.41596338e-02 ... 6.30000000e+01\n",
      "   3.69000000e+02 2.84526438e-01]\n",
      "  [9.00000000e+01 6.00000000e+00 7.33978972e-02 ... 1.92000000e+02\n",
      "   3.87000000e+02 4.11959559e-01]]], shape=(32, 15, 51), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "for x in train_dataset:\n",
    "    print(x[1])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_x1 = train_dataset.map(lambda x,y: x)\n",
    "train_ds_x2 = train_dataset.map(lambda x, y: y)\n",
    "\n",
    "val_ds_x1 = test_dataset.map(lambda x,y: x)\n",
    "val_ds_x2 = test_dataset.map(lambda x, y: y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mask array, True = needs to be predicted\n",
    "# TODO: adapt to currrent num_steps\n",
    "mask_arrays = []\n",
    "for i in range(len(train_dataset)):\n",
    "    new_batch = []\n",
    "    for j in range(20):\n",
    "        mask_arr = [False] * 10 + [True] *10\n",
    "        # hide 0-3 in between steps (for lazyness whole datapoint)\n",
    "        hidden_nr = np.random.randint(4)\n",
    "        hidden_idx = np.random.choice(range(10),hidden_nr, replace=False)\n",
    "        for v in hidden_idx:\n",
    "            mask_arr[v] = True\n",
    "        new_batch.append(mask_arr)\n",
    "    mask_arrays.append(new_batch)\n",
    "# transform to match with batch and training, test and val datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/jrdb/input_fn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test single layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Sinusoidal Embedding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/embedding.py    \"\"\"\n",
    "class SinusoidalEmbeddingLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Sinusoidal Postional Embedding for xyz and time.\"\"\"\n",
    "\n",
    "  def __init__(self, min_freq=4, max_freq=256, hidden_size=256):\n",
    "    super().__init__()\n",
    "    self.min_freq = float(min_freq)\n",
    "    self.max_freq = float(max_freq)\n",
    "    self.hidden_size = hidden_size\n",
    "    if hidden_size % 2 != 0:\n",
    "      raise ValueError('hidden_size ({hidden_size}) must be divisible by 2.')\n",
    "    self.num_freqs_int32 = hidden_size // 2\n",
    "    self.num_freqs = tf.cast(self.num_freqs_int32, dtype=tf.float32)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    log_freq_increment = (\n",
    "        tf.math.log(float(self.max_freq) / float(self.min_freq)) /\n",
    "        tf.maximum(1.0, self.num_freqs - 1))\n",
    "    # [num_freqs]\n",
    "    self.inv_freqs = self.min_freq * tf.exp(\n",
    "        tf.range(self.num_freqs, dtype=tf.float32) * -log_freq_increment)\n",
    "\n",
    "  def call(self, input_tensor):\n",
    "    \n",
    "    # [..., num_freqs]\n",
    "    input_tensor = tf.expand_dims(input_tensor, -1)\n",
    "    input_tensor = tf.tile(input_tensor, [1, 1, 1, self.num_freqs_int32])\n",
    "    \n",
    "    # Compute the sinusoidal embeddings\n",
    "    sin_embeds = tf.sin(input_tensor * self.inv_freqs)\n",
    "    cos_embeds = tf.cos(input_tensor * self.inv_freqs)\n",
    "    \n",
    "    # Concatenate along the last axis\n",
    "    embedded = tf.concat([sin_embeds, cos_embeds], axis=-1)\n",
    "    \n",
    "    # Reshape to the desired output shape (batch_size, sequence_length, feature_size * hidden_size)\n",
    "    batch_size = tf.shape(embedded)[0]\n",
    "    sequence_length = tf.shape(embedded)[1]\n",
    "    feature_size = tf.shape(embedded)[2]\n",
    "    \n",
    "    embedded = tf.reshape(embedded, (batch_size, sequence_length, feature_size * self.hidden_size))\n",
    "    \"\"\"\n",
    "    # [..., num_freqs]\n",
    "    input_tensor = tf.repeat(\n",
    "        input_tensor[..., tf.newaxis], self.num_freqs_int32, axis=-1)\n",
    "    # [..., h]\n",
    "    embedded = tf.concat([\n",
    "        tf.sin(input_tensor * self.inv_freqs),\n",
    "        tf.cos(input_tensor * self.inv_freqs)\n",
    "    ],\n",
    "                         axis=-1)\"\"\"\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Agent Position Encoding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/agent_feature_encoder.py    \"\"\"\n",
    "class AgentPositionEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes agents spatial positions.\"\"\"\n",
    "\n",
    "  def __init__(self, output_shape, embedding_size):\n",
    "    \n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding_layer = SinusoidalEmbeddingLayer(\n",
    "        hidden_size=embedding_size) # output_shape (batch_sie, sequence_length, feature size, hidden_size)\n",
    "\n",
    "    self.mlp = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        \n",
    "        activation=None)\n",
    "\n",
    "  def call(self, input_batch):\n",
    "    return self.mlp(self.embedding_layer(input_batch[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Agent Keypoint Encoding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/agent_feature_encoder.py    \"\"\"\n",
    "class AgentKeypointsEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes the agent's keypoints.\"\"\"\n",
    "\n",
    "  def __init__(self, output_shape, embedding_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mlp1 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "  def call(self, input_batch, training=None):\n",
    "\n",
    "    keypoints = input_batch[1]\n",
    "\n",
    "    out = self.mlp1(keypoints)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Optional\n",
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "    \"\"\" Applies the masking to the sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def calc_hidden_mask(self, dataset):\n",
    "        # create mask array, False = needs to be predicted\n",
    "        sequence_length = 15\n",
    "        mask_arrays = []\n",
    "        mask_arr = [True] * 6 + [False] * 9\n",
    "        # hide 0-2 in between steps (for lazyness whole datapoint)\n",
    "        hidden_nr = np.random.randint(3)\n",
    "        hidden_idx = np.random.choice(range(6),hidden_nr, replace=False)\n",
    "        for v in hidden_idx:\n",
    "            mask_arr[v] = False\n",
    "        print(np.asarray(mask_arr).shape)\n",
    "        return np.asarray(mask_arr)\n",
    "\n",
    "    def call(self,\n",
    "           raw_input_batch: Tuple[tf.Tensor, tf.Tensor],\n",
    "           is_hidden: Optional[tf.Tensor] = None) -> Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "        input_batch = raw_input_batch\n",
    "        mask = self.calc_hidden_mask(input_batch) #tf.convert_to_tensor\n",
    "        mask_tensor = tf.constant(mask, dtype=tf.bool)\n",
    "\n",
    "        batch_size = tf.shape(input_batch[0])[0]\n",
    "        sequence_length = tf.shape(input_batch[0])[1]\n",
    "        feature_size1 = tf.shape(input_batch[0])[2]\n",
    "        feature_size2 = tf.shape(input_batch[1])[2]\n",
    "\n",
    "        # Expand dimensions of mask to match the input tensor\n",
    "        expanded_mask = tf.expand_dims(mask_tensor, axis=0)  # Add batch dimension\n",
    "        expanded_mask = tf.expand_dims(expanded_mask, axis=-1)  # Add feature dimension\n",
    "\n",
    "        # Broadcast mask to match input tensor shape\n",
    "        broadcasted_mask_pos = tf.broadcast_to(expanded_mask, (batch_size, sequence_length, feature_size1))\n",
    "        broadcasted_mask_pose = tf.broadcast_to(expanded_mask, (batch_size, sequence_length, feature_size2))\n",
    "\n",
    "        #batch_mask = tf.broadcast_to(expanded_mask, (batch_size, sequence_length))\n",
    "\n",
    "        # Apply mask\n",
    "        masked_input_pos = tf.where(broadcasted_mask_pos, input_batch[0], tf.zeros_like(input_batch[0]))\n",
    "        masked_input_pose = tf.where(broadcasted_mask_pose, input_batch[1], tf.zeros_like(input_batch[1]))\n",
    "\n",
    "        return (masked_input_pos, masked_input_pose), mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTemporalEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes agents temporal positions.\"\"\"\n",
    "\n",
    "  def __init__(self,output_shape, embedding_size, num_steps):\n",
    "    super().__init__()\n",
    "    self.embedding_layer = SinusoidalEmbeddingLayer(\n",
    "        max_freq=num_steps,\n",
    "        hidden_size=embedding_size)\n",
    "\n",
    "    self.mlp = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "\n",
    "  def _get_temporal_embedding(self, input_batch):\n",
    "    # This weird thing is for exporting and loading keras model...\n",
    "    b = tf.shape(input_batch[0])[0]\n",
    "    num_steps = tf.shape(input_batch[0])[1]\n",
    "\n",
    "    t = tf.range(0, num_steps, dtype=tf.float32)\n",
    "    t = t[tf.newaxis, :]\n",
    "    t = tf.tile(t, [b, 1])\n",
    "    return self.embedding_layer(t[..., tf.newaxis])\n",
    "\n",
    "  def call(self, input_batch):\n",
    "    return self.mlp(self._get_temporal_embedding(input_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAttnAgentEncoderLearnedLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Independently encodes features and attends to them.\n",
    "\n",
    "  Agent features are cross-attended with a learned query or hidden_vecs instead\n",
    "  of MLP.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_length, batch_size=32, hidden_size=128, num_heads=4, ln_eps=1e-6, transformer_ff_dim=128, drop_prob=0.1):\n",
    "    super(FeatureAttnAgentEncoderLearnedLayer, self).__init__()\n",
    "\n",
    "    self.batch_size=batch_size\n",
    "    self.input_length = input_length\n",
    "    self.num_heads=num_heads\n",
    "\n",
    "    # Cross Attention and learned query.\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=360//num_heads,  # \"large\" to prevent a bottleneck\n",
    "        value_dim=360//num_heads)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f',\n",
    "        output_shape=transformer_ff_dim,\n",
    "        bias_axes='f',\n",
    "        activation='relu')\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    self.agent_feature_embedding_layers = []\n",
    "    # Position Feature\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentPositionEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "    # Feature Embedding - keypoints\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentKeypointsEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "\n",
    "    # Temporal Embedding\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentTemporalEncoder(output_shape=hidden_size-8, embedding_size=hidden_size, num_steps=input_length))\n",
    "\n",
    "    # [1, 1, h]\n",
    "    self.learned_query_vec = tf.Variable(\n",
    "        tf.random_uniform_initializer(\n",
    "            minval=-1., maxval=1.)(shape=[1, 1, hidden_size]),\n",
    "        trainable=True,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "  def _build_learned_query(self, input_batch):\n",
    "    \"\"\"Converts self.learned_query_vec into a learned query vector.\"\"\"\n",
    "    # This weird thing is for exporting and loading keras model...\n",
    "    b = tf.shape(input_batch[0])[0]\n",
    "    num_steps = tf.shape(input_batch[0])[1]\n",
    "\n",
    "    # [b, num_steps, h]\n",
    "    return tf.tile(self.learned_query_vec, [b, num_steps, 1])\n",
    "\n",
    "  def call(self, input_batch: Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor],\n",
    "           training: Optional[bool] = None):\n",
    "    mask = input_batch[1]\n",
    "    input_batch = input_batch[0]\n",
    "    layer_embeddings = []\n",
    "    for layer in self.agent_feature_embedding_layers:\n",
    "      layer_embedding = layer(input_batch, training=training)\n",
    "      layer_embeddings.append(layer_embedding)\n",
    "    embedding = tf.concat(layer_embeddings, axis=-1)\n",
    "\n",
    "    b = tf.shape(embedding)[0]\n",
    "    t = tf.shape(embedding)[1]\n",
    "    n = tf.shape(embedding)[2]\n",
    "\n",
    "    #print(\"embedding shape: \", embedding.shape)\n",
    "\n",
    "    attention_mask = tf.where(mask, 1, 0)\n",
    "    attention_mask = tf.cast(attention_mask, dtype=tf.float32)\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=0)\n",
    "    batch_size = tf.shape(input_batch[0])[0]\n",
    "    attention_mask = tf.broadcast_to(attention_mask, (batch_size, self.input_length, self.input_length))\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=1)  # Shape: (batch_size, 1, seq_len, seq_len)\n",
    "    attention_mask = tf.repeat(attention_mask, self.num_heads, axis=1)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "    #print(\"attention_mask shape: \", attention_mask.shape)\n",
    "    \n",
    "    learned_query = self._build_learned_query(input_batch)\n",
    "    #print(\"learned_query shape: \", learned_query.shape)\n",
    "\n",
    "    # Attention along axis 3\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=learned_query,\n",
    "        key=embedding,\n",
    "        value=embedding,\n",
    "        attention_mask=attention_mask,\n",
    "        return_attention_scores=True)\n",
    "    #print(\"attn_out shape: \", attn_out.shape)\n",
    "    # [b, t, h]\n",
    "    #attn_out = attn_out[..., 0, :]\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_dropout(out, training=training)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSelfAlignmentLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Enables agent to become aware of its temporal identity.\n",
    "\n",
    "  Agent features are cross-attended with a learned query in temporal dimension.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads=8,\n",
    "               hidden_size=128,\n",
    "               ln_eps=1e-6,\n",
    "               ff_dim=128):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_heads=num_heads\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(f'hidden_size ({hidden_size}) must be an integer '\n",
    "                       f'times bigger than num_heads ({num_heads}).')\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=ff_dim // num_heads,\n",
    "        value_dim = ff_dim // num_heads,\n",
    "        attention_axes=1)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    # [1, 1, h]\n",
    "    self.learned_query_vec = tf.Variable(\n",
    "        tf.random_uniform_initializer(\n",
    "            minval=-1., maxval=1.)(shape=[1, 1, hidden_size]),\n",
    "        trainable=True,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "  def build_learned_query(self, input_batch):\n",
    "    \"\"\"Converts self.learned_query_vec into a learned query vector.\"\"\"\n",
    "    # This weird thing is for exporting and loading keras model...\n",
    "    b = tf.shape(input_batch)[0]\n",
    "    t = tf.shape(input_batch)[1]\n",
    "\n",
    "    # [b, t, 1, h]\n",
    "    return tf.tile(self.learned_query_vec, [b, t, 1])\n",
    "\n",
    "  def call(self, input_batch):\n",
    "    # [b, t, h]\n",
    "    hidden_vecs = input_batch[0]\n",
    "    mask = input_batch[1]\n",
    "\n",
    "    # Expand the attention mask with new dims so that Keras can broadcast to\n",
    "    # the same shape as the attn_score: [b, num_heads, a, t, a, t].\n",
    "    # attn_mask shape: [b, 1, 1, 1 t,]\n",
    "    # True means the position participate in the attention while all\n",
    "    # False positions are ignored.\n",
    "    \n",
    "    #print(\"input batch shape: \", hidden_vecs.shape)\n",
    "    \n",
    "    attention_mask = tf.where(mask, 1, 0)\n",
    "    attention_mask = tf.cast(attention_mask, dtype=tf.float32)\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=0)\n",
    "    batch_size = tf.shape(hidden_vecs)[0]\n",
    "    input_length = tf.shape(hidden_vecs)[1]\n",
    "    attention_mask = tf.broadcast_to(attention_mask, (batch_size, input_length, input_length))\n",
    "    attention_mask = tf.expand_dims(attention_mask, axis=1)  # Shape: (batch_size, 1, seq_len, seq_len)\n",
    "    attention_mask = tf.repeat(attention_mask, self.num_heads, axis=1)  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "\n",
    "    # [b, t, 1, h]\n",
    "    learned_query = self.build_learned_query(hidden_vecs)\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=learned_query,\n",
    "        key=hidden_vecs,\n",
    "        value=hidden_vecs,\n",
    "        attention_mask=attention_mask,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    attn_out = self.attn_ln(attn_out + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnTransformerLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Performs full self-attention across the agent and time dimensions.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_heads=8,\n",
    "      hidden_size=128,\n",
    "      drop_prob=0.1,\n",
    "      ln_eps=1e-6,\n",
    "      ff_dim=128,\n",
    "      mask=False,\n",
    "      flatten=False,\n",
    "      multimodality_induced=False,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.mask = mask\n",
    "    self.flatten = flatten\n",
    "    self.multimodality_induced = multimodality_induced\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(\n",
    "          f'hidden_size ({hidden_size}) must be an integer '\n",
    "          f'times bigger than num_heads ({num_heads}).'\n",
    "      )\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=1,\n",
    "    )  # Full Attention time\n",
    "    self.attn_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu'\n",
    "    )\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None,\n",
    "    )\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "  def call(self, input_batch, training=None):\n",
    "    # [b, t, h] or [b, t, n, h]\n",
    "    hidden_vecs = input_batch[0]\n",
    "    mask = input_batch[1]\n",
    "\n",
    "    if self.flatten:\n",
    "      h_shape = tf.shape(hidden_vecs)\n",
    "      b = h_shape[0]\n",
    "      t = h_shape[1]\n",
    "      h = h_shape[-1]\n",
    "\n",
    "      if self.multimodality_induced:\n",
    "        n = h_shape[3]\n",
    "        hidden_vecs = tf.reshape(hidden_vecs, (b, -1, n, h))\n",
    "      else:\n",
    "        hidden_vecs = tf.reshape(hidden_vecs, (b, -1, h))\n",
    "\n",
    "    # Expand the attention mask with new dims so that Keras can broadcast to\n",
    "    # the same shape as the attn_score: [b, num_heads, a, t, a, t].\n",
    "    # attn_mask shape: [b, 1, 1, 1, a, t,]\n",
    "    # True means the position participate in the attention while all\n",
    "    # False positions are ignored.\n",
    "    if not self.mask:\n",
    "      attn_mask = None\n",
    "    else:\n",
    "      attention_mask = tf.where(mask, 1, 0)\n",
    "      attention_mask = tf.cast(attention_mask, dtype=tf.float32)\n",
    "      attention_mask = tf.expand_dims(attention_mask, axis=0)\n",
    "      batch_size = tf.shape(hidden_vecs)[0]\n",
    "      input_length = tf.shape(hidden_vecs)[1]\n",
    "      attention_mask = tf.broadcast_to(attention_mask, (batch_size, input_length, input_length))\n",
    "      attn_mask = tf.expand_dims(attention_mask, axis=1)  # Shape: (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "    if self.multimodality_induced:  # We have modes\n",
    "      attn_mask = tf.expand_dims(attn_mask, axis=1)  # Shape: (batch_size, 1, 1, seq_len, seq_len)\n",
    "\n",
    "\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=hidden_vecs,\n",
    "        key=hidden_vecs,\n",
    "        value=hidden_vecs,\n",
    "        attention_mask=attn_mask,\n",
    "        return_attention_scores=True)\n",
    "    out = self.attn_dropout(attn_out, training=training)\n",
    "    attn_out = self.attn_ln(out + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_dropout(out, training=training)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "    if self.flatten:\n",
    "      out = tf.reshape(out, h_shape)\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnModeTransformerLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Performs full self-attention across the future modes dimensions.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads=8,\n",
    "               hidden_size=128,\n",
    "               drop_prob=0.1,\n",
    "               ln_eps=1e-6,\n",
    "               ff_dim=128):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(f'hidden_size ({hidden_size}) must be an integer '\n",
    "                       f'times bigger than num_heads ({num_heads}).')\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=2)  # Attention over modes\n",
    "    self.attn_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "  def call(self, input_batch, training=None):\n",
    "\n",
    "    # [b, t, n, h]\n",
    "    hidden_vecs = input_batch\n",
    "\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=hidden_vecs,\n",
    "        key=hidden_vecs,\n",
    "        value=hidden_vecs,\n",
    "        attention_mask=None,\n",
    "        return_attention_scores=True)\n",
    "    out = self.attn_dropout(attn_out, training=training)\n",
    "    attn_out = self.attn_ln(out + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_dropout(out, training=training)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalityInduction(tf.keras.layers.Layer):\n",
    "  \"\"\"Enables the model to forward and predict multi-mode predictions.\n",
    "\n",
    "  1) Features are broadcasted to number of modes and summed with learned mode\n",
    "      tensors.\n",
    "  2) Mixture Weights are generated by cross-attention over all dimensions\n",
    "      between learned mode tensors and hidden tensors.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_modes=5,\n",
    "               num_heads=8,\n",
    "               hidden_size=128,\n",
    "               drop_prob=0.1,\n",
    "               ln_eps=1e-6,\n",
    "               ff_dim=128):\n",
    "    super().__init__()\n",
    "    self.num_modes = num_modes\n",
    "    self.hidden_size = hidden_size\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(f'hidden_size ({hidden_size}) must be an integer '\n",
    "                       f'times bigger than num_heads ({num_heads}).')\n",
    "    self.mm_attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=3)\n",
    "    self.mm_attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.mm_ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.mm_ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.mm_ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    self.mw_attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=None)\n",
    "    self.mw_attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.mw_ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.mw_ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=1,  # Single logit per mode\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.mw_ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    self.attn_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    # [1, 1, m, h]\n",
    "    self.learned_add_mm = tf.Variable(\n",
    "        tf.random_uniform_initializer(\n",
    "            minval=-1.,\n",
    "            maxval=1.)(shape=[1, 1, self.num_modes, hidden_size]),\n",
    "        trainable=True,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "  def call(self, input_batch, training=None):\n",
    "    mask = input_batch[1]\n",
    "    # [b, t, 1, h]\n",
    "    hidden_vecs = input_batch[0][..., tf.newaxis, :]\n",
    "\n",
    "    # Multi Modes\n",
    "    mm_add = self.mm_attn_ln(self.learned_add_mm + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.mm_ff_layer1(mm_add)\n",
    "    out = self.mm_ff_layer2(out)\n",
    "    out = self.ff_dropout(out)\n",
    "    out = self.mm_ff_ln(out + hidden_vecs)\n",
    "\n",
    "    # Mixture Weights\n",
    "    # [b, 1, n, h]\n",
    "    b = tf.shape(out)[0]\n",
    "    attn_out_mw = self.mw_attn_layer(\n",
    "        query=tf.tile(self.learned_add_mm, [b, 1, 1, 1]),\n",
    "        key=mm_add,\n",
    "        value=mm_add,\n",
    "        return_attention_scores=False)\n",
    "    attn_out_mw = self.attn_dropout(attn_out_mw, training=training)\n",
    "\n",
    "    # [b, 1, 1, n, h]\n",
    "    attn_out_mw = self.mw_attn_ln(attn_out_mw)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out_mw = self.mw_ff_layer1(attn_out_mw)\n",
    "    out_mw = self.mw_ff_layer2(out_mw)\n",
    "    out_mw = self.ff_dropout(out_mw, training=training)\n",
    "\n",
    "    # [b, 1, n]\n",
    "    mixture_logits = out_mw[..., 0]\n",
    "    return out, mixture_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from 2D Predictions to 3D predictions\n",
    "class Prediction2DPositionHeadLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Converts transformer hidden vectors to model predictions.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_units=None, num_stages=5):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dense_layers = []\n",
    "    # Add hidden layers.\n",
    "    if hidden_units is not None:\n",
    "      for units in hidden_units:\n",
    "        self.dense_layers.append(\n",
    "            tf.keras.layers.Dense(units, activation='relu'))\n",
    "    # Add the final prediction head.\n",
    "    self.dense_layers.append(\n",
    "        tf.keras.layers.EinsumDense(\n",
    "            '...h,hf->...f',\n",
    "            output_shape=5,\n",
    "            bias_axes='f',\n",
    "            activation=None))\n",
    "\n",
    "  def call(self, input_batch):\n",
    "    # [b, t, n,  h]\n",
    "    hidden_vecs = input_batch\n",
    "\n",
    "    x = hidden_vecs\n",
    "    # [b, t, n, 5]\n",
    "    for layer in self.dense_layers:\n",
    "      x = layer(x)\n",
    "    pred = x\n",
    "    \"\"\"predictions = {\n",
    "        'agents/position': pred[..., 0:3],\n",
    "        'agents/position/raw_scale_tril': pred[..., 3:5],\n",
    "    }\"\"\"\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "(None, 15, 5, 128)\n",
      "(15,)\n",
      "(32, 15, 5, 128)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6d64392e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_length=15\n",
    "input_dim_1 = 3\n",
    "input_dim_2 = 51\n",
    "\n",
    "# Define a simple model that includes the custom layer\n",
    "input_1 = tf.keras.Input(shape=(input_length, input_dim_1))\n",
    "input_2 = tf.keras.Input(shape=(input_length, input_dim_2))\n",
    "\n",
    "hidden_size = 128\n",
    "masked_inputs, mask = PreprocessLayer()((input_1, input_2)) # output shape (batch_size, 15, 3)\n",
    "embedded_inputs_pos = AgentPositionEncoder(output_shape=hidden_size-8, embedding_size=hidden_size)(masked_inputs)\n",
    "embedded_inputs_pose = AgentKeypointsEncoder(output_shape=hidden_size-8, embedding_size=hidden_size)(masked_inputs)\n",
    "embedded_inputs_temp = AgentTemporalEncoder(output_shape=hidden_size-8, embedding_size=hidden_size, num_steps=input_length)(masked_inputs)\n",
    "\n",
    "encoded_agent, scores = FeatureAttnAgentEncoderLearnedLayer(input_length=input_length)((masked_inputs, mask))\n",
    "self_encoded_agent, scores2 = AgentSelfAlignmentLayer()((encoded_agent, mask))\n",
    "transformed1, scores3 = SelfAttnTransformerLayer(mask=True)((self_encoded_agent, mask))\n",
    "transformed2, scores4 = SelfAttnTransformerLayer(mask=True)((transformed1, mask))\n",
    "transformed3, logits = MultimodalityInduction()((transformed2, mask))\n",
    "transformed4, scores5 = SelfAttnTransformerLayer(mask=True, multimodality_induced=True)((transformed3, mask))\n",
    "transformed5, scores6 = SelfAttnModeTransformerLayer()(transformed4)\n",
    "transformed6, scores7 = SelfAttnTransformerLayer(mask=True, multimodality_induced=True)((transformed5, mask))\n",
    "transformed7, scores8 = SelfAttnModeTransformerLayer()(transformed6)\n",
    "pred = Prediction2DPositionHeadLayer()(transformed7)\n",
    "\n",
    "\n",
    "model1 = tf.keras.Model(inputs=(input_1, input_2), outputs=masked_inputs)\n",
    "model2 = tf.keras.Model(inputs=(input_1, input_2), outputs=pred)\n",
    "\n",
    "for (batch_x1, batch_x2) in train_dataset.take(1):\n",
    "    #output = model1.predict((batch_x1, batch_x2))\n",
    "    output2 = model2.predict((batch_x1, batch_x2))\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 15, 5, 5)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
