{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 11:44:46.359830: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-09 11:44:46.439523: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-09 11:44:46.442050: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 11:44:48.028912: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from train import build_model\n",
    "from preprocess_data import load_data\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 11:44:54.922640: W tensorflow/core/framework/dataset.cc:956] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2024-07-09 11:45:24.930469: W tensorflow/core/framework/dataset.cc:956] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset, test_dataset = load_data(data_path=\"/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/final_dataset.pkl\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n",
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n",
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "for (batch_x1, batch_x2) in train_dataset.take(1):\n",
    "        output2 = model.predict((batch_x1, batch_x2))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mask', 'position', 'position_raw_scale', 'mixture_logits', 'targets'])\n"
     ]
    }
   ],
   "source": [
    "print(output2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 15, 5, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2['position_raw_scale'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_models as tfm\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Create cosine learning rate schedule w/ warmup.\n",
    "def _get_learning_rate_schedule(\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    "    learning_rate: float,\n",
    "    alpha: float = 0.0) -> tf.keras.optimizers.schedules.LearningRateSchedule:\n",
    "  \"\"\"Returns a cosine decay learning rate schedule to be used in training.\n",
    "\n",
    "  Args:\n",
    "    warmup_steps: Number of training steps to apply warmup. If global_step <\n",
    "      warmup_steps, the learning rate will be `global_step/num_warmup_steps *\n",
    "      init_lr`.\n",
    "    total_steps: The total number of training steps.\n",
    "    learning_rate: The peak learning rate anytime during the training.\n",
    "    alpha: The alpha parameter forwarded to CosineDecay\n",
    "\n",
    "  Returns:\n",
    "    A CosineDecay learning schedule w/ warmup.\n",
    "  \"\"\"\n",
    "\n",
    "  decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "      initial_learning_rate=learning_rate, decay_steps=total_steps, alpha=alpha)\n",
    "  return tfm.optimization.LinearWarmup(decay_schedule, warmup_steps, 1e-10)\n",
    "\n",
    "learning_rate_schedule = _get_learning_rate_schedule(\n",
    "    warmup_steps=5e4, total_steps=5e6,\n",
    "    learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate_schedule,\n",
    "    global_clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "  \"\"\"Base class for Human Scene Transformer Losses.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, name='Loss', clip_loss_max=tf.float32.max):\n",
    "    self.clip_loss_max = clip_loss_max\n",
    "    self.name = name\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, input_batch, predictions):\n",
    "    return self.call(input_batch, predictions)\n",
    "\n",
    "  def call(self, input_batch, predictions):\n",
    "    \"\"\"Calculates loss for fields which should be predicted.\"\"\"\n",
    "\n",
    "    # [b, a, t]\n",
    "    ##should_predict = input_batch['should_predict'][..., 0]\n",
    "    should_predict = tf.math.logical_not(predictions['mask'])\n",
    "    # [b, a, t]\n",
    "    loss_per_batch = self.get_per_batch_loss(input_batch, predictions)\n",
    "\n",
    "    loss_per_batch = tfp.math.clip_by_value_preserve_gradient(\n",
    "        loss_per_batch,\n",
    "        tf.float32.min,\n",
    "        self.clip_loss_max)\n",
    "\n",
    "    # Compute loss only on positions w/ should_predict == True.\n",
    "    print(\"should predict Loss:\", should_predict.shape)\n",
    "    should_predict_ind = tf.where(should_predict)\n",
    "    loss_should_predict_mat = tf.gather_nd(\n",
    "        params=loss_per_batch, indices=should_predict_ind)\n",
    "\n",
    "    loss_should_predict = tf.reduce_mean(loss_should_predict_mat)\n",
    "    # If there are no agents to be predicted xyz_loss_should_predict can be NaN\n",
    "    loss_should_predict = tf.math.multiply_no_nan(\n",
    "        loss_should_predict,\n",
    "        tf.cast(tf.math.reduce_any(should_predict), tf.float32))\n",
    "\n",
    "    loss_dict = {\n",
    "        'loss': loss_should_predict,\n",
    "        f'{self.name}_loss': loss_should_predict\n",
    "    }\n",
    "    return loss_dict\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    raise NotImplementedError\n",
    "\n",
    "class MinNLLPositionMixtureCategoricalCrossentropyLoss(Loss):\n",
    "  \"\"\"MinNLLPositionNLLLoss and MixtureCategoricalCrossentropyLoss.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(name='MinNLLMixture', **kwargs)\n",
    "    self.position_loss_obj = MinNLLPositionLoss()\n",
    "    self.mixture_loss_obj = MinNLLMixtureCategoricalCrossentropyLoss()\n",
    "\n",
    "  def call(self, input_batch, predictions):\n",
    "    position_loss = self.position_loss_obj(input_batch, predictions)\n",
    "    print(\"position loss: \", position_loss)\n",
    "    mixture_loss = self.mixture_loss_obj(input_batch, predictions)\n",
    "    print(\"mixture loss: \", mixture_loss)\n",
    "\n",
    "    loss = position_loss['loss'] + mixture_loss['loss']\n",
    "\n",
    "    print(\"loss: \", loss)\n",
    "\n",
    "    loss_dict = {**position_loss, **mixture_loss}\n",
    "\n",
    "    loss_dict['loss'] = loss\n",
    "\n",
    "    return loss_dict\n",
    "  \n",
    "class PositionNLLLoss(Loss):\n",
    "  \"\"\"Position NLL Loss for human trajectory predictions.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(name='position', **kwargs)\n",
    "\n",
    "  # from model.output_distributions\n",
    "  def force_positive(self, x, eps=1e-6):\n",
    "    return tf.keras.activations.elu(x) + 1. + eps\n",
    "\n",
    "  # from model.output_distributions\n",
    "  def to_positive_definite_scale_tril(self, logit_sigma):\n",
    "    tril = tfp.math.fill_triangular(logit_sigma)\n",
    "    scale_tril = tf.linalg.set_diag(\n",
    "        tril,\n",
    "        self.force_positive(tf.linalg.diag_part(tril)))\n",
    "    return scale_tril\n",
    "  \n",
    "  # from model.output_distributions\n",
    "  def get_position_distribution(self, model_output):\n",
    "    \"\"\"Multivariate Normal distribution over position.\"\"\"\n",
    "    p_pos = tfp.distributions.MultivariateNormalTriL(\n",
    "        loc=model_output['position'],\n",
    "        scale_tril=self.to_positive_definite_scale_tril(\n",
    "            model_output['position_raw_scale']))\n",
    "    return p_pos\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    \"\"\"Negative log probability of ground truth with respect to predictions.\"\"\"\n",
    "    # [b, a, t, 3]\n",
    "    p_position = self.get_position_distribution(predictions)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    position_nll = -p_position.log_prob(\n",
    "        predictions['targets'])\n",
    "    return position_nll\n",
    "  \n",
    "class MinNLLPositionLoss(PositionNLLLoss):\n",
    "  \"\"\"MinNLLPositionNLL loss.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.mixture_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    \"\"\"Negative log probability of mode with smallest ADE.\"\"\"\n",
    "    predictions = predictions.copy()\n",
    "\n",
    "    predictions['targets'] = predictions['targets'][..., tf.newaxis, :]\n",
    "    # [b, a, t, n]\n",
    "    position_nll = super().get_per_batch_loss(input_batch, predictions)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    ##should_predict = tf.cast(input_batch['should_predict'], tf.float32)\n",
    "    should_predict = tf.math.logical_not(predictions['mask'])\n",
    "    should_predict = tf.expand_dims(should_predict, axis=-1)\n",
    "    should_predict = tf.cast(should_predict, tf.float32)\n",
    "\n",
    "    # [b, a, t, n, 1]\n",
    "    per_position_nll = (\n",
    "        position_nll[..., tf.newaxis] * should_predict[..., tf.newaxis, :]\n",
    "    )\n",
    "\n",
    "    # Get mode with minimum NLL\n",
    "    # [b, a, n, 1]\n",
    "    per_mode_nll_sum = tf.reduce_sum(per_position_nll, axis=2)\n",
    "\n",
    "    t = tf.shape(position_nll)[2]\n",
    "\n",
    "    # [b, a, 1]\n",
    "    min_nll_indices = tf.math.argmin(per_mode_nll_sum, axis=-2)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    min_nll_indices_tiled = tf.tile(\n",
    "        min_nll_indices[..., tf.newaxis], [1, t, 1])\n",
    "\n",
    "    # [b, a, t]\n",
    "    position_nll_min_ade = tf.gather(\n",
    "        position_nll, indices=min_nll_indices_tiled, batch_dims=2, axis=-1\n",
    "        )[..., 0]\n",
    "\n",
    "    return position_nll_min_ade\n",
    "\n",
    "class MinNLLMixtureCategoricalCrossentropyLoss(PositionNLLLoss):\n",
    "  \"\"\"Categorical Corssentropy Loss for Mixture Weight.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.mixture_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    \"\"\"Negative log probability of ground truth with respect to predictions.\"\"\"\n",
    "    predictions= predictions.copy()\n",
    "    predictions['targets'] = predictions[\n",
    "        'targets'][..., tf.newaxis, :]\n",
    "\n",
    "    # Calculate ADE\n",
    "    # [b, a, t, n, 1]\n",
    "    position_nll = super().get_per_batch_loss(input_batch, predictions)\n",
    "    position_nll = tf.expand_dims(position_nll, axis=-1)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    ##should_predict = tf.cast(input_batch['should_predict'], tf.float32)\n",
    "    should_predict = tf.cast(tf.expand_dims(tf.math.logical_not(predictions['mask']), axis=-1), tf.float32)\n",
    "\n",
    "    # [b, a, t, n, 1]\n",
    "    per_position_nll = (\n",
    "        position_nll * should_predict[..., tf.newaxis, :]\n",
    "    )\n",
    "\n",
    "    # Get mode with minimum NLL\n",
    "    # [b, a, n, 1]\n",
    "    ##per_mode_nll_sum = tf.reduce_sum(per_position_nll, axis=2)\n",
    "    per_mode_nll_sum = tf.reduce_sum(per_position_nll, axis=1)\n",
    "\n",
    "    a = tf.shape(position_nll)[1]\n",
    "    #n = tf.shape(position_nll)[3]\n",
    "    n = tf.shape(position_nll)[2]\n",
    "\n",
    "    # [b, a, 1]\n",
    "    min_nll_indices = tf.math.argmin(per_mode_nll_sum, axis=-2)\n",
    "\n",
    "    # [b, a, n]\n",
    "    min_nll_indices_one_hot = tf.one_hot(min_nll_indices[..., 0], n)\n",
    "\n",
    "    # [b, a]\n",
    "    # TODO\n",
    "    #mixture_loss = self.mixture_loss(\n",
    "        #min_nll_indices_one_hot,\n",
    "        #tf.tile(predictions['mixture_logits'][..., 0, :], [1, a, 1]))\n",
    "    \n",
    "    mixture_loss = self.mixture_loss(\n",
    "        min_nll_indices_one_hot,\n",
    "        predictions['mixture_logits'][..., 0, :])\n",
    "\n",
    "    return mixture_loss\n",
    "\n",
    "  def call(self, input_batch, predictions):\n",
    "    \"\"\"Calculates loss.\"\"\"\n",
    "\n",
    "    # [b, a]\n",
    "    ##should_predict = tf.reduce_any(\n",
    "      ##  input_batch['should_predict'][..., 0], axis=-1)\n",
    "    should_predict = tf.reduce_any(\n",
    "        tf.math.logical_not(predictions['mask'][..., 0]), axis=-1)\n",
    "    # [b, a]\n",
    "    loss_per_batch = self.get_per_batch_loss(input_batch, predictions)\n",
    "\n",
    "    # Compute loss only on positions w/ should_predict == True.\n",
    "    print(\"predictions: \", predictions['mask'].shape)\n",
    "    print(\"shuld_predict: \", should_predict.shape)\n",
    "    should_predict_ind = tf.where(should_predict)\n",
    "    loss_should_predict_mat = tf.gather_nd(\n",
    "        params=loss_per_batch, indices=should_predict_ind)\n",
    "\n",
    "    loss_should_predict = tf.reduce_mean(loss_should_predict_mat)\n",
    "    # If there are no agents to be predicted xyz_loss_should_predict can be NaN\n",
    "    loss_should_predict = tf.math.multiply_no_nan(\n",
    "        loss_should_predict,\n",
    "        tf.cast(tf.math.reduce_any(should_predict), tf.float32))\n",
    "\n",
    "    # Mixture weights are per scene. So we do not have to mask anything\n",
    "    loss_dict = {\n",
    "        'loss': loss_should_predict,\n",
    "        f'{self.name}_loss': loss_should_predict\n",
    "    }\n",
    "    return loss_dict\n",
    "  \n",
    "loss_obj = MinNLLPositionMixtureCategoricalCrossentropyLoss()\n",
    "#loss = loss_obj(input_batch=batch_x1, predictions=output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 15, 3)]              0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 15, 51)]             0         []                            \n",
      "                                                                                                  \n",
      " preprocess_layer (Preproce  (((32, 15, 3),               0         ['input_1[0][0]',             \n",
      " ssLayer)                     (32, 15, 51)),                         'input_2[0][0]']             \n",
      "                              (32, 15),                                                           \n",
      "                              (32, 15, 3))                                                        \n",
      "                                                                                                  \n",
      " feature_attn_agent_encoder  ((32, 15, 128),              453896    ['preprocess_layer[0][0]',    \n",
      " _learned_layer (FeatureAtt   (32, 4, 15, 15))                       'preprocess_layer[0][1]',    \n",
      " nAgentEncoderLearnedLayer)                                          'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " agent_self_alignment_layer  ((32, 15, 128),              99712     ['feature_attn_agent_encoder_l\n",
      "  (AgentSelfAlignmentLayer)   (32, 8, 15, 15))                      earned_layer[0][0]',          \n",
      "                                                                     'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 128),              99584     ['agent_self_alignment_layer[0\n",
      " r (SelfAttnTransformerLaye   (32, 8, 15, 15))                      ][0]',                        \n",
      " r)                                                                  'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 128),              99584     ['self_attn_transformer_layer[\n",
      " r_1 (SelfAttnTransformerLa   (32, 8, 15, 15))                      0][0]',                       \n",
      " yer)                                                                'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " multimodality_induction (M  ((32, 15, 5, 128),           117121    ['self_attn_transformer_layer_\n",
      " ultimodalityInduction)       (32, 1, 5))                           1[0][0]',                     \n",
      "                                                                     'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 5, 128),           99584     ['multimodality_induction[0][0\n",
      " r_2 (SelfAttnTransformerLa   (32, 5, 8, 15, 15))                   ]',                           \n",
      " yer)                                                                'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_mode_transformer  ((32, 15, 5, 128),           99584     ['self_attn_transformer_layer_\n",
      " _layer (SelfAttnModeTransf   (32, 15, 8, 5, 5))                    2[0][0]']                     \n",
      " ormerLayer)                                                                                      \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 5, 128),           99584     ['self_attn_mode_transformer_l\n",
      " r_3 (SelfAttnTransformerLa   (32, 5, 8, 15, 15))                   ayer[0][0]',                  \n",
      " yer)                                                                'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_mode_transformer  ((32, 15, 5, 128),           99584     ['self_attn_transformer_layer_\n",
      " _layer_1 (SelfAttnModeTran   (32, 15, 8, 5, 5))                    3[0][0]']                     \n",
      " sformerLayer)                                                                                    \n",
      "                                                                                                  \n",
      " prediction2d_position_head  (32, 15, 5, 9)               1161      ['self_attn_mode_transformer_l\n",
      " _layer (Prediction2DPositi                                         ayer_1[0][0]']                \n",
      " onHeadLayer)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (32, 15, 5, 3)               0         ['prediction2d_position_head_l\n",
      " SlicingOpLambda)                                                   ayer[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  (32, 15, 5, 6)               0         ['prediction2d_position_head_l\n",
      "  (SlicingOpLambda)                                                 ayer[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1269394 (4.84 MB)\n",
      "Trainable params: 1269394 (4.84 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics.\n",
    "\n",
    "class Mean(tf.keras.metrics.Mean):\n",
    "\n",
    "  def __init__(self, name=\"mean\", dtype=None):\n",
    "    super().__init__(name=name, dtype=dtype)\n",
    "\n",
    "def distance_error(target: tf.Tensor, pred: tf.Tensor) -> tf.Tensor:\n",
    "  return tf.sqrt(\n",
    "      tf.reduce_sum(tf.square(pred - target), axis=-1, keepdims=True))\n",
    "\n",
    "class ADE(tf.keras.metrics.Metric):\n",
    "  \"\"\"Average Displacement Error over a n dimensional track.\n",
    "\n",
    "  Calculates the mean L2 distance over all predicted timesteps.\n",
    "  \"\"\"\n",
    "  # TODO: timestep?\n",
    "  def __init__(self, num_history_steps=6, timestep=0.4, cutoff_seconds=None, at_cutoff=False, name='ADE'):\n",
    "    \"\"\"Initializes the ADE metric.\n",
    "\n",
    "    Args:\n",
    "      params: ModelParams\n",
    "      cutoff_seconds: Cutoff up to which time the metric should be calculated\n",
    "        in seconds.\n",
    "      at_cutoff: If True metric will be calculated at cutoff timestep.\n",
    "        Otherwise metric is calculated as average up to cutoff_seconds.\n",
    "      name: Metric name.\n",
    "    \"\"\"\n",
    "    super().__init__(name=name)\n",
    "    self.cutoff_seconds = cutoff_seconds\n",
    "    self.at_cutoff = at_cutoff\n",
    "    if cutoff_seconds is None:\n",
    "      self.cutoff_idx = None\n",
    "    else:\n",
    "      # +1 due to current time step.\n",
    "      self.cutoff_idx = int(\n",
    "          num_history_steps +\n",
    "          cutoff_seconds / timestep) + 1\n",
    "\n",
    "    self.num_predictions = self.add_weight(\n",
    "        name='num_predictions', initializer='zeros')\n",
    "    self.total_deviation = self.add_weight(\n",
    "        name='total_deviation', initializer='zeros')\n",
    "\n",
    "  def _reduce(self, ade_with_modes, input_batch, predictions):\n",
    "    \"\"\"Reduces mode dimension. The base class squeezes a single mode.\"\"\"\n",
    "    return tf.squeeze(ade_with_modes, axis=-1)\n",
    "\n",
    "  def update_state(self, input_batch, predictions):\n",
    "    should_predict = tf.cast(tf.math.logical_not(predictions['mask']), tf.float32)\n",
    "    should_predict = tf.expand_dims(should_predict, axis=-1)\n",
    "\n",
    "    target = predictions['targets']\n",
    "    target = target[..., :predictions['position'].shape[-1]]\n",
    "    # [b, a, t, n, 3] -> [b, a, t, n, 1]\n",
    "    per_position_ade = distance_error(\n",
    "        target[..., tf.newaxis, :],\n",
    "        predictions['position'])\n",
    "\n",
    "    # Non-observed or past should not contribute to ade.\n",
    "    deviation = tf.math.multiply_no_nan(per_position_ade,\n",
    "                                        should_predict[..., tf.newaxis, :])\n",
    "    # Chop off the un-wanted time part.\n",
    "    # [b, a, cutoff_idx, 1]\n",
    "    if self.at_cutoff and self.cutoff_seconds is not None:\n",
    "      deviation = deviation[:, self.cutoff_idx-1:self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(\n",
    "          should_predict[:, self.cutoff_idx-1:self.cutoff_idx, :])\n",
    "    else:\n",
    "      deviation = deviation[:, :self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(should_predict[:, :self.cutoff_idx, :])\n",
    "    \n",
    "    # Reduce along time\n",
    "    deviation = tf.reduce_sum(deviation, axis=1)\n",
    "    # Reduce along modes\n",
    "    deviation = self._reduce(deviation, input_batch, predictions)\n",
    "    # [1]\n",
    "    deviation = tf.reduce_sum(deviation)\n",
    "\n",
    "    self.num_predictions.assign_add(num_predictions)\n",
    "    self.total_deviation.assign_add(deviation)\n",
    "\n",
    "  def result(self):\n",
    "    return self.total_deviation / self.num_predictions\n",
    "\n",
    "  def reset_states(self):\n",
    "    self.num_predictions.assign(0)\n",
    "    self.total_deviation.assign(0.0)\n",
    "\n",
    "class MinADE(ADE):\n",
    "  \"\"\"Takes the minimum over all modes.\"\"\"\n",
    "\n",
    "  def _reduce(self, ade_with_modes, input_batch, predictions):\n",
    "    return tf.reduce_min(ade_with_modes, axis=-2)\n",
    "\n",
    "class MLADE(ADE):\n",
    "  \"\"\"Takes the maximum likelihood mode.\"\"\"\n",
    "\n",
    "  def _reduce(self, ade_with_modes, input_batch, predictions):\n",
    "    # Get index of mixture component with highest probability\n",
    "    # [b, a=1, t=1, n]\n",
    "    ml_indices = tf.math.argmax(predictions['mixture_logits'], axis=-1)\n",
    "    a = ade_with_modes.shape[1]\n",
    "    ml_indices = tf.tile(\n",
    "        tf.squeeze(ml_indices, axis=1), [a])[..., tf.newaxis]\n",
    "\n",
    "    return tf.gather(\n",
    "        ade_with_modes, indices=ml_indices, batch_dims=2, axis=-2)[..., 0, :]\n",
    "\n",
    "def force_positive(x, eps=1e-6):\n",
    "  return tf.keras.activations.elu(x) + 1. + eps\n",
    "\n",
    "def to_positive_definite_scale_tril(logit_sigma):\n",
    "  tril = tfp.math.fill_triangular(logit_sigma)\n",
    "  scale_tril = tf.linalg.set_diag(\n",
    "      tril,\n",
    "      force_positive(tf.linalg.diag_part(tril)))\n",
    "  return scale_tril\n",
    "\n",
    "def get_position_distribution(model_output):\n",
    "  \"\"\"Multivariate Normal distribution over position.\"\"\"\n",
    "  p_pos = tfp.distributions.MultivariateNormalTriL(\n",
    "      loc=model_output['position'],\n",
    "      scale_tril=to_positive_definite_scale_tril(\n",
    "          model_output['position_raw_scale']))\n",
    "\n",
    "  return p_pos\n",
    "\n",
    "def get_multimodal_position_distribution(model_output):\n",
    "  \"\"\"Multivariate Normal Mixture distribution over position.\"\"\"\n",
    "  p_pos = get_position_distribution(model_output)\n",
    "\n",
    "  p_pos_mm = tfp.distributions.MixtureSameFamily(\n",
    "      mixture_distribution=tfp.distributions.Categorical(\n",
    "          logits=model_output['mixture_logits']),\n",
    "      components_distribution=p_pos)\n",
    "\n",
    "  return p_pos_mm\n",
    "\n",
    "class PositionNegativeLogLikelihood(tf.keras.metrics.Metric):\n",
    "  \"\"\"Position Negative Log Likelihood.\"\"\"\n",
    "\n",
    "  def __init__(self, num_history_steps=6, timestep=0.4,cutoff_seconds=None, at_cutoff=False,\n",
    "               name='PosNLL'):\n",
    "    \"\"\"Initializes the PositionNegativeLogLikelihood metric.\n",
    "\n",
    "    Args:\n",
    "      params: ModelParams\n",
    "      cutoff_seconds: Cutoff up to which time the metric should be calculated\n",
    "        in seconds.\n",
    "      at_cutoff: If True metric will be calculated at cutoff timestep.\n",
    "        Otherwise metric is calculated as average up to cutoff_seconds.\n",
    "      name: Metric name.\n",
    "    \"\"\"\n",
    "    super().__init__(name=name)\n",
    "    self.cutoff_seconds = cutoff_seconds\n",
    "    self.at_cutoff = at_cutoff\n",
    "    if cutoff_seconds is None:\n",
    "      self.cutoff_idx = None\n",
    "    else:\n",
    "      # +1 due to current time step.\n",
    "      self.cutoff_idx = int(\n",
    "          num_history_steps +\n",
    "          cutoff_seconds / timestep) + 1\n",
    "\n",
    "    self.num_predictions = self.add_weight(\n",
    "        name='num_predictions', initializer='zeros')\n",
    "    self.total_deviation = self.add_weight(\n",
    "        name='total_deviation', initializer='zeros')\n",
    "\n",
    "  def update_state(self, input_batch, predictions):\n",
    "    should_predict = tf.cast(tf.math.logical_not(predictions['mask']), tf.float32)\n",
    "    should_predict = tf.expand_dims(should_predict, axis=-1)\n",
    "\n",
    "    p_pos = get_multimodal_position_distribution(predictions)\n",
    "\n",
    "    target = predictions['targets']\n",
    "    target = target[..., :p_pos.event_shape_tensor()[0]]\n",
    "    print(target.shape)\n",
    "\n",
    "    # [b, a, t, n, 1]\n",
    "    per_position_nll = -p_pos.log_prob(target)[..., tf.newaxis]\n",
    "\n",
    "    # Non-observed or past should not contribute to metric.\n",
    "    nll = tf.math.multiply_no_nan(per_position_nll, should_predict)\n",
    "    # Chop off the un-wanted time part.\n",
    "    # [b, a, cutoff_idx, 1]\n",
    "    if self.at_cutoff and self.cutoff_seconds is not None:\n",
    "      nll = nll[:, self.cutoff_idx-1:self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(\n",
    "          should_predict[:, self.cutoff_idx-1::self.cutoff_idx, :])\n",
    "    else:\n",
    "      nll = nll[:, :self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(should_predict[:, :self.cutoff_idx, :])\n",
    "\n",
    "    # [1]\n",
    "    nll = tf.reduce_sum(nll)\n",
    "\n",
    "    self.num_predictions.assign_add(num_predictions)\n",
    "    self.total_deviation.assign_add(nll)\n",
    "\n",
    "  def result(self):\n",
    "    return self.total_deviation / self.num_predictions\n",
    "\n",
    "  def reset_states(self):\n",
    "    self.num_predictions.assign(0)\n",
    "    self.total_deviation.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 13:33:45.167485: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2783\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\030TensorSliceDataset:14786\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 15\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2024-07-09 13:33:45.186754: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2783\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\030TensorSliceDataset:14786\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 15\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "model_base_dir = \"\"\n",
    "dt_str = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "model_dir = os.path.join(model_base_dir, dt_str)\n",
    "os.makedirs(model_dir)\n",
    "ckpt_dir = os.path.join(model_dir, 'ckpts')\n",
    "os.makedirs(ckpt_dir)\n",
    "ckpt_best_dir = os.path.join(model_dir, 'ckpts_best')\n",
    "os.makedirs(ckpt_best_dir)\n",
    "checkpoint_prefix = os.path.join(ckpt_dir, 'ckpt')\n",
    "checkpoint_prefix_best = os.path.join(ckpt_best_dir, 'ckpt')\n",
    "tensorboard_dir = '/tmp/tensorboard'\n",
    "\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "      os.path.join(tensorboard_dir, 'train'))\n",
    "eval_summary_writer = tf.summary.create_file_writer(\n",
    "    os.path.join(tensorboard_dir, 'eval'))\n",
    "\n",
    "\n",
    "batches_per_train_step=1 #25000\n",
    "batches_per_eval_step =1 # 2000\n",
    "eval_every_n_step = 1 #1e4\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy('cpu')\n",
    "\n",
    "dist_train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "dist_eval_dataset = strategy.experimental_distribute_dataset(test_dataset)\n",
    "\n",
    "current_global_step = 0\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate_schedule,\n",
    "        global_clipnorm=1.)\n",
    "    loss_obj = MinNLLPositionMixtureCategoricalCrossentropyLoss()\n",
    "    train_metrics = {\n",
    "    'loss': Mean(),\n",
    "    'loss_position': Mean(),\n",
    "    'min_ade': MinADE(),\n",
    "    'ml_ade': MLADE(),\n",
    "    'pos_nll': PositionNegativeLogLikelihood()\n",
    "    }\n",
    "\n",
    "    eval_metrics = {\n",
    "      'loss': Mean(),\n",
    "      'loss_position': Mean(),\n",
    "      'min_ade': MinADE(),\n",
    "      'ml_ade': MLADE(),\n",
    "      'pos_nll': PositionNegativeLogLikelihood()\n",
    "    }\n",
    "\n",
    "best_eval_loss = tf.Variable(tf.float32.max)\n",
    "checkpoint = tf.train.Checkpoint(model=model,\n",
    "                                   optimizer=optimizer,\n",
    "                                   best_eval_loss=best_eval_loss)\n",
    "checkpoint_best = tf.train.Checkpoint(model=model)\n",
    "best_checkpoint_manager = tf.train.CheckpointManager(checkpoint_best,\n",
    "                                                    checkpoint_prefix_best,\n",
    "                                                    max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training and Eval tf.function.\n",
    "@tf.function\n",
    "def train_step(iterator):\n",
    "    \"\"\"Training function.\"\"\"\n",
    "\n",
    "    def step_fn(input_batch):\n",
    "        print(\"input batch \", input_batch[0].shape)\n",
    "        with tf.GradientTape() as tape:\n",
    "            print(\"predict\")\n",
    "            predictions = model(input_batch, training=True)\n",
    "            print(\"loss\")\n",
    "            loss_dict = loss_obj(input_batch, predictions)\n",
    "            loss = (loss_dict['loss']\n",
    "                / tf.cast(strategy.num_replicas_in_sync, tf.float32))\n",
    "        print(\"grad\")\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        print(\"optimizer\")\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        # Update the training metrics.\n",
    "        # These need special treatments as they are standard keras metrics.\n",
    "        print(\"train metrics\")\n",
    "        train_metrics['loss'].update_state(loss_dict['loss'])\n",
    "        print(\"train metrics2\")\n",
    "        train_metrics['loss_position'].update_state(loss_dict['position_loss'])\n",
    "        # Our own metrics.\n",
    "        for key in train_metrics:\n",
    "            if key in {'loss', 'loss_position', 'loss_orientation'}:\n",
    "                continue\n",
    "            print(\"key \", key)\n",
    "            train_metrics[key].update_state(input_batch, predictions)\n",
    "        print(\"fertig\")\n",
    "\n",
    "    for _ in tf.range(tf.constant(batches_per_train_step)):\n",
    "        strategy.run(\n",
    "            step_fn,\n",
    "            args=(next(iterator),),\n",
    "            options=tf.distribute.RunOptions(\n",
    "                experimental_enable_dynamic_batch_size=False))\n",
    "\n",
    "@tf.function\n",
    "def eval_step(iterator):\n",
    "\n",
    "    def step_fn(input_batch):\n",
    "        print(\"eval step\")\n",
    "        predictions = model(input_batch, training=False)\n",
    "        loss_dict = loss_obj(input_batch, predictions)\n",
    "        # Update the eval metrics.\n",
    "        # These need special treatments as they are standard keras metrics.\n",
    "        eval_metrics['loss'].update_state(loss_dict['loss'])\n",
    "        eval_metrics['loss_position'].update_state(loss_dict['position_loss'])\n",
    "        # Our own metrics.\n",
    "        for key in eval_metrics:\n",
    "            if key in {'loss', 'loss_position', 'loss_orientation'}:\n",
    "                continue\n",
    "            eval_metrics[key].update_state(input_batch, predictions)\n",
    "\n",
    "    for _ in tf.range(tf.constant(batches_per_eval_step)):\n",
    "        strategy.run(\n",
    "            step_fn,\n",
    "            args=(next(iterator),),\n",
    "            options=tf.distribute.RunOptions(\n",
    "                experimental_enable_dynamic_batch_size=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 13:33:53.450222: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range:  range(0, 2)\n",
      "step:  0\n",
      "train step\n",
      "input batch  (None, 15, 3)\n",
      "predict\n",
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n",
      "loss\n",
      "position loss:  {'loss': <tf.Tensor 'PartitionedCall:0' shape=() dtype=float32>, 'position_loss': <tf.Tensor 'PartitionedCall:1' shape=() dtype=float32>}\n",
      "predictions:  (32, 15)\n",
      "shuld_predict:  ()\n",
      "mixture loss:  {'loss': <tf.Tensor 'PartitionedCall_1:0' shape=() dtype=float32>, 'position_loss': <tf.Tensor 'PartitionedCall_1:1' shape=() dtype=float32>}\n",
      "loss:  Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "grad\n",
      "optimizer\n",
      "train metrics\n",
      "train metrics2\n",
      "key  min_ade\n",
      "key  ml_ade\n",
      "key  pos_nll\n",
      "(32, 15, 3)\n",
      "fertig\n",
      "input batch  (None, 15, 3)\n",
      "predict\n",
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n",
      "loss\n",
      "grad\n",
      "optimizer\n",
      "train metrics\n",
      "train metrics2\n",
      "key  min_ade\n",
      "key  ml_ade\n",
      "key  pos_nll\n",
      "(32, 15, 3)\n",
      "fertig\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'Where' defined at (most recent call last):\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_123269/1326693165.py\", line 21, in <module>\n      train_step(train_iter)\n    File \"/tmp/ipykernel_123269/2227693014.py\", line 33, in train_step\n      for _ in tf.range(tf.constant(batches_per_train_step)):\n    File \"/tmp/ipykernel_123269/2227693014.py\", line 34, in train_step\n      strategy.run(\n    File \"/tmp/ipykernel_123269/2227693014.py\", line 12, in step_fn\n      loss_dict = loss_obj(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1512843509.py\", line 11, in __call__\n      return self.call(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1512843509.py\", line 58, in call\n      mixture_loss = self.mixture_loss_obj(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1512843509.py\", line 11, in __call__\n      return self.call(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1616832195.py\", line 224, in call\n      should_predict_ind = tf.where(should_predict)\nNode: 'Where'\nWhereOp : Unhandled input dimensions: 0\n\t [[{{node Where}}]] [Op:__inference_train_step_223534]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Run training SGD over train_param.batches_per_train_step batches.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# optimizer.iterations = step * train_param.batches_per_train_step.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtrain step\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     train_step(train_iter)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mout\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X20sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Writing metrics to tensorboard.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'Where' defined at (most recent call last):\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 18, in <module>\n      app.launch_new_instance()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n      app.start()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 739, in start\n      self.io_loop.start()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 205, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/home/pbr-student/miniconda3/envs/openmmlab/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n      await self.process_one()\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n      await dispatch(*args)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n      await result\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 359, in execute_request\n      await super().execute_request(stream, ident, parent)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n      reply_content = await reply_content\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 446, in do_execute\n      res = shell.run_cell(\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/pbr-student/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_123269/1326693165.py\", line 21, in <module>\n      train_step(train_iter)\n    File \"/tmp/ipykernel_123269/2227693014.py\", line 33, in train_step\n      for _ in tf.range(tf.constant(batches_per_train_step)):\n    File \"/tmp/ipykernel_123269/2227693014.py\", line 34, in train_step\n      strategy.run(\n    File \"/tmp/ipykernel_123269/2227693014.py\", line 12, in step_fn\n      loss_dict = loss_obj(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1512843509.py\", line 11, in __call__\n      return self.call(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1512843509.py\", line 58, in call\n      mixture_loss = self.mixture_loss_obj(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1512843509.py\", line 11, in __call__\n      return self.call(input_batch, predictions)\n    File \"/tmp/ipykernel_123269/1616832195.py\", line 224, in call\n      should_predict_ind = tf.where(should_predict)\nNode: 'Where'\nWhereOp : Unhandled input dimensions: 0\n\t [[{{node Where}}]] [Op:__inference_train_step_223534]"
     ]
    }
   ],
   "source": [
    " # 5) Actual Training Loop\n",
    "train_iter = iter(dist_train_dataset)\n",
    "eval_iter = iter(dist_eval_dataset)\n",
    "total_train_steps = 2 # 1e6\n",
    "num_train_iter = (\n",
    "    total_train_steps // batches_per_train_step)\n",
    "current_train_iter = (\n",
    "    current_global_step // batches_per_train_step)\n",
    "\n",
    "print(\"range: \",range(current_train_iter, num_train_iter))\n",
    "\n",
    "logging.info('Beginning training.')\n",
    "for step in range(current_train_iter, num_train_iter):\n",
    "    print(\"step: \", step)\n",
    "    # Actual number of SGD steps.\n",
    "    actual_steps = step * batches_per_train_step\n",
    "    with train_summary_writer.as_default():\n",
    "        # Run training SGD over train_param.batches_per_train_step batches.\n",
    "        # optimizer.iterations = step * train_param.batches_per_train_step.\n",
    "        print(\"train step\")\n",
    "        train_step(train_iter)\n",
    "        print(\"out\")\n",
    "        # Writing metrics to tensorboard.\n",
    "        if step % 1 == 0:\n",
    "            for key in train_metrics:\n",
    "                print(train_metrics[key].result())\n",
    "                tf.summary.scalar(\n",
    "                    key, train_metrics[key].result(), step=optimizer.iterations)\n",
    "\n",
    "            if isinstance(optimizer, tf.keras.optimizers.experimental.Optimizer):\n",
    "                learning_rate = optimizer.learning_rate\n",
    "            else:\n",
    "                learning_rate = optimizer.lr(optimizer.iterations)\n",
    "            tf.summary.scalar(\n",
    "                'learning_rate',\n",
    "                learning_rate,\n",
    "                step=optimizer.iterations)\n",
    "            logging.info('Training step %d', step)\n",
    "            print('Training loss: %.4f, ADE: %.4f',\n",
    "                            train_metrics['loss'].result().numpy(),\n",
    "                            train_metrics['min_ade'].result().numpy())\n",
    "            # Reset metrics.\n",
    "            for key in train_metrics:\n",
    "                train_metrics[key].reset_states()\n",
    "\n",
    "    # Evaluation.\n",
    "    if actual_steps % eval_every_n_step == 0:\n",
    "        print(\"eval\")\n",
    "        logging.info('Evaluating step %d over %d random eval samples', step,\n",
    "                    batches_per_eval_step * batch_size)\n",
    "        with eval_summary_writer.as_default():\n",
    "            eval_step(eval_iter)\n",
    "            for key in eval_metrics:\n",
    "                tf.summary.scalar(\n",
    "                    key, eval_metrics[key].result(), step=optimizer.iterations)\n",
    "            logging.info('Eval loss: %.4f, ADE: %.4f',\n",
    "                            eval_metrics['loss'].result().numpy(),\n",
    "                            eval_metrics['min_ade'].result().numpy())\n",
    "\n",
    "            if eval_metrics['loss'].result() < best_eval_loss:\n",
    "                best_eval_loss.assign(eval_metrics['loss'].result())\n",
    "                best_checkpoint_manager.save()\n",
    "\n",
    "            # Reset metrics.\n",
    "            for key in eval_metrics:\n",
    "                eval_metrics[key].reset_states()\n",
    "\n",
    "            # Save model.\n",
    "            checkpoint_name = checkpoint.save(checkpoint_prefix)\n",
    "            logging.info('Saved checkpoint to %s', checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step:  0\n",
      "(<tf.Tensor: shape=(32, 15, 3), dtype=float64, numpy=\n",
      "array([[[ 0.04211119, -0.36976998,  3.151     ],\n",
      "        [-0.0188095 , -0.35409565,  3.103     ],\n",
      "        [-0.05425067, -0.34582817,  3.119     ],\n",
      "        ...,\n",
      "        [-0.42017372, -0.20322584,  2.262     ],\n",
      "        [-0.45865566, -0.18066558,  2.167     ],\n",
      "        [-0.47094274, -0.15554524,  2.159     ]],\n",
      "\n",
      "       [[ 1.70223823, -0.63842558,  4.508     ],\n",
      "        [ 1.55547465, -0.51670998,  4.173     ],\n",
      "        [ 1.57094049, -0.53107336,  4.289     ],\n",
      "        ...,\n",
      "        [ 1.11928094, -0.51852724,  3.32      ],\n",
      "        [ 1.11762116, -0.48925476,  3.268     ],\n",
      "        [ 1.14407304, -0.50511799,  3.268     ]],\n",
      "\n",
      "       [[ 1.80311369, -0.67323804,  8.235     ],\n",
      "        [ 1.78978252, -0.68656256,  8.235     ],\n",
      "        [ 1.78978252, -0.65991351,  8.235     ],\n",
      "        ...,\n",
      "        [ 1.45805128, -0.69422637,  6.862     ],\n",
      "        [ 1.44694279, -0.68312341,  6.862     ],\n",
      "        [ 1.40664234, -0.66468482,  6.57      ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[-3.33366421, -0.33092185,  9.501     ],\n",
      "        [-3.47967556, -0.3204975 ,  9.65      ],\n",
      "        [-3.44132854, -0.33092185,  9.501     ],\n",
      "        ...,\n",
      "        [-3.14274848, -0.3401249 ,  8.235     ],\n",
      "        [-3.15808804, -0.37171772,  8.346     ],\n",
      "        [-3.0770229 , -0.34470946,  8.346     ]],\n",
      "\n",
      "       [[-3.47825534, -0.35850738, 10.293     ],\n",
      "        [-3.33381389, -0.33082649,  9.961     ],\n",
      "        [-3.51158083, -0.35850738, 10.293     ],\n",
      "        ...,\n",
      "        [-3.33366421, -0.33092185,  9.501     ],\n",
      "        [-3.47967556, -0.3204975 ,  9.65      ],\n",
      "        [-3.44132854, -0.33092185,  9.501     ]],\n",
      "\n",
      "       [[-1.16499693, -0.63838103,  9.501     ],\n",
      "        [-1.22307742, -0.61356557,  9.357     ],\n",
      "        [-1.21539149, -0.5410581 ,  8.46      ],\n",
      "        ...,\n",
      "        [-1.25757709, -0.49985668,  4.863     ],\n",
      "        [-1.19496466, -0.51442578,  4.508     ],\n",
      "        [-1.10278998, -0.45414609,  4.037     ]]])>, <tf.Tensor: shape=(32, 15, 51), dtype=float64, numpy=\n",
      "array([[[2.40000000e+01, 2.10000000e+01, 2.37134218e-01, ...,\n",
      "         5.10000000e+01, 2.63000000e+02, 4.10480440e-01],\n",
      "        [3.00000000e+01, 2.50000000e+01, 2.88929820e-01, ...,\n",
      "         1.50000000e+01, 2.92000000e+02, 4.65164125e-01],\n",
      "        [3.80000000e+01, 2.70000000e+01, 1.91303313e-01, ...,\n",
      "         5.60000000e+01, 2.93000000e+02, 7.01300025e-01],\n",
      "        ...,\n",
      "        [4.80000000e+01, 4.00000000e+00, 2.66359270e-01, ...,\n",
      "         8.00000000e+01, 3.25000000e+02, 5.36187410e-01],\n",
      "        [4.80000000e+01, 4.00000000e+00, 2.37992600e-01, ...,\n",
      "         9.00000000e+01, 3.37000000e+02, 2.81829268e-01],\n",
      "        [6.20000000e+01, 4.00000000e+00, 2.70877033e-01, ...,\n",
      "         1.05000000e+02, 3.32000000e+02, 7.53431678e-01]],\n",
      "\n",
      "       [[4.60000000e+01, 4.00000000e+01, 1.48477480e-01, ...,\n",
      "         6.30000000e+01, 2.23000000e+02, 3.40649664e-01],\n",
      "        [3.90000000e+01, 2.80000000e+01, 3.13071936e-01, ...,\n",
      "         3.60000000e+01, 2.35000000e+02, 5.06850123e-01],\n",
      "        [1.60000000e+01, 2.70000000e+01, 2.73960143e-01, ...,\n",
      "         3.90000000e+01, 2.08000000e+02, 6.09051213e-02],\n",
      "        ...,\n",
      "        [3.20000000e+01, 2.60000000e+01, 2.49345154e-01, ...,\n",
      "         5.10000000e+01, 2.91000000e+02, 4.44741726e-01],\n",
      "        [2.30000000e+01, 1.30000000e+01, 3.11845034e-01, ...,\n",
      "         3.10000000e+01, 2.90000000e+02, 3.95866543e-01],\n",
      "        [2.80000000e+01, 3.90000000e+01, 1.93110675e-01, ...,\n",
      "         3.20000000e+01, 2.83000000e+02, 4.06487405e-01]],\n",
      "\n",
      "       [[1.40000000e+01, 1.10000000e+01, 2.39689305e-01, ...,\n",
      "         1.60000000e+01, 1.16000000e+02, 6.13795757e-01],\n",
      "        [2.50000000e+01, 1.10000000e+01, 1.39819771e-01, ...,\n",
      "         1.70000000e+01, 1.18000000e+02, 6.22320116e-01],\n",
      "        [1.70000000e+01, 1.60000000e+01, 1.92373380e-01, ...,\n",
      "         1.80000000e+01, 1.15000000e+02, 3.91298980e-01],\n",
      "        ...,\n",
      "        [8.00000000e+00, 1.90000000e+01, 2.22804815e-01, ...,\n",
      "         2.90000000e+01, 1.38000000e+02, 2.35311598e-01],\n",
      "        [2.90000000e+01, 1.70000000e+01, 2.11041331e-01, ...,\n",
      "         1.30000000e+01, 1.36000000e+02, 2.55962789e-01],\n",
      "        [2.80000000e+01, 1.60000000e+01, 3.66358638e-01, ...,\n",
      "         7.00000000e+00, 1.43000000e+02, 2.90637463e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[1.90000000e+01, 0.00000000e+00, 2.51043767e-01, ...,\n",
      "         4.00000000e+00, 9.70000000e+01, 3.91620725e-01],\n",
      "        [3.00000000e+01, 4.00000000e+00, 1.90934688e-01, ...,\n",
      "         1.50000000e+01, 9.90000000e+01, 1.68083355e-01],\n",
      "        [2.40000000e+01, 1.10000000e+01, 3.32810879e-01, ...,\n",
      "         1.20000000e+01, 1.00000000e+02, 3.09606522e-01],\n",
      "        ...,\n",
      "        [3.00000000e+01, 7.00000000e+00, 3.77940416e-01, ...,\n",
      "         1.70000000e+01, 1.16000000e+02, 5.83269596e-01],\n",
      "        [3.50000000e+01, 1.00000000e+01, 3.65579784e-01, ...,\n",
      "         1.80000000e+01, 1.21000000e+02, 7.94767857e-01],\n",
      "        [3.20000000e+01, 4.00000000e+00, 4.01108742e-01, ...,\n",
      "         1.20000000e+01, 1.13000000e+02, 6.71177268e-01]],\n",
      "\n",
      "       [[1.70000000e+01, 6.00000000e+00, 1.79855600e-01, ...,\n",
      "         1.00000000e+01, 9.00000000e+01, 1.78962380e-01],\n",
      "        [2.10000000e+01, 7.00000000e+00, 2.78586388e-01, ...,\n",
      "         9.00000000e+00, 9.20000000e+01, 2.37737834e-01],\n",
      "        [1.50000000e+01, 9.00000000e+00, 2.25935400e-01, ...,\n",
      "         1.20000000e+01, 9.50000000e+01, 6.04440451e-01],\n",
      "        ...,\n",
      "        [1.90000000e+01, 0.00000000e+00, 2.51043767e-01, ...,\n",
      "         4.00000000e+00, 9.70000000e+01, 3.91620725e-01],\n",
      "        [3.00000000e+01, 4.00000000e+00, 1.90934688e-01, ...,\n",
      "         1.50000000e+01, 9.90000000e+01, 1.68083355e-01],\n",
      "        [2.40000000e+01, 1.10000000e+01, 3.32810879e-01, ...,\n",
      "         1.20000000e+01, 1.00000000e+02, 3.09606522e-01]],\n",
      "\n",
      "       [[2.90000000e+01, 1.40000000e+01, 3.85514557e-01, ...,\n",
      "         2.20000000e+01, 1.24000000e+02, 4.25000250e-01],\n",
      "        [3.00000000e+01, 1.20000000e+01, 3.44008595e-01, ...,\n",
      "         2.20000000e+01, 1.25000000e+02, 3.22757602e-01],\n",
      "        [2.30000000e+01, 1.50000000e+01, 1.66387081e-01, ...,\n",
      "         2.10000000e+01, 1.27000000e+02, 4.57175165e-01],\n",
      "        ...,\n",
      "        [4.00000000e+01, 1.60000000e+01, 4.22114849e-01, ...,\n",
      "         4.70000000e+01, 2.01000000e+02, 1.34470835e-01],\n",
      "        [3.90000000e+01, 1.60000000e+01, 3.09132636e-01, ...,\n",
      "         4.60000000e+01, 2.09000000e+02, 4.35936928e-01],\n",
      "        [4.00000000e+01, 1.10000000e+01, 4.79174823e-01, ...,\n",
      "         5.20000000e+01, 2.21000000e+02, 2.78019816e-01]]])>)\n",
      "step:  1\n",
      "(<tf.Tensor: shape=(32, 15, 3), dtype=float64, numpy=\n",
      "array([[[-0.24771293, -0.29547682,  2.745     ],\n",
      "        [-0.28576436, -0.2542629 ,  2.685     ],\n",
      "        [-0.29577355, -0.25539927,  2.697     ],\n",
      "        ...,\n",
      "        [-0.60033201, -0.08569193,  1.924     ],\n",
      "        [-0.59328105, -0.12688373,  1.844     ],\n",
      "        [-0.58335021, -0.11540829,  1.76      ]],\n",
      "\n",
      "       [[ 1.02767881, -0.15454266,  7.625     ],\n",
      "        [ 1.07419663, -0.14900952,  7.352     ],\n",
      "        [ 1.09800008, -0.14900952,  7.352     ],\n",
      "        ...,\n",
      "        [ 1.85643654, -0.12152626,  5.996     ],\n",
      "        [ 1.92641028, -0.1275074 ,  5.826     ],\n",
      "        [ 1.96413577, -0.1275074 ,  5.826     ]],\n",
      "\n",
      "       [[ 0.15779091, -0.48754945,  5.062     ],\n",
      "        [ 0.12797046, -0.4211716 ,  4.863     ],\n",
      "        [ 0.12797046, -0.41330309,  4.863     ],\n",
      "        ...,\n",
      "        [-0.09677079, -0.28030634,  3.57      ],\n",
      "        [-0.17227645, -0.36035317,  3.25      ],\n",
      "        [-0.16701521, -0.38138764,  3.25      ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.30662149, -0.41007349,  4.825     ],\n",
      "        [ 0.26264656, -0.38756794,  4.475     ],\n",
      "        [ 0.23366929, -0.39480865,  4.475     ],\n",
      "        ...,\n",
      "        [-0.19964712, -0.29886723,  3.103     ],\n",
      "        [-0.19603385, -0.26369424,  2.833     ],\n",
      "        [-0.17789108, -0.27285464,  2.697     ]],\n",
      "\n",
      "       [[ 0.20131749, -0.41871946,  3.338     ],\n",
      "        [ 0.08804248, -0.336807  ,  2.685     ],\n",
      "        [ 0.09604663, -0.39716349,  3.25      ],\n",
      "        ...,\n",
      "        [-0.34532944, -0.25653564,  2.709     ],\n",
      "        [-0.33350385, -0.22234283,  2.431     ],\n",
      "        [-0.3767705 , -0.21176097,  2.357     ]],\n",
      "\n",
      "       [[ 2.18453565, -0.75447194,  5.938     ],\n",
      "        [ 2.18297783, -0.81397768,  5.882     ],\n",
      "        [ 2.19414834, -0.75447194,  5.938     ],\n",
      "        ...,\n",
      "        [ 2.00971729, -0.72373144,  4.941     ],\n",
      "        [ 2.03353213, -0.73388215,  4.902     ],\n",
      "        [ 2.00940057, -0.72235442,  4.825     ]]])>, <tf.Tensor: shape=(32, 15, 51), dtype=float64, numpy=\n",
      "array([[[5.20000000e+01, 1.70000000e+01, 3.37305248e-01, ...,\n",
      "         5.10000000e+01, 3.06000000e+02, 5.71861386e-01],\n",
      "        [5.30000000e+01, 6.00000000e+00, 3.53849500e-01, ...,\n",
      "         3.80000000e+01, 3.08000000e+02, 7.27502346e-01],\n",
      "        [5.00000000e+01, 6.00000000e+00, 2.83895046e-01, ...,\n",
      "         3.40000000e+01, 3.11000000e+02, 6.41842544e-01],\n",
      "        ...,\n",
      "        [6.60000000e+01, 2.00000000e+00, 1.29154727e-01, ...,\n",
      "         8.50000000e+01, 3.53000000e+02, 4.37601537e-01],\n",
      "        [7.30000000e+01, 2.00000000e+00, 1.36087134e-01, ...,\n",
      "         4.30000000e+01, 3.57000000e+02, 3.35820884e-01],\n",
      "        [6.90000000e+01, 4.00000000e+00, 1.28808796e-01, ...,\n",
      "         3.50000000e+01, 3.47000000e+02, 2.54261583e-01]],\n",
      "\n",
      "       [[3.00000000e+01, 2.10000000e+01, 3.47634226e-01, ...,\n",
      "         3.00000000e+00, 1.11000000e+02, 7.30900764e-01],\n",
      "        [3.60000000e+01, 2.10000000e+01, 3.78073126e-01, ...,\n",
      "         1.60000000e+01, 1.21000000e+02, 6.65246487e-01],\n",
      "        [3.20000000e+01, 2.10000000e+01, 4.26788598e-01, ...,\n",
      "         1.90000000e+01, 1.21000000e+02, 5.74690819e-01],\n",
      "        ...,\n",
      "        [4.00000000e+01, 3.00000000e+01, 2.87624031e-01, ...,\n",
      "         1.20000000e+01, 1.56000000e+02, 7.82372832e-01],\n",
      "        [5.00000000e+01, 3.60000000e+01, 3.85288864e-01, ...,\n",
      "         6.00000000e+00, 1.61000000e+02, 8.45217466e-01],\n",
      "        [6.00000000e+01, 3.00000000e+01, 4.27215517e-01, ...,\n",
      "         9.00000000e+00, 1.55000000e+02, 8.38269293e-01]],\n",
      "\n",
      "       [[2.70000000e+01, 2.50000000e+01, 3.67380887e-01, ...,\n",
      "         4.10000000e+01, 1.62000000e+02, 2.16418833e-01],\n",
      "        [3.00000000e+01, 2.80000000e+01, 3.10829371e-01, ...,\n",
      "         3.20000000e+01, 1.82000000e+02, 4.87570912e-01],\n",
      "        [2.90000000e+01, 3.70000000e+01, 3.39479923e-01, ...,\n",
      "         2.70000000e+01, 1.83000000e+02, 6.47624016e-01],\n",
      "        ...,\n",
      "        [3.70000000e+01, 2.30000000e+01, 2.41148636e-01, ...,\n",
      "         2.40000000e+01, 2.31000000e+02, 5.09998918e-01],\n",
      "        [5.50000000e+01, 4.90000000e+01, 2.96126664e-01, ...,\n",
      "         3.50000000e+01, 2.61000000e+02, 6.29578471e-01],\n",
      "        [3.80000000e+01, 2.00000000e+01, 2.71187514e-01, ...,\n",
      "         3.70000000e+01, 2.59000000e+02, 5.17552078e-01]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[2.80000000e+01, 2.90000000e+01, 2.28669465e-01, ...,\n",
      "         2.30000000e+01, 1.94000000e+02, 2.66298294e-01],\n",
      "        [3.30000000e+01, 1.20000000e+01, 1.55484959e-01, ...,\n",
      "         1.50000000e+01, 2.02000000e+02, 2.69513041e-01],\n",
      "        [2.80000000e+01, 1.40000000e+01, 2.74416089e-01, ...,\n",
      "         1.80000000e+01, 2.12000000e+02, 3.21052462e-01],\n",
      "        ...,\n",
      "        [3.20000000e+01, 4.60000000e+01, 1.45831421e-01, ...,\n",
      "         3.80000000e+01, 2.45000000e+02, 1.61041960e-01],\n",
      "        [4.70000000e+01, 4.30000000e+01, 1.80839196e-01, ...,\n",
      "         5.50000000e+01, 2.78000000e+02, 2.53233343e-01],\n",
      "        [5.30000000e+01, 6.40000000e+01, 1.41146168e-01, ...,\n",
      "         3.90000000e+01, 2.74000000e+02, 2.68770069e-01]],\n",
      "\n",
      "       [[2.60000000e+01, 3.70000000e+01, 3.13077718e-01, ...,\n",
      "         2.40000000e+01, 2.61000000e+02, 1.79145366e-01],\n",
      "        [2.20000000e+01, 3.20000000e+01, 1.75636351e-01, ...,\n",
      "         3.20000000e+01, 2.72000000e+02, 2.60931551e-01],\n",
      "        [1.00000000e+01, 2.90000000e+01, 1.25371426e-01, ...,\n",
      "         4.20000000e+01, 2.82000000e+02, 2.07547188e-01],\n",
      "        ...,\n",
      "        [5.20000000e+01, 7.00000000e+00, 2.74553150e-01, ...,\n",
      "         4.60000000e+01, 3.24000000e+02, 4.03943986e-01],\n",
      "        [3.70000000e+01, 4.00000000e+00, 3.49247873e-01, ...,\n",
      "         5.20000000e+01, 3.24000000e+02, 2.68476307e-01],\n",
      "        [3.50000000e+01, 4.00000000e+00, 2.72228092e-01, ...,\n",
      "         6.80000000e+01, 3.24000000e+02, 5.78499079e-01]],\n",
      "\n",
      "       [[2.30000000e+01, 2.70000000e+01, 3.53103399e-01, ...,\n",
      "         4.50000000e+01, 2.00000000e+02, 8.18233132e-01],\n",
      "        [1.70000000e+01, 2.70000000e+01, 2.53789514e-01, ...,\n",
      "         3.70000000e+01, 2.10000000e+02, 8.18100154e-01],\n",
      "        [3.00000000e+01, 2.10000000e+01, 3.50954443e-01, ...,\n",
      "         4.60000000e+01, 1.99000000e+02, 7.71811724e-01],\n",
      "        ...,\n",
      "        [3.00000000e+01, 3.10000000e+01, 3.39199483e-01, ...,\n",
      "         4.30000000e+01, 2.38000000e+02, 5.44942975e-01],\n",
      "        [3.00000000e+01, 3.40000000e+01, 3.35341573e-01, ...,\n",
      "         4.10000000e+01, 2.43000000e+02, 7.97706306e-01],\n",
      "        [3.20000000e+01, 1.40000000e+01, 3.02541316e-01, ...,\n",
      "         4.70000000e+01, 2.40000000e+02, 7.16536045e-01]]])>)\n"
     ]
    }
   ],
   "source": [
    "train_iter = iter(dist_train_dataset)\n",
    "for step in range(current_train_iter, num_train_iter):\n",
    "    print(\"step: \", step)\n",
    "    print(next(train_iter))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
