{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 11:07:40.868401: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-08 11:07:40.922896: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-08 11:07:40.924150: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 11:07:41.833738: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from train import build_model\n",
    "from preprocess_data import load_data\n",
    "from layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 11:07:46.392127: W tensorflow/core/framework/dataset.cc:956] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n",
      "2024-07-08 11:08:16.412139: W tensorflow/core/framework/dataset.cc:956] Input of Window will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_dataset, test_dataset = load_data(data_path=\"/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/final_dataset.pkl\", batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n",
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n",
      "1/1 [==============================] - 1s 946ms/step\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "for (batch_x1, batch_x2) in train_dataset.take(1):\n",
    "        output2 = model.predict((batch_x1, batch_x2))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['mask', 'position', 'position_raw_scale', 'mixture_logits', 'targets'])\n"
     ]
    }
   ],
   "source": [
    "print(output2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 15, 5, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2['position_raw_scale'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_models as tfm\n",
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Create cosine learning rate schedule w/ warmup.\n",
    "def _get_learning_rate_schedule(\n",
    "    warmup_steps: int,\n",
    "    total_steps: int,\n",
    "    learning_rate: float,\n",
    "    alpha: float = 0.0) -> tf.keras.optimizers.schedules.LearningRateSchedule:\n",
    "  \"\"\"Returns a cosine decay learning rate schedule to be used in training.\n",
    "\n",
    "  Args:\n",
    "    warmup_steps: Number of training steps to apply warmup. If global_step <\n",
    "      warmup_steps, the learning rate will be `global_step/num_warmup_steps *\n",
    "      init_lr`.\n",
    "    total_steps: The total number of training steps.\n",
    "    learning_rate: The peak learning rate anytime during the training.\n",
    "    alpha: The alpha parameter forwarded to CosineDecay\n",
    "\n",
    "  Returns:\n",
    "    A CosineDecay learning schedule w/ warmup.\n",
    "  \"\"\"\n",
    "\n",
    "  decay_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "      initial_learning_rate=learning_rate, decay_steps=total_steps, alpha=alpha)\n",
    "  return tfm.optimization.LinearWarmup(decay_schedule, warmup_steps, 1e-10)\n",
    "\n",
    "learning_rate_schedule = _get_learning_rate_schedule(\n",
    "    warmup_steps=5e4, total_steps=5e6,\n",
    "    learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=learning_rate_schedule,\n",
    "    global_clipnorm=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "  \"\"\"Base class for Human Scene Transformer Losses.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, name='Loss', clip_loss_max=tf.float32.max):\n",
    "    self.clip_loss_max = clip_loss_max\n",
    "    self.name = name\n",
    "\n",
    "  @tf.function\n",
    "  def __call__(self, input_batch, predictions):\n",
    "    return self.call(input_batch, predictions)\n",
    "\n",
    "  def call(self, input_batch, predictions):\n",
    "    \"\"\"Calculates loss for fields which should be predicted.\"\"\"\n",
    "\n",
    "    # [b, a, t]\n",
    "    ##should_predict = input_batch['should_predict'][..., 0]\n",
    "    should_predict = tf.math.logical_not(predictions['mask'])\n",
    "    # [b, a, t]\n",
    "    loss_per_batch = self.get_per_batch_loss(input_batch, predictions)\n",
    "\n",
    "    loss_per_batch = tfp.math.clip_by_value_preserve_gradient(\n",
    "        loss_per_batch,\n",
    "        tf.float32.min,\n",
    "        self.clip_loss_max)\n",
    "\n",
    "    # Compute loss only on positions w/ should_predict == True.\n",
    "    should_predict_ind = tf.where(should_predict)\n",
    "    loss_should_predict_mat = tf.gather_nd(\n",
    "        params=loss_per_batch, indices=should_predict_ind)\n",
    "\n",
    "    loss_should_predict = tf.reduce_mean(loss_should_predict_mat)\n",
    "    # If there are no agents to be predicted xyz_loss_should_predict can be NaN\n",
    "    loss_should_predict = tf.math.multiply_no_nan(\n",
    "        loss_should_predict,\n",
    "        tf.cast(tf.math.reduce_any(should_predict), tf.float32))\n",
    "\n",
    "    loss_dict = {\n",
    "        'loss': loss_should_predict,\n",
    "        f'{self.name}_loss': loss_should_predict\n",
    "    }\n",
    "    return loss_dict\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    raise NotImplementedError\n",
    "\n",
    "class MinNLLPositionMixtureCategoricalCrossentropyLoss(Loss):\n",
    "  \"\"\"MinNLLPositionNLLLoss and MixtureCategoricalCrossentropyLoss.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(name='MinNLLMixture', **kwargs)\n",
    "    self.position_loss_obj = MinNLLPositionLoss()\n",
    "    self.mixture_loss_obj = MinNLLMixtureCategoricalCrossentropyLoss()\n",
    "\n",
    "  def call(self, input_batch, predictions):\n",
    "    position_loss = self.position_loss_obj(input_batch, predictions)\n",
    "    print(\"position loss: \", position_loss)\n",
    "    mixture_loss = self.mixture_loss_obj(input_batch, predictions)\n",
    "    print(\"mixture loss: \", mixture_loss)\n",
    "\n",
    "    loss = position_loss['loss'] + mixture_loss['loss']\n",
    "\n",
    "    print(\"loss: \", loss)\n",
    "\n",
    "    loss_dict = {**position_loss, **mixture_loss}\n",
    "\n",
    "    loss_dict['loss'] = loss\n",
    "\n",
    "    return loss_dict\n",
    "  \n",
    "class PositionNLLLoss(Loss):\n",
    "  \"\"\"Position NLL Loss for human trajectory predictions.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(name='position', **kwargs)\n",
    "\n",
    "  # from model.output_distributions\n",
    "  def force_positive(self, x, eps=1e-6):\n",
    "    return tf.keras.activations.elu(x) + 1. + eps\n",
    "\n",
    "  # from model.output_distributions\n",
    "  def to_positive_definite_scale_tril(self, logit_sigma):\n",
    "    tril = tfp.math.fill_triangular(logit_sigma)\n",
    "    scale_tril = tf.linalg.set_diag(\n",
    "        tril,\n",
    "        self.force_positive(tf.linalg.diag_part(tril)))\n",
    "    return scale_tril\n",
    "  \n",
    "  # from model.output_distributions\n",
    "  def get_position_distribution(self, model_output):\n",
    "    \"\"\"Multivariate Normal distribution over position.\"\"\"\n",
    "    p_pos = tfp.distributions.MultivariateNormalTriL(\n",
    "        loc=model_output['position'],\n",
    "        scale_tril=self.to_positive_definite_scale_tril(\n",
    "            model_output['position_raw_scale']))\n",
    "    return p_pos\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    \"\"\"Negative log probability of ground truth with respect to predictions.\"\"\"\n",
    "    # [b, a, t, 3]\n",
    "    p_position = self.get_position_distribution(predictions)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    position_nll = -p_position.log_prob(\n",
    "        predictions['targets'])\n",
    "    return position_nll\n",
    "  \n",
    "class MinNLLPositionLoss(PositionNLLLoss):\n",
    "  \"\"\"MinNLLPositionNLL loss.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.mixture_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    \"\"\"Negative log probability of mode with smallest ADE.\"\"\"\n",
    "    predictions = predictions.copy()\n",
    "\n",
    "    predictions['targets'] = predictions['targets'][..., tf.newaxis, :]\n",
    "    # [b, a, t, n]\n",
    "    position_nll = super().get_per_batch_loss(input_batch, predictions)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    ##should_predict = tf.cast(input_batch['should_predict'], tf.float32)\n",
    "    should_predict = tf.math.logical_not(predictions['mask'])\n",
    "    should_predict = tf.expand_dims(should_predict, axis=-1)\n",
    "    should_predict = tf.cast(should_predict, tf.float32)\n",
    "\n",
    "    # [b, a, t, n, 1]\n",
    "    per_position_nll = (\n",
    "        position_nll[..., tf.newaxis] * should_predict[..., tf.newaxis, :]\n",
    "    )\n",
    "\n",
    "    # Get mode with minimum NLL\n",
    "    # [b, a, n, 1]\n",
    "    per_mode_nll_sum = tf.reduce_sum(per_position_nll, axis=2)\n",
    "\n",
    "    t = tf.shape(position_nll)[2]\n",
    "\n",
    "    # [b, a, 1]\n",
    "    min_nll_indices = tf.math.argmin(per_mode_nll_sum, axis=-2)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    min_nll_indices_tiled = tf.tile(\n",
    "        min_nll_indices[..., tf.newaxis], [1, t, 1])\n",
    "\n",
    "    # [b, a, t]\n",
    "    position_nll_min_ade = tf.gather(\n",
    "        position_nll, indices=min_nll_indices_tiled, batch_dims=2, axis=-1\n",
    "        )[..., 0]\n",
    "\n",
    "    return position_nll_min_ade\n",
    "\n",
    "class MinNLLMixtureCategoricalCrossentropyLoss(PositionNLLLoss):\n",
    "  \"\"\"Categorical Corssentropy Loss for Mixture Weight.\"\"\"\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.mixture_loss = tf.keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "  def get_per_batch_loss(self, input_batch, predictions):\n",
    "    \"\"\"Negative log probability of ground truth with respect to predictions.\"\"\"\n",
    "    predictions= predictions.copy()\n",
    "    predictions['targets'] = predictions[\n",
    "        'targets'][..., tf.newaxis, :]\n",
    "\n",
    "    # Calculate ADE\n",
    "    # [b, a, t, n, 1]\n",
    "    position_nll = super().get_per_batch_loss(input_batch, predictions)\n",
    "    position_nll = tf.expand_dims(position_nll, axis=-1)\n",
    "\n",
    "    # [b, a, t, 1]\n",
    "    ##should_predict = tf.cast(input_batch['should_predict'], tf.float32)\n",
    "    should_predict = tf.cast(tf.expand_dims(tf.math.logical_not(predictions['mask']), axis=-1), tf.float32)\n",
    "\n",
    "    # [b, a, t, n, 1]\n",
    "    per_position_nll = (\n",
    "        position_nll * should_predict[..., tf.newaxis, :]\n",
    "    )\n",
    "\n",
    "    # Get mode with minimum NLL\n",
    "    # [b, a, n, 1]\n",
    "    ##per_mode_nll_sum = tf.reduce_sum(per_position_nll, axis=2)\n",
    "    per_mode_nll_sum = tf.reduce_sum(per_position_nll, axis=1)\n",
    "\n",
    "    a = tf.shape(position_nll)[1]\n",
    "    #n = tf.shape(position_nll)[3]\n",
    "    n = tf.shape(position_nll)[2]\n",
    "\n",
    "    # [b, a, 1]\n",
    "    min_nll_indices = tf.math.argmin(per_mode_nll_sum, axis=-2)\n",
    "\n",
    "    # [b, a, n]\n",
    "    min_nll_indices_one_hot = tf.one_hot(min_nll_indices[..., 0], n)\n",
    "\n",
    "    # [b, a]\n",
    "    # TODO\n",
    "    #mixture_loss = self.mixture_loss(\n",
    "        #min_nll_indices_one_hot,\n",
    "        #tf.tile(predictions['mixture_logits'][..., 0, :], [1, a, 1]))\n",
    "    \n",
    "    mixture_loss = self.mixture_loss(\n",
    "        min_nll_indices_one_hot,\n",
    "        predictions['mixture_logits'][..., 0, :])\n",
    "\n",
    "    return mixture_loss\n",
    "\n",
    "  def call(self, input_batch, predictions):\n",
    "    \"\"\"Calculates loss.\"\"\"\n",
    "\n",
    "    # [b, a]\n",
    "    ##should_predict = tf.reduce_any(\n",
    "      ##  input_batch['should_predict'][..., 0], axis=-1)\n",
    "    should_predict = tf.reduce_any(\n",
    "        tf.math.logical_not(predictions['mask'][..., 0]), axis=-1)\n",
    "    # [b, a]\n",
    "    loss_per_batch = self.get_per_batch_loss(input_batch, predictions)\n",
    "\n",
    "    # Compute loss only on positions w/ should_predict == True.\n",
    "    should_predict_ind = tf.where(should_predict)\n",
    "    loss_should_predict_mat = tf.gather_nd(\n",
    "        params=loss_per_batch, indices=should_predict_ind)\n",
    "\n",
    "    loss_should_predict = tf.reduce_mean(loss_should_predict_mat)\n",
    "    # If there are no agents to be predicted xyz_loss_should_predict can be NaN\n",
    "    loss_should_predict = tf.math.multiply_no_nan(\n",
    "        loss_should_predict,\n",
    "        tf.cast(tf.math.reduce_any(should_predict), tf.float32))\n",
    "\n",
    "    # Mixture weights are per scene. So we do not have to mask anything\n",
    "    loss_dict = {\n",
    "        'loss': loss_should_predict,\n",
    "        f'{self.name}_loss': loss_should_predict\n",
    "    }\n",
    "    return loss_dict\n",
    "  \n",
    "loss_obj = MinNLLPositionMixtureCategoricalCrossentropyLoss()\n",
    "#loss = loss_obj(input_batch=batch_x1, predictions=output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 15, 3)]              0         []                            \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)        [(None, 15, 51)]             0         []                            \n",
      "                                                                                                  \n",
      " preprocess_layer (Preproce  (((32, 15, 3),               0         ['input_1[0][0]',             \n",
      " ssLayer)                     (32, 15, 51)),                         'input_2[0][0]']             \n",
      "                              (32, 15),                                                           \n",
      "                              (32, 15, 3))                                                        \n",
      "                                                                                                  \n",
      " feature_attn_agent_encoder  ((32, 15, 128),              453896    ['preprocess_layer[0][0]',    \n",
      " _learned_layer (FeatureAtt   (32, 4, 15, 15))                       'preprocess_layer[0][1]',    \n",
      " nAgentEncoderLearnedLayer)                                          'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " agent_self_alignment_layer  ((32, 15, 128),              99712     ['feature_attn_agent_encoder_l\n",
      "  (AgentSelfAlignmentLayer)   (32, 8, 15, 15))                      earned_layer[0][0]',          \n",
      "                                                                     'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 128),              99584     ['agent_self_alignment_layer[0\n",
      " r (SelfAttnTransformerLaye   (32, 8, 15, 15))                      ][0]',                        \n",
      " r)                                                                  'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 128),              99584     ['self_attn_transformer_layer[\n",
      " r_1 (SelfAttnTransformerLa   (32, 8, 15, 15))                      0][0]',                       \n",
      " yer)                                                                'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " multimodality_induction (M  ((32, 15, 5, 128),           117121    ['self_attn_transformer_layer_\n",
      " ultimodalityInduction)       (32, 1, 5))                           1[0][0]',                     \n",
      "                                                                     'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 5, 128),           99584     ['multimodality_induction[0][0\n",
      " r_2 (SelfAttnTransformerLa   (32, 5, 8, 15, 15))                   ]',                           \n",
      " yer)                                                                'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_mode_transformer  ((32, 15, 5, 128),           99584     ['self_attn_transformer_layer_\n",
      " _layer (SelfAttnModeTransf   (32, 15, 8, 5, 5))                    2[0][0]']                     \n",
      " ormerLayer)                                                                                      \n",
      "                                                                                                  \n",
      " self_attn_transformer_laye  ((32, 15, 5, 128),           99584     ['self_attn_mode_transformer_l\n",
      " r_3 (SelfAttnTransformerLa   (32, 5, 8, 15, 15))                   ayer[0][0]',                  \n",
      " yer)                                                                'preprocess_layer[0][2]']    \n",
      "                                                                                                  \n",
      " self_attn_mode_transformer  ((32, 15, 5, 128),           99584     ['self_attn_transformer_layer_\n",
      " _layer_1 (SelfAttnModeTran   (32, 15, 8, 5, 5))                    3[0][0]']                     \n",
      " sformerLayer)                                                                                    \n",
      "                                                                                                  \n",
      " prediction2d_position_head  (32, 15, 5, 9)               1161      ['self_attn_mode_transformer_l\n",
      " _layer (Prediction2DPositi                                         ayer_1[0][0]']                \n",
      " onHeadLayer)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem (  (32, 15, 5, 3)               0         ['prediction2d_position_head_l\n",
      " SlicingOpLambda)                                                   ayer[0][0]']                  \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_1  (32, 15, 5, 6)               0         ['prediction2d_position_head_l\n",
      "  (SlicingOpLambda)                                                 ayer[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1269394 (4.84 MB)\n",
      "Trainable params: 1269394 (4.84 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics.\n",
    "\n",
    "class Mean(tf.keras.metrics.Mean):\n",
    "\n",
    "  def __init__(self, name=\"mean\", dtype=None):\n",
    "    super().__init__(name=name, dtype=dtype)\n",
    "\n",
    "def distance_error(target: tf.Tensor, pred: tf.Tensor) -> tf.Tensor:\n",
    "  return tf.sqrt(\n",
    "      tf.reduce_sum(tf.square(pred - target), axis=-1, keepdims=True))\n",
    "\n",
    "class ADE(tf.keras.metrics.Metric):\n",
    "  \"\"\"Average Displacement Error over a n dimensional track.\n",
    "\n",
    "  Calculates the mean L2 distance over all predicted timesteps.\n",
    "  \"\"\"\n",
    "  # TODO: timestep?\n",
    "  def __init__(self, num_history_steps=6, timestep=0.4, cutoff_seconds=None, at_cutoff=False, name='ADE'):\n",
    "    \"\"\"Initializes the ADE metric.\n",
    "\n",
    "    Args:\n",
    "      params: ModelParams\n",
    "      cutoff_seconds: Cutoff up to which time the metric should be calculated\n",
    "        in seconds.\n",
    "      at_cutoff: If True metric will be calculated at cutoff timestep.\n",
    "        Otherwise metric is calculated as average up to cutoff_seconds.\n",
    "      name: Metric name.\n",
    "    \"\"\"\n",
    "    super().__init__(name=name)\n",
    "    self.cutoff_seconds = cutoff_seconds\n",
    "    self.at_cutoff = at_cutoff\n",
    "    if cutoff_seconds is None:\n",
    "      self.cutoff_idx = None\n",
    "    else:\n",
    "      # +1 due to current time step.\n",
    "      self.cutoff_idx = int(\n",
    "          num_history_steps +\n",
    "          cutoff_seconds / timestep) + 1\n",
    "\n",
    "    self.num_predictions = self.add_weight(\n",
    "        name='num_predictions', initializer='zeros')\n",
    "    self.total_deviation = self.add_weight(\n",
    "        name='total_deviation', initializer='zeros')\n",
    "\n",
    "  def _reduce(self, ade_with_modes, input_batch, predictions):\n",
    "    \"\"\"Reduces mode dimension. The base class squeezes a single mode.\"\"\"\n",
    "    return tf.squeeze(ade_with_modes, axis=-1)\n",
    "\n",
    "  def update_state(self, input_batch, predictions):\n",
    "    should_predict = tf.cast(tf.math.logical_not(predictions['mask']), tf.float32)\n",
    "    print(\"should_predict: \", should_predict.shape)\n",
    "\n",
    "    target = predictions['targets']\n",
    "    target = target[..., :predictions['position'].shape[-1]]\n",
    "    print(\"target: \", target.shape)\n",
    "    # [b, a, t, n, 3] -> [b, a, t, n, 1]\n",
    "    per_position_ade = distance_error(\n",
    "        target[..., tf.newaxis, :],\n",
    "        predictions['position'])\n",
    "    print(\"per_position_ade: \", per_position_ade.shape)\n",
    "\n",
    "    # Non-observed or past should not contribute to ade.\n",
    "    deviation = tf.math.multiply_no_nan(per_position_ade,\n",
    "                                        should_predict[..., tf.newaxis, :])\n",
    "    print(\"deviation: \", deviation.shape)\n",
    "    # Chop off the un-wanted time part.\n",
    "    # [b, a, cutoff_idx, 1]\n",
    "    if self.at_cutoff and self.cutoff_seconds is not None:\n",
    "      deviation = deviation[:, self.cutoff_idx-1:self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(\n",
    "          should_predict[:, self.cutoff_idx-1:self.cutoff_idx, :])\n",
    "    else:\n",
    "      deviation = deviation[:, :self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(should_predict[:, :self.cutoff_idx, :])\n",
    "    print(\"deviation: \", deviation.shape)\n",
    "    print(\"num_predictions: \", num_predictions.shape)\n",
    "    \n",
    "    # Reduce along time\n",
    "    deviation = tf.reduce_sum(deviation, axis=1)\n",
    "    print(\"deviation: \", deviation.shape)\n",
    "    # Reduce along modes\n",
    "    deviation = self._reduce(deviation, input_batch, predictions)\n",
    "    print(\"deviation: \", deviation.shape)\n",
    "    # [1]\n",
    "    deviation = tf.reduce_sum(deviation)\n",
    "    print(\"deviation: \", deviation.shape)\n",
    "\n",
    "    self.num_predictions.assign_add(num_predictions)\n",
    "    self.total_deviation.assign_add(deviation)\n",
    "\n",
    "  def result(self):\n",
    "    return self.total_deviation / self.num_predictions\n",
    "\n",
    "  def reset_states(self):\n",
    "    self.num_predictions.assign(0)\n",
    "    self.total_deviation.assign(0.0)\n",
    "\n",
    "class MinADE(ADE):\n",
    "  \"\"\"Takes the minimum over all modes.\"\"\"\n",
    "\n",
    "  def _reduce(self, ade_with_modes, input_batch, predictions):\n",
    "    return tf.reduce_min(ade_with_modes, axis=-2)\n",
    "\n",
    "class MLADE(ADE):\n",
    "  \"\"\"Takes the maximum likelihood mode.\"\"\"\n",
    "\n",
    "  def _reduce(self, ade_with_modes, input_batch, predictions):\n",
    "    # Get index of mixture component with highest probability\n",
    "    # [b, a=1, t=1, n]\n",
    "    ml_indices = tf.math.argmax(predictions['mixture_logits'], axis=-1)\n",
    "    a = ade_with_modes.shape[1]\n",
    "    ml_indices = tf.tile(\n",
    "        tf.squeeze(ml_indices, axis=1), [1, a])[..., tf.newaxis]\n",
    "\n",
    "    return tf.gather(\n",
    "        ade_with_modes, indices=ml_indices, batch_dims=2, axis=-2)[..., 0, :]\n",
    "\n",
    "def force_positive(x, eps=1e-6):\n",
    "  return tf.keras.activations.elu(x) + 1. + eps\n",
    "\n",
    "def to_positive_definite_scale_tril(logit_sigma):\n",
    "  tril = tfp.math.fill_triangular(logit_sigma)\n",
    "  scale_tril = tf.linalg.set_diag(\n",
    "      tril,\n",
    "      force_positive(tf.linalg.diag_part(tril)))\n",
    "  return scale_tril\n",
    "\n",
    "def get_position_distribution(model_output):\n",
    "  \"\"\"Multivariate Normal distribution over position.\"\"\"\n",
    "  p_pos = tfp.distributions.MultivariateNormalTriL(\n",
    "      loc=model_output['position'],\n",
    "      scale_tril=to_positive_definite_scale_tril(\n",
    "          model_output['position_raw_scale']))\n",
    "\n",
    "  return p_pos\n",
    "\n",
    "def get_multimodal_position_distribution(model_output):\n",
    "  \"\"\"Multivariate Normal Mixture distribution over position.\"\"\"\n",
    "  p_pos = get_position_distribution(model_output)\n",
    "\n",
    "  p_pos_mm = tfp.distributions.MixtureSameFamily(\n",
    "      mixture_distribution=tfp.distributions.Categorical(\n",
    "          logits=model_output['mixture_logits']),\n",
    "      components_distribution=p_pos)\n",
    "\n",
    "  return p_pos_mm\n",
    "\n",
    "class PositionNegativeLogLikelihood(tf.keras.metrics.Metric):\n",
    "  \"\"\"Position Negative Log Likelihood.\"\"\"\n",
    "\n",
    "  def __init__(self, num_history_steps=6, timestep=0.4,cutoff_seconds=None, at_cutoff=False,\n",
    "               name='PosNLL'):\n",
    "    \"\"\"Initializes the PositionNegativeLogLikelihood metric.\n",
    "\n",
    "    Args:\n",
    "      params: ModelParams\n",
    "      cutoff_seconds: Cutoff up to which time the metric should be calculated\n",
    "        in seconds.\n",
    "      at_cutoff: If True metric will be calculated at cutoff timestep.\n",
    "        Otherwise metric is calculated as average up to cutoff_seconds.\n",
    "      name: Metric name.\n",
    "    \"\"\"\n",
    "    super().__init__(name=name)\n",
    "    self.cutoff_seconds = cutoff_seconds\n",
    "    self.at_cutoff = at_cutoff\n",
    "    if cutoff_seconds is None:\n",
    "      self.cutoff_idx = None\n",
    "    else:\n",
    "      # +1 due to current time step.\n",
    "      self.cutoff_idx = int(\n",
    "          num_history_steps +\n",
    "          cutoff_seconds / timestep) + 1\n",
    "\n",
    "    self.num_predictions = self.add_weight(\n",
    "        name='num_predictions', initializer='zeros')\n",
    "    self.total_deviation = self.add_weight(\n",
    "        name='total_deviation', initializer='zeros')\n",
    "\n",
    "  def update_state(self, input_batch, predictions):\n",
    "    should_predict = tf.cast(tf.math.logical_not(predictions['mask']), tf.float32)\n",
    "    print(\"should_predict: \", should_predict.shape)\n",
    "\n",
    "    p_pos = get_multimodal_position_distribution(predictions)\n",
    "    print(\"p_pos: \", p_pos.shape)\n",
    "\n",
    "    target = input_batch['targets']\n",
    "    target = target[..., :p_pos.event_shape_tensor()[0]]\n",
    "    print(target.shape)\n",
    "\n",
    "    # [b, a, t, n, 1]\n",
    "    per_position_nll = -p_pos.log_prob(target)[..., tf.newaxis]\n",
    "    print(\"per_position_nll: \", per_position_nll)\n",
    "\n",
    "    # Non-observed or past should not contribute to metric.\n",
    "    nll = tf.math.multiply_no_nan(per_position_nll, should_predict)\n",
    "    # Chop off the un-wanted time part.\n",
    "    # [b, a, cutoff_idx, 1]\n",
    "    if self.at_cutoff and self.cutoff_seconds is not None:\n",
    "      nll = nll[:, self.cutoff_idx-1:self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(\n",
    "          should_predict[:, self.cutoff_idx-1::self.cutoff_idx, :])\n",
    "    else:\n",
    "      nll = nll[:, :self.cutoff_idx, :]\n",
    "      num_predictions = tf.reduce_sum(should_predict[:, :self.cutoff_idx, :])\n",
    "    print(\"nll: \", nll.shape)\n",
    "    print(\"num_predictions: \", num_predictions)\n",
    "\n",
    "    # [1]\n",
    "    nll = tf.reduce_sum(nll)\n",
    "    print(\"nll: \", nll.shape)\n",
    "\n",
    "    self.num_predictions.assign_add(num_predictions)\n",
    "    self.total_deviation.assign_add(nll)\n",
    "\n",
    "  def result(self):\n",
    "    return self.total_deviation / self.num_predictions\n",
    "\n",
    "  def reset_states(self):\n",
    "    self.num_predictions.assign(0)\n",
    "    self.total_deviation.assign(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 12:59:21.819339: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2783\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\030TensorSliceDataset:14786\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 15\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "2024-07-08 12:59:21.828322: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:786] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Found an unshardable source dataset: name: \"TensorSliceDataset/_1\"\n",
      "op: \"TensorSliceDataset\"\n",
      "input: \"Placeholder/_0\"\n",
      "attr {\n",
      "  key: \"Toutput_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_DOUBLE\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: 2783\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"is_files\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\030TensorSliceDataset:14786\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: 15\n",
      "        }\n",
      "        dim {\n",
      "          size: 3\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"replicate_on_split\"\n",
      "  value {\n",
      "    b: false\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_DOUBLE\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "model_base_dir = \"\"\n",
    "dt_str = datetime.datetime.now().strftime('%Y-%m-%d_%H:%M:%S')\n",
    "model_dir = os.path.join(model_base_dir, dt_str)\n",
    "os.makedirs(model_dir)\n",
    "ckpt_dir = os.path.join(model_dir, 'ckpts')\n",
    "os.makedirs(ckpt_dir)\n",
    "ckpt_best_dir = os.path.join(model_dir, 'ckpts_best')\n",
    "os.makedirs(ckpt_best_dir)\n",
    "checkpoint_prefix = os.path.join(ckpt_dir, 'ckpt')\n",
    "checkpoint_prefix_best = os.path.join(ckpt_best_dir, 'ckpt')\n",
    "tensorboard_dir = '/tmp/tensorboard'\n",
    "\n",
    "best_eval_loss = tf.Variable(tf.float32.max)\n",
    "checkpoint = tf.train.Checkpoint(model=model,\n",
    "                                   optimizer=optimizer,\n",
    "                                   best_eval_loss=best_eval_loss)\n",
    "checkpoint_best = tf.train.Checkpoint(model=model)\n",
    "best_checkpoint_manager = tf.train.CheckpointManager(checkpoint_best,\n",
    "                                                    checkpoint_prefix_best,\n",
    "                                                    max_to_keep=1)\n",
    "\n",
    "train_summary_writer = tf.summary.create_file_writer(\n",
    "      os.path.join(tensorboard_dir, 'train'))\n",
    "eval_summary_writer = tf.summary.create_file_writer(\n",
    "    os.path.join(tensorboard_dir, 'eval'))\n",
    "\n",
    "\n",
    "batches_per_train_step=1 #25000\n",
    "batches_per_eval_step =1 # 2000\n",
    "eval_every_n_step = 1 #1e4\n",
    "\n",
    "strategy = tf.distribute.OneDeviceStrategy('cpu')\n",
    "\n",
    "dist_train_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
    "dist_eval_dataset = strategy.experimental_distribute_dataset(test_dataset)\n",
    "\n",
    "current_global_step = 0\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model()\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=learning_rate_schedule,\n",
    "        global_clipnorm=1.)\n",
    "    loss_obj = MinNLLPositionMixtureCategoricalCrossentropyLoss()\n",
    "    train_metrics = {\n",
    "    'loss': Mean(),\n",
    "    'loss_position': Mean(),\n",
    "    'min_ade': MinADE(),\n",
    "    'ml_ade': MLADE(),\n",
    "    'pos_nll': PositionNegativeLogLikelihood\n",
    "    }\n",
    "\n",
    "    eval_metrics = {\n",
    "      'loss': Mean(),\n",
    "      'loss_position': Mean(),\n",
    "      'min_ade': MinADE(),\n",
    "      'ml_ade': MLADE(),\n",
    "      'pos_nll': PositionNegativeLogLikelihood\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training and Eval tf.function.\n",
    "@tf.function\n",
    "def train_step(iterator):\n",
    "    \"\"\"Training function.\"\"\"\n",
    "\n",
    "    def step_fn(input_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(input_batch, training=True)\n",
    "            loss_dict = loss_obj(input_batch, predictions)\n",
    "            loss = (loss_dict['loss']\n",
    "                / tf.cast(strategy.num_replicas_in_sync, tf.float32))\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        # Update the training metrics.\n",
    "        # These need special treatments as they are standard keras metrics.\n",
    "        train_metrics['loss'].update_state(loss_dict['loss'])\n",
    "        train_metrics['loss_position'].update_state(loss_dict['position_loss'])\n",
    "        # Our own metrics.\n",
    "        for key in train_metrics:\n",
    "            if key in {'loss', 'loss_position', 'loss_orientation'}:\n",
    "                continue\n",
    "            train_metrics[key].update_state(input_batch, predictions)\n",
    "\n",
    "    for _ in tf.range(tf.constant(batches_per_train_step)):\n",
    "        strategy.run(\n",
    "            step_fn,\n",
    "            args=(next(iterator),),\n",
    "            options=tf.distribute.RunOptions(\n",
    "                experimental_enable_dynamic_batch_size=False))\n",
    "\n",
    "@tf.function\n",
    "def eval_step(iterator):\n",
    "\n",
    "    def step_fn(input_batch):\n",
    "        predictions = model(input_batch, training=False)\n",
    "        loss_dict = loss_obj(input_batch, predictions)\n",
    "        # Update the eval metrics.\n",
    "        # These need special treatments as they are standard keras metrics.\n",
    "        eval_metrics['loss'].update_state(loss_dict['loss'])\n",
    "        eval_metrics['loss_position'].update_state(loss_dict['position_loss'])\n",
    "        # Our own metrics.\n",
    "        for key in eval_metrics:\n",
    "            if key in {'loss', 'loss_position', 'loss_orientation'}:\n",
    "                continue\n",
    "            eval_metrics[key].update_state(input_batch, predictions)\n",
    "\n",
    "    for _ in tf.range(tf.constant(batches_per_eval_step)):\n",
    "        strategy.run(\n",
    "            step_fn,\n",
    "            args=(next(iterator),),\n",
    "            options=tf.distribute.RunOptions(\n",
    "                experimental_enable_dynamic_batch_size=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 12:59:27.882139: W tensorflow/core/framework/dataset.cc:956] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 32\n",
      "sequence_length: 15\n",
      "mask aaray: (32, 15)\n",
      "position loss:  {'loss': <tf.Tensor 'PartitionedCall:0' shape=() dtype=float32>, 'position_loss': <tf.Tensor 'PartitionedCall:1' shape=() dtype=float32>}\n",
      "mixture loss:  {'loss': <tf.Tensor 'PartitionedCall_1:0' shape=() dtype=float32>, 'position_loss': <tf.Tensor 'PartitionedCall_1:1' shape=() dtype=float32>}\n",
      "loss:  Tensor(\"add:0\", shape=(), dtype=float32)\n",
      "should_predict:  (32, 15)\n",
      "target:  (32, 15, 3)\n",
      "per_position_ade:  (32, 15, 5, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_966172/812415416.py\", line 24, in step_fn  *\n        train_metrics[key].update_state(input_batch, predictions)\n    File \"/tmp/ipykernel_966172/1282607232.py\", line 63, in update_state  *\n        deviation = tf.math.multiply_no_nan(per_position_ade,\n\n    ValueError: Dimensions must be equal, but are 15 and 32 for '{{node while/multiply_no_nan}} = MulNoNan[T=DT_FLOAT](while/Sqrt, while/strided_slice_2)' with input shapes: [32,15,5,1], [32,1,15].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X25sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m actual_steps \u001b[39m=\u001b[39m step \u001b[39m*\u001b[39m batches_per_train_step\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X25sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X25sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Run training SGD over train_param.batches_per_train_step batches.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X25sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# optimizer.iterations = step * train_param.batches_per_train_step.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     train_step(train_iter)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X25sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# Writing metrics to tensorboard.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/test_train.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filek0_bstsv.py:74\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     72\u001b[0m     ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(strategy)\u001b[39m.\u001b[39mrun, (ag__\u001b[39m.\u001b[39mld(step_fn),), \u001b[39mdict\u001b[39m(args\u001b[39m=\u001b[39m(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(\u001b[39mnext\u001b[39m), (ag__\u001b[39m.\u001b[39mld(iterator),), \u001b[39mNone\u001b[39;00m, fscope),), options\u001b[39m=\u001b[39mag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mRunOptions, (), \u001b[39mdict\u001b[39m(experimental_enable_dynamic_batch_size\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), fscope)), fscope)\n\u001b[1;32m     73\u001b[0m _ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 74\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mrange, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mconstant, (ag__\u001b[39m.\u001b[39mld(batches_per_train_step),), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope), \u001b[39mNone\u001b[39;00m, loop_body_1, get_state_3, set_state_3, (), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m})\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filek0_bstsv.py:72\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.loop_body_1\u001b[0;34m(itr_1)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloop_body_1\u001b[39m(itr_1):\n\u001b[1;32m     71\u001b[0m     _ \u001b[39m=\u001b[39m itr_1\n\u001b[0;32m---> 72\u001b[0m     ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(strategy)\u001b[39m.\u001b[39;49mrun, (ag__\u001b[39m.\u001b[39;49mld(step_fn),), \u001b[39mdict\u001b[39;49m(args\u001b[39m=\u001b[39;49m(ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(\u001b[39mnext\u001b[39;49m), (ag__\u001b[39m.\u001b[39;49mld(iterator),), \u001b[39mNone\u001b[39;49;00m, fscope),), options\u001b[39m=\u001b[39;49mag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(tf)\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mRunOptions, (), \u001b[39mdict\u001b[39;49m(experimental_enable_dynamic_batch_size\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m), fscope)), fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filek0_bstsv.py:62\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.step_fn\u001b[0;34m(input_batch)\u001b[0m\n\u001b[1;32m     60\u001b[0m key \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m continue_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mUndefined(\u001b[39m'\u001b[39m\u001b[39mcontinue_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m ag__\u001b[39m.\u001b[39mfor_stmt(ag__\u001b[39m.\u001b[39mld(train_metrics), \u001b[39mNone\u001b[39;00m, loop_body, get_state_2, set_state_2, (), {\u001b[39m'\u001b[39m\u001b[39miterate_names\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mkey\u001b[39m\u001b[39m'\u001b[39m})\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filek0_bstsv.py:59\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.step_fn.<locals>.loop_body\u001b[0;34m(itr)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39melse_body_1\u001b[39m():\n\u001b[1;32m     58\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m ag__\u001b[39m.\u001b[39;49mif_stmt(ag__\u001b[39m.\u001b[39;49mnot_(continue_), if_body_1, else_body_1, get_state_1, set_state_1, (), \u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filek0_bstsv.py:55\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_step.<locals>.step_fn.<locals>.loop_body.<locals>.if_body_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mif_body_1\u001b[39m():\n\u001b[0;32m---> 55\u001b[0m     ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(train_metrics)[ag__\u001b[39m.\u001b[39;49mld(key)]\u001b[39m.\u001b[39;49mupdate_state, (ag__\u001b[39m.\u001b[39;49mld(input_batch), ag__\u001b[39m.\u001b[39;49mld(predictions)), \u001b[39mNone\u001b[39;49;00m, fscope_1)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/metrics_utils.py:77\u001b[0m, in \u001b[0;36mupdate_state_wrapper.<locals>.decorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTrying to run metric.update_state in replica context when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe metric was not created in TPUStrategy scope. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMake sure the keras Metric is created in TPUstrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mscope. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m tf_utils\u001b[39m.\u001b[39mgraph_context_for_symbolic_tensors(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 77\u001b[0m     update_op \u001b[39m=\u001b[39m update_state_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m update_op \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# update_op will be None in eager execution.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     metric_obj\u001b[39m.\u001b[39madd_update(update_op)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/metrics/base_metric.py:140\u001b[0m, in \u001b[0;36mMetric.__new__.<locals>.update_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m control_status \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    137\u001b[0m ag_update_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    138\u001b[0m     obj_update_state, control_status\n\u001b[1;32m    139\u001b[0m )\n\u001b[0;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m ag_update_state(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filejusjeetf.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__update_state\u001b[0;34m(self, input_batch, predictions)\u001b[0m\n\u001b[1;32m     13\u001b[0m per_position_ade \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(distance_error), (ag__\u001b[39m.\u001b[39mld(target)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mnewaxis, :], ag__\u001b[39m.\u001b[39mld(predictions)[\u001b[39m'\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m'\u001b[39m]), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     14\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mper_position_ade: \u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(per_position_ade)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 15\u001b[0m deviation \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mmultiply_no_nan, (ag__\u001b[39m.\u001b[39mld(per_position_ade), ag__\u001b[39m.\u001b[39mld(should_predict)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mnewaxis, :]), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m ag__\u001b[39m.\u001b[39mld(\u001b[39mprint\u001b[39m)(\u001b[39m'\u001b[39m\u001b[39mdeviation: \u001b[39m\u001b[39m'\u001b[39m, ag__\u001b[39m.\u001b[39mld(deviation)\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_state\u001b[39m():\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_966172/812415416.py\", line 24, in step_fn  *\n        train_metrics[key].update_state(input_batch, predictions)\n    File \"/tmp/ipykernel_966172/1282607232.py\", line 63, in update_state  *\n        deviation = tf.math.multiply_no_nan(per_position_ade,\n\n    ValueError: Dimensions must be equal, but are 15 and 32 for '{{node while/multiply_no_nan}} = MulNoNan[T=DT_FLOAT](while/Sqrt, while/strided_slice_2)' with input shapes: [32,15,5,1], [32,1,15].\n"
     ]
    }
   ],
   "source": [
    " # 5) Actual Training Loop\n",
    "train_iter = iter(dist_train_dataset)\n",
    "eval_iter = iter(dist_eval_dataset)\n",
    "total_train_steps = 2 # 1e6\n",
    "num_train_iter = (\n",
    "    total_train_steps // batches_per_train_step)\n",
    "current_train_iter = (\n",
    "    current_global_step // batches_per_train_step)\n",
    "\n",
    "logging.info('Beginning training.')\n",
    "for step in range(current_train_iter, num_train_iter):\n",
    "    # Actual number of SGD steps.\n",
    "    actual_steps = step * batches_per_train_step\n",
    "    with train_summary_writer.as_default():\n",
    "        # Run training SGD over train_param.batches_per_train_step batches.\n",
    "        # optimizer.iterations = step * train_param.batches_per_train_step.\n",
    "        train_step(train_iter)\n",
    "        # Writing metrics to tensorboard.\n",
    "        if step % 1 == 0:\n",
    "            for key in train_metrics:\n",
    "                tf.summary.scalar(\n",
    "                    key, train_metrics[key].result(), step=optimizer.iterations)\n",
    "\n",
    "            if isinstance(optimizer, tf.keras.optimizers.experimental.Optimizer):\n",
    "                learning_rate = optimizer.learning_rate\n",
    "            else:\n",
    "                learning_rate = optimizer.lr(optimizer.iterations)\n",
    "            tf.summary.scalar(\n",
    "                'learning_rate',\n",
    "                learning_rate,\n",
    "                step=optimizer.iterations)\n",
    "            logging.info('Training step %d', step)\n",
    "            logging.info('Training loss: %.4f, ADE: %.4f',\n",
    "                            train_metrics['loss'].result().numpy(),\n",
    "                            train_metrics['min_ade'].result().numpy())\n",
    "            # Reset metrics.\n",
    "            for key in train_metrics:\n",
    "                train_metrics[key].reset_states()\n",
    "\n",
    "    # Evaluation.\n",
    "    if actual_steps % eval_every_n_step == 0:\n",
    "        logging.info('Evaluating step %d over %d random eval samples', step,\n",
    "                    batches_per_eval_step * batch_size)\n",
    "        with eval_summary_writer.as_default():\n",
    "            eval_step(eval_iter)\n",
    "            for key in eval_metrics:\n",
    "                tf.summary.scalar(\n",
    "                    key, eval_metrics[key].result(), step=optimizer.iterations)\n",
    "            logging.info('Eval loss: %.4f, ADE: %.4f',\n",
    "                            eval_metrics['loss'].result().numpy(),\n",
    "                            eval_metrics['min_ade'].result().numpy())\n",
    "\n",
    "            if eval_metrics['loss'].result() < best_eval_loss:\n",
    "                best_eval_loss.assign(eval_metrics['loss'].result())\n",
    "                best_checkpoint_manager.save()\n",
    "\n",
    "            # Reset metrics.\n",
    "            for key in eval_metrics:\n",
    "                eval_metrics[key].reset_states()\n",
    "\n",
    "            # Save model.\n",
    "            checkpoint_name = checkpoint.save(checkpoint_prefix)\n",
    "            logging.info('Saved checkpoint to %s', checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
