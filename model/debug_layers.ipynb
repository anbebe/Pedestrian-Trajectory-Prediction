{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 10:22:44.071071: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-06 10:22:45.662264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from preprocess_data import load_data\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_1759/1714552977.py:1: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 10:23:23.598865: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:23.813376: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:23.814041: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:23.817459: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:23.818300: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:23.819643: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:25.584595: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:25.585254: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:25.585288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1726] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-08-06 10:23:25.585912: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-06 10:23:25.585954: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 16222427923489919739\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.experimental.load(\n",
    "    \"/home/annalena/PedestrianTrajectoryPrediction/tests/train_dataset_scaled\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6608"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "    \"\"\" Applies the masking to the sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "    def calc_hidden_mask(self, batch_size=32, sequence_length=15):\n",
    "        # create mask array, False = needs to be predicted\n",
    "        mask_arrays = []\n",
    "        #print(\"batch_size:\", batch_size)\n",
    "        #print(\"sequence_length:\", sequence_length)\n",
    "        for i in range(batch_size):\n",
    "          mask_arr = [True] * 6 + [False] * (sequence_length-6)\n",
    "          # hide 0-2 in between steps (for lazyness whole datapoint)\n",
    "          hidden_nr = np.random.randint(3)\n",
    "          hidden_idx = np.random.choice(range(6),hidden_nr, replace=False)\n",
    "          for v in hidden_idx:\n",
    "              mask_arr[v] = False\n",
    "          mask_arrays.append(mask_arr)\n",
    "        #print(\"mask aaray:\", np.asarray(mask_arrays).shape)\n",
    "        return np.asarray(mask_arrays)\n",
    "\n",
    "    def call(self,\n",
    "           raw_input_batch: Tuple[tf.Tensor, tf.Tensor],\n",
    "           is_hidden: Optional[tf.Tensor] = None) -> Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "        input_batch = raw_input_batch\n",
    "  \n",
    "\n",
    "        batch_size = tf.shape(input_batch[0])[0]\n",
    "        sequence_length = tf.shape(input_batch[0])[1]\n",
    "        feature_size1 = tf.shape(input_batch[0])[2]\n",
    "        feature_size2 = tf.shape(input_batch[1])[2]\n",
    "\n",
    "        mask = self.calc_hidden_mask() #tf.convert_to_tensor\n",
    "\n",
    "        mask_tensor = tf.constant(mask, dtype=tf.bool)\n",
    "\n",
    "        # Expand dimensions of mask to match the input tensor\n",
    "        #expanded_mask = tf.expand_dims(mask_tensor, axis=0)  # Add batch dimension\n",
    "        expanded_mask = tf.expand_dims(mask_tensor, axis=-1)  # Add feature dimension\n",
    "\n",
    "        # Broadcast mask to match input tensor shape\n",
    "        broadcasted_mask_pos = tf.broadcast_to(expanded_mask, (batch_size, sequence_length, feature_size1))\n",
    "        broadcasted_mask_pose = tf.broadcast_to(expanded_mask, (batch_size, sequence_length, feature_size2))\n",
    "\n",
    "        #batch_mask = tf.broadcast_to(expanded_mask, (batch_size, sequence_length))\n",
    "\n",
    "        # Apply mask\n",
    "        masked_input_pos = tf.where(broadcasted_mask_pos, input_batch[0], tf.zeros_like(input_batch[0]))\n",
    "        masked_input_pose = tf.where(broadcasted_mask_pose, input_batch[1], tf.zeros_like(input_batch[1]))\n",
    "        targets = tf.where(tf.math.logical_not(broadcasted_mask_pos), input_batch[0], tf.zeros_like(input_batch[0]))\n",
    "\n",
    "      # scale\n",
    "        #scale_factor = 100.0\n",
    "        #masked_input_pos = tf.math.scalar_mul(scale_factor, masked_input_pos)\n",
    "        #masked_input_pose = tf.math.scalar_mul(scale_factor, masked_input_pose) \n",
    "\n",
    "        return (masked_input_pos, masked_input_pose), mask, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessLayer_2(tf.keras.layers.Layer):\n",
    "    \"\"\" Applies the masking to the sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_steps=15, num_history_steps=5):\n",
    "        super().__init__()\n",
    "        self._num_steps = num_steps\n",
    "        self._num_history_steps = num_history_steps\n",
    "\n",
    "    # from BPIsHiddenGenerator\n",
    "    def calc_hidden_mask(self, batch_size=32, sequence_length=15):\n",
    "        \"\"\"Returns the is_hidden tensor for behavior prediction.\n",
    "\n",
    "        Always returns 0 (not hidden) for history/current steps and 1 (hidden)\n",
    "        for future steps.\n",
    "\n",
    "        Args:\n",
    "        num_agents: Number of agents in the scene.\n",
    "        train_progress: A float between 0 to 1 representing the overall progress\n",
    "            of training. This float can be current_step / total_training_steps. This\n",
    "            float can be used for training w/ an annealing schedule.\n",
    "\n",
    "        Returns:\n",
    "        is_hidden: The is_hidden tensor for behavior prediction.\n",
    "        \"\"\"\n",
    "        # [1, a, t, 1].\n",
    "        is_hidden = np.ones([1, self._num_steps, 1],\n",
    "                            dtype=bool)\n",
    "        is_hidden[:, :self._num_history_steps + 1, :] = False\n",
    "        return is_hidden\n",
    "    \n",
    "    def _set_elems_to_value(self, target, should_set, new_val):\n",
    "        target = tf.where(should_set, tf.cast(new_val, target.dtype), target)\n",
    "        return target\n",
    "\n",
    "    def call(self,\n",
    "           raw_input_batch: Tuple[tf.Tensor, tf.Tensor]) -> Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "        input_batch = raw_input_batch\n",
    "  \n",
    "        is_hidden = self.calc_hidden_mask() #tf.convert_to_tensor\n",
    "        print(\"is_hidden\", is_hidden.shape)\n",
    "\n",
    "        input_batch_new = []\n",
    "\n",
    "        for feature_batch in input_batch:\n",
    "            input_batch_new.append(tf.where(\n",
    "            tf.math.is_nan(feature_batch),\n",
    "            tf.zeros_like(feature_batch),\n",
    "            feature_batch))\n",
    "        \n",
    "        print(\"input_batch_new 0\", input_batch_new[0].shape)\n",
    "        print(\"input_batch_new 1\", input_batch_new[1].shape)\n",
    "        \n",
    "        t = input_batch_new[0]\n",
    "        if t.dtype.is_floating:\n",
    "            has_data =  tf.math.logical_not(\n",
    "                tf.reduce_any(tf.math.is_nan(t), axis=-1, keepdims=True))\n",
    "        else:\n",
    "            has_data =  tf.math.logical_not(\n",
    "                tf.reduce_any(t == t.dtype.min, axis=-1, keepdims=True))\n",
    "            \n",
    "        print(\"has_data\", has_data.shape)\n",
    "        \n",
    "        has_historic_data = tf.reduce_any( has_data[..., :self._num_history_steps + 1, :], axis=-2, keepdims=True)\n",
    "        should_predict = tf.logical_and(is_hidden,tf.logical_and(has_data, has_historic_data))\n",
    "\n",
    "        print(\"has_historic_data\", has_historic_data.shape)\n",
    "        print(\"should_predict\", should_predict.shape)\n",
    "        \n",
    "        feature_is_padded = tf.logical_not(has_data)\n",
    "        masked_input_pos = self._set_elems_to_value( input_batch_new[0], tf.logical_or(feature_is_padded,is_hidden), 0.)\n",
    "        masked_input_pose = self._set_elems_to_value( input_batch_new[1], tf.logical_or(feature_is_padded,is_hidden), 0.)\n",
    "\n",
    "        print(\"masked_input_pos\", masked_input_pos.shape)\n",
    "        print(\"masked_input_pose\", masked_input_pose.shape)\n",
    "        targets = input_batch_new[0]\n",
    "\n",
    "        output_dict={\n",
    "            \"masked_inputs\": (masked_input_pos, masked_input_pose),\n",
    "            \"has_data\": has_data, \n",
    "            \"is_hidden\": is_hidden, \n",
    "            \"targets\": targets,\n",
    "            \"has_historic_data\": has_historic_data\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_hidden (1, 15, 1)\n",
      "input_batch_new 0 (32, 15, 3)\n",
      "input_batch_new 1 (32, 15, 51)\n",
      "has_data (32, 15, 1)\n",
      "has_historic_data (32, 1, 1)\n",
      "should_predict (32, 15, 1)\n",
      "masked_input_pos (32, 15, 3)\n",
      "masked_input_pose (32, 15, 51)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 15207561690904214291\n"
     ]
    }
   ],
   "source": [
    "class Test(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.preprocess_layer = PreprocessLayer_2() \n",
    "\n",
    "    def call(self, input_batch, training = False):\n",
    "        (input_1, input_2) = input_batch\n",
    "        output_dict = self.preprocess_layer((input_1, input_2)) # output shape (batch_size, 15, 3)\n",
    "\n",
    "        return output_dict\n",
    "    \n",
    "strategy = tf.distribute.OneDeviceStrategy('gpu')\n",
    "with strategy.scope():\n",
    "    model = Test()\n",
    "    model.compile(loss='msle', optimizer='rmsprop')\n",
    "\n",
    "for (batch_x1, batch_x2) in train_dataset.take(1):\n",
    "    input_batch = (batch_x1, batch_x2)\n",
    "    output = model(input_batch, training=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"is_hidden\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Sinusoidal Embedding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/embedding.py    \"\"\"\n",
    "class SinusoidalEmbeddingLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Sinusoidal Postional Embedding for xyz and time.\"\"\"\n",
    "\n",
    "  def __init__(self, min_freq=4, max_freq=256, hidden_size=256):\n",
    "    super().__init__()\n",
    "    self.min_freq = float(min_freq)\n",
    "    self.max_freq = float(max_freq)\n",
    "    self.hidden_size = hidden_size\n",
    "    if hidden_size % 2 != 0:\n",
    "      raise ValueError('hidden_size ({hidden_size}) must be divisible by 2.')\n",
    "    self.num_freqs_int32 = hidden_size // 2\n",
    "    self.num_freqs = tf.cast(self.num_freqs_int32, dtype=tf.float32)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    log_freq_increment = (\n",
    "        tf.math.log(float(self.max_freq) / float(self.min_freq)) /\n",
    "        tf.maximum(1.0, self.num_freqs - 1))\n",
    "    # [num_freqs]\n",
    "    self.inv_freqs = self.min_freq * tf.exp(\n",
    "        tf.range(self.num_freqs, dtype=tf.float32) * -log_freq_increment)\n",
    "\n",
    "  def call(self, input_tensor):\n",
    "    \n",
    "    # [batch_size, sequence_length, feature_size, num_freqs]\n",
    "    input_tensor = tf.expand_dims(input_tensor, axis=-1)\n",
    "    input_tensor = tf.repeat(input_tensor, self.num_freqs_int32, axis=-1)\n",
    "\n",
    "    # [batch_size, sequence_length, feature_size, hidden_size]\n",
    "    embedded = tf.concat([\n",
    "        tf.sin(input_tensor * self.inv_freqs),\n",
    "        tf.cos(input_tensor * self.inv_freqs)\n",
    "    ], axis=-1)\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Agent Position Encoding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/agent_feature_encoder.py    \"\"\"\n",
    "class AgentPositionEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes agents spatial positions.\"\"\"\n",
    "\n",
    "  def __init__(self, output_shape, embedding_size):\n",
    "    \n",
    "    super().__init__()\n",
    "\n",
    "    #self.embedding_layer = SinusoidalEmbeddingLayer(\n",
    "       # hidden_size=embedding_size) # output_shape (batch_sie, sequence_length, feature size, hidden_size)\n",
    "    self.embedding_layer = keras_nlp.layers.SinePositionEncoding(max_wavelength=10000)\n",
    "    #self.layer_norm = tf.keras.layers.LayerNormalization(axis=-1)  \n",
    "    self.mlp = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        \n",
    "        activation=None)\n",
    "\n",
    "  \"\"\"def call(self, input_batch):\n",
    "    normalized_input = input_batch[0] #self.layer_norm(input_batch[0])\n",
    "    embedded_input = self.embedding_layer(normalized_input)\n",
    "    return self.mlp(embedded_input)\"\"\"\n",
    "  def call(self, input_dict):\n",
    "    is_hidden = input_dict[\"is_hidden\"]\n",
    "    has_data = input_dict[\"has_data\"]\n",
    "    input_batch = input_dict[\"masked_inputs\"]\n",
    "    not_is_hidden = tf.logical_not(is_hidden)\n",
    "    mask = tf.logical_and(has_data, not_is_hidden)\n",
    "    mask = tf.repeat(mask, tf.shape(input_batch[0])[-1], axis=-1)\n",
    "    return self.mlp(self.embedding_layer(input_batch[0]))[..., tf.newaxis, :], mask\n",
    "class AgentTemporalEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes agents temporal positions.\"\"\"\n",
    "\n",
    "  def __init__(self,output_shape, embedding_size, num_steps):\n",
    "    super().__init__()\n",
    "    #self.embedding_layer = SinusoidalEmbeddingLayer(\n",
    "        #max_freq=num_steps,\n",
    "        #hidden_size=embedding_size)\n",
    "    self.embedding_layer = keras_nlp.layers.SinePositionEncoding(max_wavelength=10000)\n",
    "\n",
    "    self.mlp = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "\n",
    "  def _get_temporal_embedding(self, input_batch):\n",
    "    # This weird thing is for exporting and loading keras model...\n",
    "    b = tf.shape(input_batch[0])[0]\n",
    "    num_steps = tf.shape(input_batch[0])[1]\n",
    "\n",
    "    t = tf.range(0, num_steps, dtype=tf.float32)\n",
    "    t = t[tf.newaxis, :]\n",
    "    t = tf.tile(t, [b, 1])\n",
    "    return self.embedding_layer(t[..., tf.newaxis])\n",
    "\n",
    "  def call(self, input_dict):\n",
    "    has_data = input_dict[\"has_data\"]\n",
    "    input_batch = input_dict[\"masked_inputs\"]\n",
    "    return (self.mlp(self._get_temporal_embedding(input_batch))[..., tf.newaxis, :],\n",
    "            tf.ones_like(has_data))\n",
    "  \n",
    "\n",
    "  \"\"\" Adapted Agent Keypoint Encoding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/agent_feature_encoder.py    \"\"\"\n",
    "class AgentKeypointsEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes the agent's keypoints.\"\"\"\n",
    "\n",
    "  def __init__(self, output_shape, embedding_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mlp1 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "  \"\"\"def call(self, input_batch, training=None):\n",
    "\n",
    "    keypoints = input_batch[1]\n",
    "\n",
    "    out = self.mlp1(keypoints)\n",
    "\n",
    "    return out\"\"\"\n",
    "  def call(self, input_dict, training=None):\n",
    "    is_hidden = input_dict[\"is_hidden\"]\n",
    "    has_data = input_dict[\"has_data\"]\n",
    "    input_batch = input_dict[\"masked_inputs\"]\n",
    "    not_is_hidden = tf.logical_not(is_hidden)\n",
    "    mask = tf.logical_and(has_data, not_is_hidden)\n",
    "\n",
    "    keypoints = input_batch[1]\n",
    "\n",
    "    out = self.mlp1(keypoints)[..., tf.newaxis, :]\n",
    "\n",
    "    return out, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureConcatAgentEncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Independently encodes features and attends to them.\n",
    "\n",
    "  Agent features are cross-attended with a learned query or hidden_vecs instead\n",
    "  of MLP.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_length, batch_size=32, hidden_size=128, num_heads=4, ln_eps=1e-6, transformer_ff_dim=128, drop_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    # Cross Attention and learned query.\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None,\n",
    "    )\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    self.agent_feature_embedding_layers = []\n",
    "    # Position Feature [batch, sequence_len, feature_size, hidden_size]\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentPositionEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "    # Feature Embedding - keypoints [batch, sequence_len, hidden_size]\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentKeypointsEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "\n",
    "    # Temporal Embedding [batch, sequence_len, 1, hidden_size]\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentTemporalEncoder(output_shape=hidden_size-8, embedding_size=hidden_size, num_steps=input_length))\n",
    "\n",
    "\n",
    "  def call(self, input_dict, training = None):\n",
    "    is_hidden = input_dict[\"is_hidden\"]\n",
    "    has_data = input_dict[\"has_data\"]\n",
    "    input_batch = input_dict[\"masked_inputs\"]\n",
    "    layer_embeddings = []\n",
    "    for layer in self.agent_feature_embedding_layers:\n",
    "      layer_embedding, _ = layer(input_dict, training=training)\n",
    "      print(layer_embedding.shape)\n",
    "      layer_embedding = layer_embedding[...,0,:]\n",
    "      print(layer_embedding.shape)\n",
    "      layer_embeddings.append(layer_embedding)\n",
    "\n",
    "    embedding = tf.concat(layer_embeddings, axis=-1)\n",
    "    print(\"concat embedding\", embedding.shape)\n",
    "\n",
    "    out = self.ff_layer2(embedding)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Feature Encoding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/agent_encoder.py    \"\"\"\n",
    "class FeatureAttnAgentEncoderLearnedLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Independently encodes features and attends to them.\n",
    "\n",
    "  Agent features are cross-attended with a learned query or hidden_vecs instead\n",
    "  of MLP.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_length=15, batch_size=32, hidden_size=128, num_heads=4, ln_eps=1e-6, transformer_ff_dim=128, drop_prob=0.1):\n",
    "    super(FeatureAttnAgentEncoderLearnedLayer, self).__init__()\n",
    "\n",
    "    self.batch_size=batch_size\n",
    "    self.input_length = input_length\n",
    "    self.num_heads=num_heads\n",
    "\n",
    "    # Cross Attention and learned query.\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        #key_dim=360//num_heads,  # \"large\" to prevent a bottleneck\n",
    "        #value_dim=360//num_heads)\n",
    "        key_dim= hidden_size,\n",
    "        attention_axes=1)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f',\n",
    "        output_shape=transformer_ff_dim,\n",
    "        bias_axes='f',\n",
    "        activation='relu')\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    self.agent_feature_embedding_layers = []\n",
    "    # Position Feature\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentPositionEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "    # Feature Embedding - keypoints\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentKeypointsEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "\n",
    "    # Temporal Embedding\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentTemporalEncoder(output_shape=hidden_size-8, embedding_size=hidden_size, num_steps=input_length))\n",
    "\n",
    "    # [1, 1, h]\n",
    "    self.learned_query_vec = tf.Variable(\n",
    "        tf.random_uniform_initializer(\n",
    "            minval=-1., maxval=1.)(shape=[1, 1, 1, hidden_size]),\n",
    "        trainable=True,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "  def _build_learned_query(self, input_dict):\n",
    "    \"\"\"Converts self.learned_query_vec into a learned query vector.\"\"\"\n",
    "    # This weird thing is for exporting and loading keras model...\n",
    "    b = tf.shape(input_dict[\"masked_inputs\"][0])[0]\n",
    "    num_steps = tf.shape(input_dict[\"masked_inputs\"][0])[1]\n",
    "\n",
    "    # [b, num_steps, h]s\n",
    "    return tf.tile(self.learned_query_vec, [b, num_steps, 1,1])\n",
    "\n",
    "  def call(self, input_dict,training = None):\n",
    "    \n",
    "    layer_embeddings = []\n",
    "    layer_masks = []\n",
    "    for layer in self.agent_feature_embedding_layers:\n",
    "      layer_embedding, layer_mask = layer(input_dict, training=training)\n",
    "      print(\"layer_mask\", layer_mask.shape)\n",
    "      layer_embeddings.append(layer_embedding)\n",
    "      layer_masks.append(layer_mask)\n",
    "    embedding = tf.concat(layer_embeddings, axis=2)\n",
    "    print(\"embedding:\", embedding.shape)\n",
    "\n",
    "    b = tf.shape(embedding)[0]\n",
    "    t = tf.shape(embedding)[1]\n",
    "    n = tf.shape(embedding)[2]\n",
    "\n",
    "    # [1, 1, 1, N, 8]\n",
    "    one_hot = tf.one_hot(tf.range(0, n), 8)[None, None]\n",
    "    print(\"one_hot:\", one_hot.shape)\n",
    "    # [b, a, t, N, 8]\n",
    "    one_hot_id = tf.tile(one_hot, (b, t, 1, 1))\n",
    "    print(\"one_hot_id:\", one_hot_id.shape)\n",
    "\n",
    "    embedding = tf.concat([embedding, one_hot_id], axis=-1)\n",
    "    print(\"embedding:\", embedding.shape)\n",
    "\n",
    "    attention_mask = tf.concat(layer_masks, axis=-1)\n",
    "    print(\"attention_mask:\", attention_mask.shape)\n",
    "\n",
    "    # [b, a, t, num_heads, 1, num_features] <- broadcasted\n",
    "    # Newaxis for num_heads, num_features\n",
    "    attention_mask = attention_mask[..., tf.newaxis, tf.newaxis,:]\n",
    "    print(\"attention_mask:\", attention_mask.shape)\n",
    "    attention_mask = tf.reshape(attention_mask, [b, 1, t, n])\n",
    "    print(\"attention_mask:\", attention_mask.shape)\n",
    "\n",
    "    learned_query = self._build_learned_query(input_dict)\n",
    "    print(\"learned_query:\", learned_query.shape)\n",
    "\n",
    "    # Attention along axis 3\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=learned_query,\n",
    "        key=embedding,\n",
    "        value=embedding,\n",
    "        attention_mask=attention_mask,\n",
    "        return_attention_scores=True)\n",
    "    print(\"attn_out shape: \", attn_out.shape)\n",
    "    # [b, t, h]\n",
    "    attn_out = attn_out[..., 0, :]\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_dropout(out, training=training)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSelfAlignmentLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Enables agent to become aware of its temporal identity.\n",
    "\n",
    "  Agent features are cross-attended with a learned query in temporal dimension.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads=8,\n",
    "               hidden_size=128,\n",
    "               ln_eps=1e-6,\n",
    "               ff_dim=128):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_heads=num_heads\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(f'hidden_size ({hidden_size}) must be an integer '\n",
    "                       f'times bigger than num_heads ({num_heads}).')\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=ff_dim // num_heads,\n",
    "        #value_dim = ff_dim // num_heads,\n",
    "        attention_axes=1)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    # [1, 1, h]\n",
    "    self.learned_query_vec = tf.Variable(\n",
    "        tf.random_uniform_initializer(\n",
    "            minval=-1., maxval=1.)(shape=[1, 1, hidden_size]),\n",
    "        trainable=True,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "  def build_learned_query(self, input_dict):\n",
    "    \"\"\"Converts self.learned_query_vec into a learned query vector.\"\"\"\n",
    "    # This weird thing is for exporting and loading keras model...\n",
    "    b = tf.shape(input_dict['hidden_vecs'])[0]\n",
    "    t = tf.shape(input_dict['hidden_vecs'])[1]\n",
    "\n",
    "    # [b, t, 1, h]\n",
    "    return tf.tile(self.learned_query_vec, [b, t, 1])\n",
    "\n",
    "  def call(self, input_dict):\n",
    "    # [b, t, h]\n",
    "    hidden_vecs = input_dict['hidden_vecs']\n",
    "\n",
    "    # Expand the attention mask with new dims so that Keras can broadcast to\n",
    "    # the same shape as the attn_score: [b, num_heads, a, t, a, t].\n",
    "    # attn_mask shape: [b, 1, 1, 1 t,]\n",
    "    # True means the position participate in the attention while all\n",
    "    # False positions are ignored.\n",
    "    \n",
    "    #print(\"input batch shape: \", hidden_vecs.shape)\n",
    "    \n",
    "    has_historic_data = tf.logical_and(\n",
    "          input_dict['has_historic_data'][..., 0],\n",
    "          tf.logical_not(input_dict['is_hidden'][..., 0]))\n",
    "    attn_mask = has_historic_data[:, tf.newaxis, tf.newaxis, :]\n",
    "    # [b, t, 1, h]\n",
    "    learned_query = self.build_learned_query(input_dict)\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=learned_query,\n",
    "        key=hidden_vecs,\n",
    "        value=hidden_vecs,\n",
    "        attention_mask=attn_mask,\n",
    "        return_attention_scores=True)\n",
    "\n",
    "    attn_out = self.attn_ln(attn_out + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnTransformerLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Performs full self-attention across the agent and time dimensions.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      num_heads=8,\n",
    "      hidden_size=128,\n",
    "      drop_prob=0.1,\n",
    "      ln_eps=1e-6,\n",
    "      ff_dim=128,\n",
    "      mask=False,\n",
    "      flatten=False,\n",
    "      multimodality_induced=False,\n",
    "  ):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.mask = mask\n",
    "    self.flatten = flatten\n",
    "    self.multimodality_induced = multimodality_induced\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(\n",
    "          f'hidden_size ({hidden_size}) must be an integer '\n",
    "          f'times bigger than num_heads ({num_heads}).'\n",
    "      )\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=1,\n",
    "    )  # Full Attention time\n",
    "    self.attn_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu'\n",
    "    )\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None,\n",
    "    )\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "  def call(self, input_dict, training=None):\n",
    "    # [b, t, h] or [b, t, n, h]\n",
    "    hidden_vecs = input_dict[\"hidden_vecs\"]\n",
    "    print(\"hidden_vecs\", hidden_vecs.shape)\n",
    "\n",
    "    if self.flatten:\n",
    "      h_shape = tf.shape(hidden_vecs)\n",
    "      b = h_shape[0]\n",
    "      t = h_shape[1]\n",
    "      h = h_shape[-1]\n",
    "\n",
    "      if self.multimodality_induced:\n",
    "        n = h_shape[2]\n",
    "        hidden_vecs = tf.reshape(hidden_vecs, (b, -1, n, h))\n",
    "      else:\n",
    "        hidden_vecs = tf.reshape(hidden_vecs, (b, -1, h))\n",
    "\n",
    "    # Expand the attention mask with new dims so that Keras can broadcast to\n",
    "    # the same shape as the attn_score: [b, num_heads, a, t, a, t].\n",
    "    # attn_mask shape: [b, 1, 1, 1, a, t,]\n",
    "    # True means the position participate in the attention while all\n",
    "    # False positions are ignored.\n",
    "    if not self.mask:\n",
    "      attn_mask = None\n",
    "    else:\n",
    "      print(\"has_historic_data\", input_dict['has_historic_data'][..., 0].shape)\n",
    "      has_historic_data = input_dict['has_historic_data'][..., 0]\n",
    "      attn_mask = has_historic_data[:, tf.newaxis, tf.newaxis, :]\n",
    "      print(\"attn_mask\", attn_mask.shape)\n",
    "\n",
    "    if attn_mask is not None and self.flatten:\n",
    "      t = h_shape[1]\n",
    "\n",
    "      if self.multimodality_induced:  # We have modes\n",
    "        n = h_shape[2]\n",
    "        attn_mask_with_modes = attn_mask[..., tf.newaxis, :, :]\n",
    "        tiled_mask = tf.tile(attn_mask_with_modes, [1, 1, 1, 1, t])\n",
    "        attn_mask = tf.reshape(\n",
    "            tiled_mask,\n",
    "            [b, 1, 1, tf.cast(t*n, tf.int32), tf.cast(t*n, tf.int32)]\n",
    "        )\n",
    "      else:\n",
    "        tiled_mask = tf.tile(attn_mask, [1, 1, t, 1])\n",
    "        attn_mask = tf.reshape(\n",
    "            tiled_mask,\n",
    "            [b, 1, tf.cast(t, tf.int32), tf.cast(t, tf.int32)]\n",
    "        )\n",
    "\n",
    "\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=hidden_vecs,\n",
    "        key=hidden_vecs,\n",
    "        value=hidden_vecs,\n",
    "        attention_mask=attn_mask,\n",
    "        return_attention_scores=True)\n",
    "    out = self.attn_dropout(attn_out, training=training)\n",
    "    attn_out = self.attn_ln(out + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_dropout(out, training=training)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "    if self.flatten:\n",
    "      out = tf.reshape(out, h_shape)\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnModeTransformerLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Performs full self-attention across the future modes dimensions.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_heads=8,\n",
    "               hidden_size=128,\n",
    "               drop_prob=0.1,\n",
    "               ln_eps=1e-6,\n",
    "               ff_dim=128):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(f'hidden_size ({hidden_size}) must be an integer '\n",
    "                       f'times bigger than num_heads ({num_heads}).')\n",
    "    self.attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=2)  # Attention over modes\n",
    "    self.attn_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "  def call(self, input_dict, training=None):\n",
    "\n",
    "    # [b, t, n, h]\n",
    "    hidden_vecs = input_dict[\"hidden_vecs\"]\n",
    "    print(\"hidden_vecs 3\", hidden_vecs.shape)\n",
    "\n",
    "    attn_out, attn_score = self.attn_layer(\n",
    "        query=hidden_vecs,\n",
    "        key=hidden_vecs,\n",
    "        value=hidden_vecs,\n",
    "        attention_mask=None,\n",
    "        return_attention_scores=True)\n",
    "    out = self.attn_dropout(attn_out, training=training)\n",
    "    attn_out = self.attn_ln(out + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.ff_layer1(attn_out)\n",
    "    out = self.ff_layer2(out)\n",
    "    out = self.ff_dropout(out, training=training)\n",
    "    out = self.ff_ln(out + attn_out)\n",
    "\n",
    "    return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalityInduction(tf.keras.layers.Layer):\n",
    "  \"\"\"Enables the model to forward and predict multi-mode predictions.\n",
    "\n",
    "  1) Features are broadcasted to number of modes and summed with learned mode\n",
    "      tensors.\n",
    "  2) Mixture Weights are generated by cross-attention over all dimensions\n",
    "      between learned mode tensors and hidden tensors.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               num_modes=5,\n",
    "               num_heads=8,\n",
    "               hidden_size=128,\n",
    "               drop_prob=0.1,\n",
    "               ln_eps=1e-6,\n",
    "               ff_dim=128):\n",
    "    super().__init__()\n",
    "    self.num_modes = num_modes\n",
    "    self.hidden_size = hidden_size\n",
    "    if hidden_size % num_heads != 0:\n",
    "      raise ValueError(f'hidden_size ({hidden_size}) must be an integer '\n",
    "                       f'times bigger than num_heads ({num_heads}).')\n",
    "    self.mm_attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=2)\n",
    "    self.mm_attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.mm_ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.mm_ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.mm_ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    self.mw_attn_layer = tf.keras.layers.MultiHeadAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=hidden_size // num_heads,\n",
    "        attention_axes=None)\n",
    "    self.mw_attn_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "    self.mw_ff_layer1 = tf.keras.layers.EinsumDense(\n",
    "        '...h,hf->...f', output_shape=ff_dim, bias_axes='f', activation='relu')\n",
    "    self.mw_ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=1,  # Single logit per mode\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "    self.mw_ff_ln = tf.keras.layers.LayerNormalization(epsilon=ln_eps)\n",
    "\n",
    "    self.attn_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    # [1, 1, ,m,h]\n",
    "    self.learned_add_mm = tf.Variable(\n",
    "        tf.random_uniform_initializer(\n",
    "            minval=-1.,\n",
    "            maxval=1.)(shape=[1, 1, self.num_modes, hidden_size]),\n",
    "        trainable=True,\n",
    "        dtype=tf.float32)\n",
    "\n",
    "  def call(self, input_dict, training=None):\n",
    "    hidden_vecs = input_dict[\"hidden_vecs\"]\n",
    "    # [b, t, 1, h]\n",
    "    hidden_vecs = hidden_vecs[..., tf.newaxis, :]\n",
    "\n",
    "    # Multi Modes\n",
    "    mm_add = self.mm_attn_ln(self.learned_add_mm + hidden_vecs)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out = self.mm_ff_layer1(mm_add)\n",
    "    out = self.mm_ff_layer2(out)\n",
    "    out = self.ff_dropout(out)\n",
    "    out = self.mm_ff_ln(out + hidden_vecs)\n",
    "\n",
    "    # Mixture Weights\n",
    "    # [b, 1, n, h]\n",
    "    b = tf.shape(out)[0]\n",
    "    attn_out_mw = self.mw_attn_layer(\n",
    "        query=tf.tile(self.learned_add_mm, [b, 1, 1, 1]),\n",
    "        key=mm_add,\n",
    "        value=mm_add,\n",
    "        return_attention_scores=False)\n",
    "    attn_out_mw = self.attn_dropout(attn_out_mw, training=training)\n",
    "\n",
    "    # [b, 1, n, h]\n",
    "    attn_out_mw = self.mw_attn_ln(attn_out_mw)\n",
    "\n",
    "    # Feed-forward layers.\n",
    "    out_mw = self.mw_ff_layer1(attn_out_mw)\n",
    "    out_mw = self.mw_ff_layer2(out_mw)\n",
    "    out_mw = self.ff_dropout(out_mw, training=training)\n",
    "\n",
    "    # [b, 1, n]\n",
    "    mixture_logits = out_mw[..., 0]\n",
    "    return out, mixture_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from 2D Predictions to 3D predictions\n",
    "class Prediction2DPositionHeadLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Converts transformer hidden vectors to model predictions.\"\"\"\n",
    "\n",
    "  def __init__(self, hidden_units=None, num_stages=5):\n",
    "    super().__init__()\n",
    "\n",
    "    self.dense_layers = []\n",
    "    # Add hidden layers.\n",
    "    if hidden_units is not None:\n",
    "      for units in hidden_units:\n",
    "        self.dense_layers.append(\n",
    "            tf.keras.layers.Dense(units, activation='relu'))\n",
    "    # Add the final prediction head.\n",
    "    self.dense_layers.append(\n",
    "        tf.keras.layers.EinsumDense(\n",
    "            '...h,hf->...f',\n",
    "            output_shape=9,\n",
    "            bias_axes='f',\n",
    "            activation=None))\n",
    "\n",
    "  def call(self, input_dict):\n",
    "    # [b, t, n,  h]\n",
    "    hidden_vecs = input_dict[\"hidden_vecs\"]\n",
    "\n",
    "    x = hidden_vecs\n",
    "    # [b, t, n, 5]\n",
    "    for layer in self.dense_layers:\n",
    "      x = layer(x)\n",
    "    pred = x\n",
    "    \"\"\"predictions = {\n",
    "        'agents/position': pred[..., 0:3],\n",
    "        'agents/position/raw_scale_tril': pred[..., 3:5],\n",
    "    }\"\"\"\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HST(tf.keras.Model):\n",
    "    def __init__(self, input_length):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = 128\n",
    "        self.preprocess_layer = PreprocessLayer_2() \n",
    "        self.agent_pose_encoder = AgentKeypointsEncoder(output_shape=hidden_size-8, embedding_size=hidden_size)\n",
    "        self.agent_pos_encoder = AgentPositionEncoder(output_shape=hidden_size-8, embedding_size=hidden_size)\n",
    "        self.agent_temp_encoder = AgentTemporalEncoder(output_shape=hidden_size-8, embedding_size=hidden_size, num_steps=input_length)\n",
    "        self.agent_encoder = FeatureConcatAgentEncoderLayer(input_length=input_length)\n",
    "        self.align_layer = AgentSelfAlignmentLayer()\n",
    "        self.tranformer1 = SelfAttnTransformerLayer(mask=True)\n",
    "        self.multimodality_induction = MultimodalityInduction()\n",
    "        self.tranformer3= SelfAttnTransformerLayer(mask=True, multimodality_induced=True)\n",
    "        self.transformer4 = SelfAttnModeTransformerLayer()\n",
    "        self.prediction_layer = Prediction2DPositionHeadLayer()\n",
    "        \n",
    "\n",
    "    def call(self, input_batch, training = False):\n",
    "        (input_1, input_2) = input_batch\n",
    "        output_dict = self.preprocess_layer((input_1, input_2)) # output shape (batch_size, 15, 3)\n",
    "  \n",
    "        encoded_keys, mask1 = self.agent_pose_encoder(output_dict)\n",
    "        encoded_pos, mask2 = self.agent_pos_encoder(output_dict)\n",
    "        encoded_temp, mask3 = self.agent_temp_encoder(output_dict)\n",
    "        hidden_vecs = self.agent_encoder(output_dict)\n",
    "        output_dict[\"hidden_vecs\"] = hidden_vecs\n",
    "        self_encoded_agent, _ = self.align_layer(output_dict)\n",
    "        output_dict[\"hidden_vecs\"] = self_encoded_agent\n",
    "        hidden_vecs, _ = self.tranformer1(output_dict)\n",
    "        hidden_vecs2, mixture_logits = self.multimodality_induction(output_dict)\n",
    "        output_dict[\"hidden_vecs\"] = hidden_vecs2\n",
    "        hidden_vecs3, _ = self.tranformer3(output_dict)\n",
    "        output_dict[\"hidden_vecs\"] = hidden_vecs3\n",
    "        hidden_vecs4, _ = self.transformer4(output_dict)\n",
    "        output_dict[\"hidden_vecs\"] = hidden_vecs4\n",
    "        pred = self.prediction_layer(output_dict)\n",
    "\n",
    "\n",
    "        output_dict={\n",
    "            \"encoded_keys\": encoded_keys,\n",
    "            \"encoded_pos\": encoded_pos,\n",
    "            \"encoded_temp\": encoded_temp,\n",
    "            \"encoded_keys_mask\": mask1,\n",
    "            \"encoded_pos_mask\": mask2,\n",
    "            \"encoded_temp_mas\": mask3,\n",
    "            \"output_dict\": output_dict,\n",
    "            \"hidden_vecs\": hidden_vecs,\n",
    "            \"hidden_vecs2\": hidden_vecs2,\n",
    "            \"hidden_vecs3\": hidden_vecs3,\n",
    "            \"mixture_logits\": mixture_logits,\n",
    "            'pred_pos': pred[...,0:3]\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 16222427923489919739\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_hidden (1, 15, 1)\n",
      "input_batch_new 0 (32, 15, 3)\n",
      "input_batch_new 1 (32, 15, 51)\n",
      "has_data (32, 15, 1)\n",
      "has_historic_data (32, 1, 1)\n",
      "should_predict (32, 15, 1)\n",
      "masked_input_pos (32, 15, 3)\n",
      "masked_input_pose (32, 15, 51)\n",
      "(32, 15, 1, 120)\n",
      "(32, 15, 120)\n",
      "(32, 15, 1, 120)\n",
      "(32, 15, 120)\n",
      "(32, 15, 1, 120)\n",
      "(32, 15, 120)\n",
      "concat embedding (32, 15, 360)\n",
      "hidden_vecs (32, 15, 128)\n",
      "has_historic_data (32, 1)\n",
      "attn_mask (32, 1, 1, 1)\n",
      "hidden_vecs (32, 15, 5, 128)\n",
      "has_historic_data (32, 1)\n",
      "attn_mask (32, 1, 1, 1)\n",
      "hidden_vecs 3 (32, 15, 5, 128)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy('gpu')\n",
    "with strategy.scope():\n",
    "    model = HST(15)\n",
    "    model.compile(loss='msle', optimizer='rmsprop')\n",
    "for (batch_x1, batch_x2) in train_dataset.take(1):\n",
    "    input_batch = (batch_x1, batch_x2)\n",
    "    output = model(input_batch, training=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 15, 5, 3), dtype=float32, numpy=\n",
       "array([[[[-0.58479565,  1.1494462 ,  1.1985414 ],\n",
       "         [-0.26594967,  0.9777623 ,  1.1316311 ],\n",
       "         [-0.48050267,  1.0369124 ,  1.1699543 ],\n",
       "         [-0.5627724 ,  1.1177565 ,  1.1594312 ],\n",
       "         [-0.41522986,  0.9024835 ,  1.1322446 ]],\n",
       "\n",
       "        [[-0.91070414,  0.9054512 ,  1.0719429 ],\n",
       "         [-0.5814908 ,  0.73537123,  1.012743  ],\n",
       "         [-0.80036604,  0.7546586 ,  1.0327294 ],\n",
       "         [-0.87831354,  0.85735786,  1.0260043 ],\n",
       "         [-0.71396804,  0.64900315,  1.0046213 ]],\n",
       "\n",
       "        [[-0.7963791 ,  0.45825484,  0.82039595],\n",
       "         [-0.48476166,  0.29880816,  0.7310183 ],\n",
       "         [-0.6578388 ,  0.36424407,  0.80825573],\n",
       "         [-0.76236796,  0.39674795,  0.7418035 ],\n",
       "         [-0.6185138 ,  0.19237648,  0.71325004]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.01664364,  1.5041437 , -1.7927768 ],\n",
       "         [ 0.4392018 , -0.29263675, -1.3876725 ],\n",
       "         [ 0.7232453 ,  0.26380447, -2.2020106 ],\n",
       "         [ 1.5644839 , -0.3590243 , -2.7106385 ],\n",
       "         [ 0.51660955, -1.7744112 , -2.5296578 ]],\n",
       "\n",
       "        [[-0.19891822,  1.3226244 , -1.6745906 ],\n",
       "         [ 0.8823297 , -0.4941681 , -1.6274657 ],\n",
       "         [ 0.39247724, -0.02394936, -1.9684606 ],\n",
       "         [ 1.7129824 , -0.42197132, -2.3847923 ],\n",
       "         [ 0.46162942, -2.0443287 , -2.26361   ]],\n",
       "\n",
       "        [[-0.3401829 ,  1.2038189 , -1.7100854 ],\n",
       "         [ 0.7525505 , -0.7751383 , -1.5337212 ],\n",
       "         [ 0.20942794, -0.5203929 , -1.7027416 ],\n",
       "         [ 1.6869407 , -0.6138778 , -2.2021022 ],\n",
       "         [ 0.2494249 , -2.0141473 , -1.8079368 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.84485793,  1.9251361 ,  2.850384  ],\n",
       "         [ 1.1583401 ,  1.6971852 ,  2.8831825 ],\n",
       "         [ 0.98523986,  1.7182493 ,  2.8190315 ],\n",
       "         [ 0.93881476,  1.8647254 ,  2.87697   ],\n",
       "         [ 1.0448003 ,  1.6966641 ,  2.8836153 ]],\n",
       "\n",
       "        [[ 0.8431165 ,  1.9280038 ,  2.850159  ],\n",
       "         [ 1.1564609 ,  1.7017404 ,  2.8827033 ],\n",
       "         [ 0.9834634 ,  1.7202787 ,  2.8185198 ],\n",
       "         [ 0.9374001 ,  1.8685496 ,  2.8754272 ],\n",
       "         [ 1.0429633 ,  1.700062  ,  2.8825521 ]],\n",
       "\n",
       "        [[ 0.83864725,  1.9300771 ,  2.8487134 ],\n",
       "         [ 1.1516991 ,  1.7041167 ,  2.8813214 ],\n",
       "         [ 0.97897506,  1.7212012 ,  2.816629  ],\n",
       "         [ 0.9322635 ,  1.8702812 ,  2.8734493 ],\n",
       "         [ 1.0383998 ,  1.7017884 ,  2.8806489 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.00977075,  1.4813207 , -1.8332458 ],\n",
       "         [ 0.44368565, -0.38471982, -1.4303609 ],\n",
       "         [ 0.69962883,  0.19976236, -2.2769327 ],\n",
       "         [ 1.567935  , -0.3933421 , -2.7611663 ],\n",
       "         [ 0.4441277 , -1.8668443 , -2.606463  ]],\n",
       "\n",
       "        [[-0.21345925,  1.3180442 , -1.7325325 ],\n",
       "         [ 0.88929164, -0.6110558 , -1.6725053 ],\n",
       "         [ 0.39084488, -0.11108609, -2.032148  ],\n",
       "         [ 1.6989698 , -0.47588992, -2.4060082 ],\n",
       "         [ 0.39650026, -2.1589193 , -2.336823  ]],\n",
       "\n",
       "        [[-0.3290763 ,  1.171446  , -1.7570214 ],\n",
       "         [ 0.78236353, -0.8614528 , -1.589534  ],\n",
       "         [ 0.20528458, -0.60522366, -1.7412924 ],\n",
       "         [ 1.6837978 , -0.658127  , -2.21554   ],\n",
       "         [ 0.19334403, -2.1102011 , -1.8611689 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.685742  ,  1.9183497 ,  2.6683197 ],\n",
       "         [ 1.0100118 ,  1.7409489 ,  2.6881824 ],\n",
       "         [ 0.8338706 ,  1.7642033 ,  2.6203036 ],\n",
       "         [ 0.7725968 ,  1.8920093 ,  2.684715  ],\n",
       "         [ 0.8817967 ,  1.7350272 ,  2.6857839 ]],\n",
       "\n",
       "        [[ 0.6930821 ,  1.9130137 ,  2.6661558 ],\n",
       "         [ 1.0169673 ,  1.7355769 ,  2.6854837 ],\n",
       "         [ 0.84124327,  1.7588041 ,  2.6178532 ],\n",
       "         [ 0.78085184,  1.8874614 ,  2.6818695 ],\n",
       "         [ 0.88828963,  1.7293849 ,  2.6833868 ]],\n",
       "\n",
       "        [[ 0.69742954,  1.9075209 ,  2.6640646 ],\n",
       "         [ 1.0214615 ,  1.729603  ,  2.6826913 ],\n",
       "         [ 0.84600425,  1.752491  ,  2.6149912 ],\n",
       "         [ 0.78530806,  1.8816087 ,  2.6791782 ],\n",
       "         [ 0.89268255,  1.7237952 ,  2.6811929 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.01238525,  1.477719  , -1.8346639 ],\n",
       "         [ 0.44696552, -0.38291138, -1.4261998 ],\n",
       "         [ 0.71674955,  0.19549155, -2.2663283 ],\n",
       "         [ 1.5645568 , -0.3972597 , -2.7589238 ],\n",
       "         [ 0.44812712, -1.871007  , -2.5954342 ]],\n",
       "\n",
       "        [[-0.21621144,  1.312336  , -1.7280806 ],\n",
       "         [ 0.8976265 , -0.6167848 , -1.6725208 ],\n",
       "         [ 0.40843076, -0.11293241, -2.0211017 ],\n",
       "         [ 1.7012981 , -0.47614154, -2.3995042 ],\n",
       "         [ 0.40250158, -2.1612885 , -2.3269672 ]],\n",
       "\n",
       "        [[-0.33062804,  1.1677808 , -1.753657  ],\n",
       "         [ 0.7869994 , -0.86645824, -1.58852   ],\n",
       "         [ 0.22252202, -0.60947484, -1.7343634 ],\n",
       "         [ 1.6848754 , -0.65784657, -2.2089562 ],\n",
       "         [ 0.2010979 , -2.1140873 , -1.8512701 ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.12772113,  0.55322534,  0.88942814],\n",
       "         [ 0.41253674,  0.3856694 ,  0.7808132 ],\n",
       "         [ 0.25242722,  0.44730487,  0.8592252 ],\n",
       "         [ 0.21145302,  0.49455467,  0.80123585],\n",
       "         [ 0.3281821 ,  0.31731516,  0.78554875]],\n",
       "\n",
       "        [[ 0.28028917,  0.79762876,  0.88958627],\n",
       "         [ 0.54693526,  0.6346078 ,  0.7841386 ],\n",
       "         [ 0.37261134,  0.70283127,  0.8208535 ],\n",
       "         [ 0.35249287,  0.72594666,  0.827382  ],\n",
       "         [ 0.46037543,  0.580345  ,  0.79092497]],\n",
       "\n",
       "        [[ 0.3670631 ,  0.9493862 ,  0.9933624 ],\n",
       "         [ 0.65857553,  0.7641181 ,  0.8893068 ],\n",
       "         [ 0.42094338,  0.852918  ,  0.93197405],\n",
       "         [ 0.43995994,  0.87616813,  0.93465614],\n",
       "         [ 0.5383643 ,  0.71832025,  0.9099177 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.00606954,  1.5016246 , -1.81773   ],\n",
       "         [ 0.43641105, -0.30647445, -1.41344   ],\n",
       "         [ 0.72601056,  0.26446712, -2.2187214 ],\n",
       "         [ 1.5766354 , -0.36401343, -2.7494926 ],\n",
       "         [ 0.5001921 , -1.788877  , -2.5630062 ]],\n",
       "\n",
       "        [[-0.20882928,  1.3182725 , -1.7013423 ],\n",
       "         [ 0.8798568 , -0.5167626 , -1.6537199 ],\n",
       "         [ 0.3962003 , -0.03504224, -1.9913546 ],\n",
       "         [ 1.715267  , -0.42805952, -2.4068968 ],\n",
       "         [ 0.4391686 , -2.064177  , -2.3021173 ]],\n",
       "\n",
       "        [[-0.351956  ,  1.1999842 , -1.7342442 ],\n",
       "         [ 0.7551639 , -0.7857398 , -1.566995  ],\n",
       "         [ 0.21165536, -0.5316104 , -1.7192765 ],\n",
       "         [ 1.6933701 , -0.6160596 , -2.2259097 ],\n",
       "         [ 0.22948754, -2.027754  , -1.8368521 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.05852491, -0.22697365,  0.99736893],\n",
       "         [ 0.3385002 , -0.40306082,  0.90228975],\n",
       "         [ 0.25566977, -0.32453293,  0.9521661 ],\n",
       "         [ 0.16964434, -0.24451447,  0.9488995 ],\n",
       "         [ 0.2848295 , -0.4591699 ,  0.85168743]],\n",
       "\n",
       "        [[ 0.11625257,  0.09954347,  1.1400702 ],\n",
       "         [ 0.4034996 , -0.11965699,  1.0581701 ],\n",
       "         [ 0.30482727, -0.01798773,  1.119792  ],\n",
       "         [ 0.22372764,  0.07597561,  1.1221347 ],\n",
       "         [ 0.34820774, -0.1501764 ,  1.026238  ]],\n",
       "\n",
       "        [[ 0.1838811 ,  0.43787295,  1.3327031 ],\n",
       "         [ 0.46847394,  0.25744116,  1.255878  ],\n",
       "         [ 0.37077117,  0.3400688 ,  1.3087517 ],\n",
       "         [ 0.3061848 ,  0.44625518,  1.302845  ],\n",
       "         [ 0.42335644,  0.22990882,  1.1922333 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.03771853,  1.481072  , -1.8005416 ],\n",
       "         [ 0.4250639 , -0.3302234 , -1.3973384 ],\n",
       "         [ 0.6889853 ,  0.24107245, -2.2051117 ],\n",
       "         [ 1.5334891 , -0.39573264, -2.7260973 ],\n",
       "         [ 0.48096123, -1.8048608 , -2.551983  ]],\n",
       "\n",
       "        [[-0.22126353,  1.3054973 , -1.682883  ],\n",
       "         [ 0.87659085, -0.5534307 , -1.6333168 ],\n",
       "         [ 0.3642407 , -0.04291274, -1.9731076 ],\n",
       "         [ 1.6785641 , -0.4595254 , -2.3950992 ],\n",
       "         [ 0.41818953, -2.0782254 , -2.2856452 ]],\n",
       "\n",
       "        [[-0.36846232,  1.1800746 , -1.717767  ],\n",
       "         [ 0.7409809 , -0.8176843 , -1.5438383 ],\n",
       "         [ 0.18092273, -0.5383086 , -1.7102214 ],\n",
       "         [ 1.658457  , -0.64636207, -2.20824   ],\n",
       "         [ 0.22095238, -2.0474715 , -1.8159616 ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.86769754,  1.9156232 ,  2.855522  ],\n",
       "         [ 1.1804248 ,  1.6856169 ,  2.8871455 ],\n",
       "         [ 1.0077081 ,  1.7088482 ,  2.8240366 ],\n",
       "         [ 0.9615664 ,  1.8535614 ,  2.8813083 ],\n",
       "         [ 1.0678507 ,  1.684741  ,  2.8884401 ]],\n",
       "\n",
       "        [[ 0.8660693 ,  1.9182754 ,  2.855309  ],\n",
       "         [ 1.1784002 ,  1.6891346 ,  2.885719  ],\n",
       "         [ 1.0059934 ,  1.7107561 ,  2.8236616 ],\n",
       "         [ 0.9602479 ,  1.8572037 ,  2.8798153 ],\n",
       "         [ 1.0660918 ,  1.6879144 ,  2.8873947 ]],\n",
       "\n",
       "        [[ 0.8617372 ,  1.9202564 ,  2.853654  ],\n",
       "         [ 1.1736761 ,  1.6915879 ,  2.8842638 ],\n",
       "         [ 1.001559  ,  1.7117697 ,  2.821646  ],\n",
       "         [ 0.955165  ,  1.8590145 ,  2.8777058 ],\n",
       "         [ 1.0615685 ,  1.6897163 ,  2.8854158 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.01038146,  1.4809775 , -1.8334789 ],\n",
       "         [ 0.44316348, -0.3847708 , -1.4303882 ],\n",
       "         [ 0.6991923 ,  0.19962583, -2.277019  ],\n",
       "         [ 1.5672992 , -0.39353585, -2.761629  ],\n",
       "         [ 0.4432212 , -1.8669028 , -2.607029  ]],\n",
       "\n",
       "        [[-0.2142607 ,  1.3175796 , -1.7328622 ],\n",
       "         [ 0.88890654, -0.6113678 , -1.6727198 ],\n",
       "         [ 0.390503  , -0.1113667 , -2.0323768 ],\n",
       "         [ 1.6984288 , -0.4759702 , -2.40658   ],\n",
       "         [ 0.39548892, -2.158964  , -2.33751   ]],\n",
       "\n",
       "        [[-0.32999134,  1.1709746 , -1.7572641 ],\n",
       "         [ 0.7820634 , -0.86209404, -1.5898461 ],\n",
       "         [ 0.2047392 , -0.60532975, -1.7414309 ],\n",
       "         [ 1.6829337 , -0.65830266, -2.2159479 ],\n",
       "         [ 0.1923519 , -2.1104717 , -1.8618375 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"pred_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.81089574,  0.79600906, -0.70223266,  0.28319982, -1.0845588 ,\n",
       "       -1.1686233 , -0.36819622,  0.2499477 , -0.6736832 ,  1.7577071 ,\n",
       "        0.24864085, -0.01561074, -0.9247952 ,  0.5717657 , -0.05981993,\n",
       "       -0.58544475, -0.6305727 ,  0.23966281, -0.04127312, -0.07515548,\n",
       "       -0.7684814 ,  0.46016762, -1.2552994 , -0.11084849,  0.38545057,\n",
       "       -0.35010007, -0.65156966,  0.43060076, -1.8044866 ,  0.42750764,\n",
       "        1.9555049 , -0.39100063, -0.37983632,  0.31078747,  0.23831747,\n",
       "       -0.8058383 , -0.569484  , -0.13494697,  0.38479537,  0.406515  ,\n",
       "        0.21507372,  1.5170391 , -0.45104837,  0.02057071,  0.25320405,\n",
       "        0.03062555,  0.24210678, -1.5947713 ,  0.8438032 , -0.6528839 ,\n",
       "        0.36894557, -0.13338293, -0.70126474,  0.5603812 , -0.19411625,\n",
       "        1.1276221 , -0.58010834,  0.73732305, -0.47832206,  0.19454718,\n",
       "        1.0612475 , -0.13222125, -0.3588956 , -0.8896779 , -0.35958642,\n",
       "        0.27276537,  0.47625348,  1.1778684 , -0.57056594,  0.2722594 ,\n",
       "       -1.5029417 , -2.1515765 ,  1.7467065 ,  0.64532816, -0.40548697,\n",
       "       -0.30252832, -0.7610415 ,  1.2134541 ,  0.03303681, -3.0151408 ,\n",
       "        0.02652086,  1.322162  ,  0.17779079, -0.51181567, -2.5536106 ,\n",
       "       -1.272782  , -0.06361625, -0.5113702 ,  1.4569368 ,  0.89028114,\n",
       "        0.4722177 , -1.0022151 , -0.06729916, -0.8909719 ,  0.4823557 ,\n",
       "       -0.5183393 ,  1.3706635 , -1.3241087 , -0.5208028 ,  0.582793  ,\n",
       "        0.8133391 ,  0.4767809 ,  0.9184714 ,  0.09100311, -0.48305762,\n",
       "        0.4907189 , -0.52529037,  1.1521325 ,  0.76850736, -1.1143308 ,\n",
       "       -0.7054026 ,  0.8884932 ,  0.36993808,  0.7795363 , -0.17171142,\n",
       "       -0.19055071, -0.19004309, -0.39337373,  0.75889283, -0.8409766 ,\n",
       "        0.7285873 ,  0.36872888,  0.45060405, -0.25678885,  0.45260793,\n",
       "        0.42619264, -0.3449939 , -0.6107882 ], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"encoded_agent\"][0][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
