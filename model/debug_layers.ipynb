{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "import tensorflow_models as tfm\n",
    "import tensorflow_probability as tfp\n",
    "from preprocess_data import load_data\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_16790/3231426434.py:1: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 18117979812700169767\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.experimental.load(\n",
    "    \"/home/pbr-student/personal/thesis/test/PedestrianTrajectoryPrediction/model/train_dataset\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "    \"\"\" Applies the masking to the sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "    def calc_hidden_mask(self, batch_size=32, sequence_length=15):\n",
    "        # create mask array, False = needs to be predicted\n",
    "        mask_arrays = []\n",
    "        #print(\"batch_size:\", batch_size)\n",
    "        #print(\"sequence_length:\", sequence_length)\n",
    "        for i in range(batch_size):\n",
    "          mask_arr = [True] * 6 + [False] * (sequence_length-6)\n",
    "          # hide 0-2 in between steps (for lazyness whole datapoint)\n",
    "          hidden_nr = np.random.randint(3)\n",
    "          hidden_idx = np.random.choice(range(6),hidden_nr, replace=False)\n",
    "          for v in hidden_idx:\n",
    "              mask_arr[v] = False\n",
    "          mask_arrays.append(mask_arr)\n",
    "        #print(\"mask aaray:\", np.asarray(mask_arrays).shape)\n",
    "        return np.asarray(mask_arrays)\n",
    "\n",
    "    def call(self,\n",
    "           raw_input_batch: Tuple[tf.Tensor, tf.Tensor],\n",
    "           is_hidden: Optional[tf.Tensor] = None) -> Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "        input_batch = raw_input_batch\n",
    "  \n",
    "\n",
    "        batch_size = tf.shape(input_batch[0])[0]\n",
    "        sequence_length = tf.shape(input_batch[0])[1]\n",
    "        feature_size1 = tf.shape(input_batch[0])[2]\n",
    "        feature_size2 = tf.shape(input_batch[1])[2]\n",
    "\n",
    "        mask = self.calc_hidden_mask() #tf.convert_to_tensor\n",
    "\n",
    "        mask_tensor = tf.constant(mask, dtype=tf.bool)\n",
    "\n",
    "        # Expand dimensions of mask to match the input tensor\n",
    "        #expanded_mask = tf.expand_dims(mask_tensor, axis=0)  # Add batch dimension\n",
    "        expanded_mask = tf.expand_dims(mask_tensor, axis=-1)  # Add feature dimension\n",
    "\n",
    "        # Broadcast mask to match input tensor shape\n",
    "        broadcasted_mask_pos = tf.broadcast_to(expanded_mask, (batch_size, sequence_length, feature_size1))\n",
    "        broadcasted_mask_pose = tf.broadcast_to(expanded_mask, (batch_size, sequence_length, feature_size2))\n",
    "\n",
    "        #batch_mask = tf.broadcast_to(expanded_mask, (batch_size, sequence_length))\n",
    "\n",
    "        # Apply mask\n",
    "        masked_input_pos = tf.where(broadcasted_mask_pos, input_batch[0], tf.zeros_like(input_batch[0]))\n",
    "        masked_input_pose = tf.where(broadcasted_mask_pose, input_batch[1], tf.zeros_like(input_batch[1]))\n",
    "        targets = tf.where(tf.math.logical_not(broadcasted_mask_pos), input_batch[0], tf.zeros_like(input_batch[0]))\n",
    "\n",
    "      # scale\n",
    "        scale_factor = 100.0\n",
    "        masked_input_pos = tf.math.scalar_mul(scale_factor, masked_input_pos)\n",
    "        masked_input_pose = tf.math.scalar_mul(scale_factor, masked_input_pose) \n",
    "\n",
    "        return (masked_input_pos, masked_input_pose), mask, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Sinusoidal Embedding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/embedding.py    \"\"\"\n",
    "class SinusoidalEmbeddingLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Sinusoidal Postional Embedding for xyz and time.\"\"\"\n",
    "\n",
    "  def __init__(self, min_freq=4, max_freq=256, hidden_size=256):\n",
    "    super().__init__()\n",
    "    self.min_freq = float(min_freq)\n",
    "    self.max_freq = float(max_freq)\n",
    "    self.hidden_size = hidden_size\n",
    "    if hidden_size % 2 != 0:\n",
    "      raise ValueError('hidden_size ({hidden_size}) must be divisible by 2.')\n",
    "    self.num_freqs_int32 = hidden_size // 2\n",
    "    self.num_freqs = tf.cast(self.num_freqs_int32, dtype=tf.float32)\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    log_freq_increment = (\n",
    "        tf.math.log(float(self.max_freq) / float(self.min_freq)) /\n",
    "        tf.maximum(1.0, self.num_freqs - 1))\n",
    "    # [num_freqs]\n",
    "    self.inv_freqs = self.min_freq * tf.exp(\n",
    "        tf.range(self.num_freqs, dtype=tf.float32) * -log_freq_increment)\n",
    "\n",
    "  def call(self, input_tensor):\n",
    "    \n",
    "    # [batch_size, sequence_length, feature_size, num_freqs]\n",
    "    input_tensor = tf.expand_dims(input_tensor, axis=-1)\n",
    "    input_tensor = tf.repeat(input_tensor, self.num_freqs_int32, axis=-1)\n",
    "\n",
    "    # [batch_size, sequence_length, feature_size, hidden_size]\n",
    "    embedded = tf.concat([\n",
    "        tf.sin(input_tensor * self.inv_freqs),\n",
    "        tf.cos(input_tensor * self.inv_freqs)\n",
    "    ], axis=-1)\n",
    "    return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Adapted Agent Position Encoding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/agent_feature_encoder.py    \"\"\"\n",
    "class AgentPositionEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes agents spatial positions.\"\"\"\n",
    "\n",
    "  def __init__(self, output_shape, embedding_size):\n",
    "    \n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding_layer = SinusoidalEmbeddingLayer(\n",
    "        hidden_size=embedding_size) # output_shape (batch_sie, sequence_length, feature size, hidden_size)\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization(axis=-1)  \n",
    "    self.mlp = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        \n",
    "        activation=None)\n",
    "\n",
    "  def call(self, input_batch):\n",
    "    normalized_input = input_batch[0] #self.layer_norm(input_batch[0])\n",
    "    embedded_input = self.embedding_layer(normalized_input)\n",
    "    return self.mlp(embedded_input)\n",
    "  \n",
    "\n",
    "\n",
    "class AgentTemporalEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes agents temporal positions.\"\"\"\n",
    "\n",
    "  def __init__(self,output_shape, embedding_size, num_steps):\n",
    "    super().__init__()\n",
    "    self.embedding_layer = SinusoidalEmbeddingLayer(\n",
    "        max_freq=num_steps,\n",
    "        hidden_size=embedding_size)\n",
    "\n",
    "    self.mlp = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        activation=None)\n",
    "\n",
    "  def _get_temporal_embedding(self, input_batch):\n",
    "    # This weird thing is for exporting and loading keras model...\n",
    "    b = tf.shape(input_batch[0])[0]\n",
    "    num_steps = tf.shape(input_batch[0])[1]\n",
    "\n",
    "    t = tf.range(0, num_steps, dtype=tf.float32)\n",
    "    t = t[tf.newaxis, :]\n",
    "    t = tf.tile(t, [b, 1])\n",
    "    return self.embedding_layer(t[..., tf.newaxis])\n",
    "\n",
    "  def call(self, input_batch):\n",
    "    return self.mlp(self._get_temporal_embedding(input_batch))\n",
    "  \n",
    "\n",
    "  \"\"\" Adapted Agent Keypoint Encoding Layer from source: https://github.com/google-research/human-scene-transformer/blob/main/human_scene_transformer/model/agent_feature_encoder.py    \"\"\"\n",
    "class AgentKeypointsEncoder(tf.keras.layers.Layer):\n",
    "  \"\"\"Encodes the agent's keypoints.\"\"\"\n",
    "\n",
    "  def __init__(self, output_shape, embedding_size):\n",
    "    super().__init__()\n",
    "\n",
    "    self.mlp1 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=output_shape,\n",
    "        bias_axes='h',\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "  def call(self, input_batch, training=None):\n",
    "\n",
    "    keypoints = input_batch[1]\n",
    "\n",
    "    out = self.mlp1(keypoints)[..., tf.newaxis, :]\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureConcatAgentEncoderLayer(tf.keras.layers.Layer):\n",
    "  \"\"\"Independently encodes features and attends to them.\n",
    "\n",
    "  Agent features are cross-attended with a learned query or hidden_vecs instead\n",
    "  of MLP.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, input_length, batch_size=32, hidden_size=128, num_heads=4, ln_eps=1e-6, transformer_ff_dim=128, drop_prob=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    # Cross Attention and learned query.\n",
    "    self.ff_layer2 = tf.keras.layers.EinsumDense(\n",
    "        '...f,fh->...h',\n",
    "        output_shape=hidden_size,\n",
    "        bias_axes='h',\n",
    "        activation=None,\n",
    "    )\n",
    "    self.ff_dropout = tf.keras.layers.Dropout(drop_prob)\n",
    "\n",
    "    self.agent_feature_embedding_layers = []\n",
    "    # Position Feature [batch, sequence_len, feature_size, hidden_size]\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentPositionEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "    # Feature Embedding - keypoints [batch, sequence_len, hidden_size]\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentKeypointsEncoder(output_shape=hidden_size-8, embedding_size=hidden_size))\n",
    "\n",
    "    # Temporal Embedding [batch, sequence_len, 1, hidden_size]\n",
    "    self.agent_feature_embedding_layers.append(\n",
    "        AgentTemporalEncoder(output_shape=hidden_size-8, embedding_size=hidden_size, num_steps=input_length))\n",
    "\n",
    "\n",
    "  def call(self, input_batch: Tuple[Tuple[tf.Tensor, tf.Tensor], tf.Tensor],\n",
    "           training: Optional[bool] = None):\n",
    "    mask = input_batch[1]\n",
    "    input_batch = input_batch[0]\n",
    "    layer_embeddings = []\n",
    "    for layer in self.agent_feature_embedding_layers:\n",
    "      layer_embedding = layer(input_batch, training=training)\n",
    "      layer_embedding = tf.reshape(\n",
    "          layer_embedding,\n",
    "          layer_embedding.shape[:-2]\n",
    "          + [layer_embedding.shape[-2] * layer_embedding.shape[-1]],\n",
    "      )\n",
    "      layer_embeddings.append(layer_embedding)\n",
    "    embedding = tf.concat(layer_embeddings, axis=-1)\n",
    "\n",
    "    out = self.ff_layer2(embedding)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HST(tf.keras.Model):\n",
    "    def __init__(self, input_length):\n",
    "        super().__init__()\n",
    "\n",
    "        hidden_size = 128\n",
    "        self.preprocess_layer = PreprocessLayer() \n",
    "        self.agent_pose_encoder = AgentKeypointsEncoder(output_shape=hidden_size-8, embedding_size=hidden_size)\n",
    "        self.agent_pos_encoder = AgentPositionEncoder(output_shape=hidden_size-8, embedding_size=hidden_size)\n",
    "        self.agent_temp_encoder = AgentTemporalEncoder(output_shape=hidden_size-8, embedding_size=hidden_size, num_steps=input_length)\n",
    "        self.agent_encoder = FeatureConcatAgentEncoderLayer(input_length=input_length)\n",
    "\n",
    "    def call(self, input_batch, training = False):\n",
    "        (input_1, input_2) = input_batch\n",
    "        masked_inputs, mask, targets = self.preprocess_layer((input_1, input_2)) # output shape (batch_size, 15, 3)\n",
    "  \n",
    "        encoded_keys = self.agent_pose_encoder(masked_inputs)\n",
    "        encoded_pos = self.agent_pos_encoder(masked_inputs)\n",
    "        encoded_temp = self.agent_temp_encoder(masked_inputs)\n",
    "        encoded_agent = self.agent_encoder((masked_inputs, mask))\n",
    "\n",
    "        output_dict={\n",
    "            \"masked_inputs\": masked_inputs,\n",
    "            \"encoded_keys\": encoded_keys,\n",
    "            \"encoded_pos\": encoded_pos,\n",
    "            \"encoded_temp\": encoded_temp,\n",
    "            \"encoded_agent\": encoded_agent,\n",
    "            \"mask\": mask, \n",
    "            \"targets\": targets\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = tf.distribute.OneDeviceStrategy('cpu')\n",
    "with strategy.scope():\n",
    "    model = HST(15)\n",
    "    model.compile(loss='msle', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.AgentPositionEncoder object at 0x7f739636af40>\n",
      "(32, 15, 360)\n",
      "<__main__.AgentKeypointsEncoder object at 0x7f739630a0a0>\n",
      "(32, 15, 120)\n",
      "<__main__.AgentTemporalEncoder object at 0x7f739630a790>\n",
      "(32, 15, 120)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:1: Invalid control characters encountered in text.\n",
      "[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/text_format.cc:337] Error parsing text-format tensorflow.data.experimental.DistributedSnapshotMetadata: 1:3: Expected identifier, got: 18117979812700169767\n"
     ]
    }
   ],
   "source": [
    "for (batch_x1, batch_x2) in train_dataset.take(1):\n",
    "    input_batch = (batch_x1, batch_x2)\n",
    "    output = model(input_batch, training=False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ -3598.3284  ,   3669.6824  ,  -1108.4233  ,   6216.9717  ,\n",
       "         4478.1216  ,   6694.082   ,  11930.198   ,   1407.5459  ,\n",
       "        11565.539   ,  -2656.3096  ,   5338.704   ,   -723.87555 ,\n",
       "         1385.4274  ,    -70.11976 ,   3937.3447  ,   3330.6067  ,\n",
       "         1776.8395  ,   3642.5796  ,  -6280.229   ,    557.0555  ,\n",
       "         1392.6835  ,    792.41864 ,   3056.999   ,   5369.44    ,\n",
       "         3407.309   ,   3031.4333  ,  -4531.4478  ,  -1885.7577  ,\n",
       "        -4108.478   ,  -2197.484   ,  -5746.8477  ,   1387.911   ,\n",
       "         4124.1777  ,   2118.2458  ,  -5028.1284  ,  -7050.427   ,\n",
       "         7873.676   ,   1253.348   ,   1032.2197  ,   6468.8403  ,\n",
       "        -2805.4321  ,  11784.496   ,   2987.487   ,   8209.725   ,\n",
       "         -977.20355 ,  -2739.6658  ,  -3253.9353  ,   5345.1304  ,\n",
       "          229.07828 ,   9424.035   ,  -2878.368   ,    986.3005  ,\n",
       "        -2161.611   ,   9170.379   ,  -3237.4458  ,    410.93564 ,\n",
       "        -7998.2065  ,   7349.599   ,  -2613.403   ,  11124.048   ,\n",
       "        -4334.8145  ,   8617.708   ,  -4707.7188  ,  -6402.831   ,\n",
       "        -8272.055   ,  10792.811   ,   2091.2637  ,   8900.407   ,\n",
       "        -6465.282   ,  -3975.503   ,   -238.10768 ,  -6749.326   ,\n",
       "           53.368954,   8271.055   ,  -9720.682   ,   1444.8303  ,\n",
       "          510.968   ,  -6137.615   ,   6294.916   ,  -1354.7311  ,\n",
       "          848.1733  ,  -6873.5117  ,    718.5112  ,   2417.7573  ,\n",
       "         4180.448   ,  -2189.5737  ,  -3686.1055  ,   2056.0925  ,\n",
       "         9058.014   ,  -2943.061   ,   2828.9636  ,  22418.615   ,\n",
       "        -4611.026   ,   -647.62933 ,  -4799.255   ,    492.9757  ,\n",
       "         2675.2056  ,    521.79926 ,   6236.574   ,   7936.3735  ,\n",
       "         4612.3145  ,   7905.8926  ,  -4719.1147  ,  -2908.2646  ,\n",
       "         1712.7505  ,  -4162.282   ,   7249.6167  ,  -7075.8013  ,\n",
       "         2034.7542  ,    948.52954 ,   -627.5331  ,  11215.766   ,\n",
       "         2375.6106  ,  -1341.5507  ,   -437.53854 ,   6269.7183  ,\n",
       "        -4026.4587  ,   -930.3188  ,   9085.29    ,   2001.3569  ,\n",
       "        -2071.1514  ,   8145.3193  ,  13329.749   ,   -753.17084 ,\n",
       "       -13438.055   ,    288.99448 ,   3493.0571  ,  -2724.8433  ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"encoded_agent\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       "array([ 0.81089574,  0.79600906, -0.70223266,  0.28319982, -1.0845588 ,\n",
       "       -1.1686233 , -0.36819622,  0.2499477 , -0.6736832 ,  1.7577071 ,\n",
       "        0.24864085, -0.01561074, -0.9247952 ,  0.5717657 , -0.05981993,\n",
       "       -0.58544475, -0.6305727 ,  0.23966281, -0.04127312, -0.07515548,\n",
       "       -0.7684814 ,  0.46016762, -1.2552994 , -0.11084849,  0.38545057,\n",
       "       -0.35010007, -0.65156966,  0.43060076, -1.8044866 ,  0.42750764,\n",
       "        1.9555049 , -0.39100063, -0.37983632,  0.31078747,  0.23831747,\n",
       "       -0.8058383 , -0.569484  , -0.13494697,  0.38479537,  0.406515  ,\n",
       "        0.21507372,  1.5170391 , -0.45104837,  0.02057071,  0.25320405,\n",
       "        0.03062555,  0.24210678, -1.5947713 ,  0.8438032 , -0.6528839 ,\n",
       "        0.36894557, -0.13338293, -0.70126474,  0.5603812 , -0.19411625,\n",
       "        1.1276221 , -0.58010834,  0.73732305, -0.47832206,  0.19454718,\n",
       "        1.0612475 , -0.13222125, -0.3588956 , -0.8896779 , -0.35958642,\n",
       "        0.27276537,  0.47625348,  1.1778684 , -0.57056594,  0.2722594 ,\n",
       "       -1.5029417 , -2.1515765 ,  1.7467065 ,  0.64532816, -0.40548697,\n",
       "       -0.30252832, -0.7610415 ,  1.2134541 ,  0.03303681, -3.0151408 ,\n",
       "        0.02652086,  1.322162  ,  0.17779079, -0.51181567, -2.5536106 ,\n",
       "       -1.272782  , -0.06361625, -0.5113702 ,  1.4569368 ,  0.89028114,\n",
       "        0.4722177 , -1.0022151 , -0.06729916, -0.8909719 ,  0.4823557 ,\n",
       "       -0.5183393 ,  1.3706635 , -1.3241087 , -0.5208028 ,  0.582793  ,\n",
       "        0.8133391 ,  0.4767809 ,  0.9184714 ,  0.09100311, -0.48305762,\n",
       "        0.4907189 , -0.52529037,  1.1521325 ,  0.76850736, -1.1143308 ,\n",
       "       -0.7054026 ,  0.8884932 ,  0.36993808,  0.7795363 , -0.17171142,\n",
       "       -0.19055071, -0.19004309, -0.39337373,  0.75889283, -0.8409766 ,\n",
       "        0.7285873 ,  0.36872888,  0.45060405, -0.25678885,  0.45260793,\n",
       "        0.42619264, -0.3449939 , -0.6107882 ], dtype=float32)>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[\"encoded_agent\"][0][3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
